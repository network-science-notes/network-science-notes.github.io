[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in network science. The target audience for these notes are undergraduates in mathematics and computer science who have completed courses in linear algebra, discrete mathematics, and programming.\nThese notes are a collaborative project between Dr. Heather Zinn Brooks (Department of Mathematics, Harvey Mudd College) and Dr. Phil Chodrow (Department of Computer Science, Middlebury College).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction with students who are interested in and fluent in both mathematics and computation. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as Jupyter Notebooks which can be opened in Google Colab. Certain code components have been removed. The purpose is to facilitate live-coding in lectures.\n\n\nUse and Reuse\nThese notes are free to reuse and adapt for educational purposes. Attribution is appreciated but not required.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, the most important of which is Newman (2018).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages. We thank Mason Porter for inspiration in earlier versions of this course.\nHZB was funded in part by the National Science Foundation (grant DMS-2109239) through their program on Applied Mathematics.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "References",
    "text": "References\n\n\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "source/01-networkrepresentations.html",
    "href": "source/01-networkrepresentations.html",
    "title": "1  Networks and their representations",
    "section": "",
    "text": "Introductory Graph Terminology\nWe’ll begin by introducing some terminology for types of graphs we may encounter.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "source/01-networkrepresentations.html#introductory-graph-terminology",
    "href": "source/01-networkrepresentations.html#introductory-graph-terminology",
    "title": "1  Networks and their representations",
    "section": "",
    "text": "Simple graphs\n\n\n\n\n\n\n\nDefinition 1.2 Edges that connect nodes to themselves are called self-edges or self-loops.\nIf there is more than one edge between the same two nodes, this is called a multiedge. A network with multiedges is called a multigraph.\n\n\n\n\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is left as a coding exercise in the live notes.\n\n#---\n\nG = nx.Graph()\nedgelist = [(1, 4), (2, 5), (3, 5), (4, 5), (5, 6)]\nG.add_edges_from(edgelist)\nnx.draw(G)\n\n#---\n\n\n\n\n\n\n\n\nFigure 1.2: A simple graph with 6 nodes and 5 edges.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 A network that has neither self-edges nor multiedges is called a simple graph or simple network.\n\n\n\n\nMany of the networks we’ll study this semester will be simple graphs.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is left as a coding exercise in the live notes.\n\n#---\n\nG = nx.Graph()\nG.add_edges_from([(1, 4), (2, 4), (3, 3), (3, 4)])\nnx.draw(G)\n\n#---\n\n\n\n\n\n\n\n\nFigure 1.3: A graph with self-edges.\n\n\n\n\n\n\n\nPlanar graphs\n\n\n\n\n\n\n\nDefinition 1.4 A planar graph is a graph that can be embedded in the plane without having any edges cross.\n\n\n\n\nPlanar graphs are commonly studied objects in graph theory and there are many cool theorems about them (see, for example, the four-color theorem or Kuratowski’s theorem). This means that a network that can be represented with a planar graph can leverage this theory.\n\n\nDirected graphs\nIn this course, we will spend much of our time focused on the analysis of undirected graphs, where an edge between nodes \\(i\\) and \\(j\\) is an unordered pair \\(\\{i,j\\}\\). However, there is an extension that can be important in many modeling contexts where there may be an edge from \\(j\\) to \\(i\\), but no edge from \\(i\\) to \\(j\\). This is called a directed graph. Informally, each edge in a directed graph has a direction, pointing from one node to another. Directed edges are usually represented by lines with arrows.\n\n\n\n\n\n\n\nDefinition 1.5 A directed graph (also called a directed network or digraph) is a graph in which each edge is an ordered pair \\((j, i)\\), which indicates an edge from node \\(j\\) to node \\(i\\). Such edges are called directed edges (or arcs).\n\n\n\n\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is left as a coding exercise in the live notes.\n\n#---\n\nDG = nx.DiGraph()\nDG.add_edges_from([(1, 2), (1, 3), (3, 4), (4, 1)])\n\nnx.draw(DG, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')\n\n#---\n\n\n\n\n\n\n\n\nFigure 1.4: A directed graph. We’ve chosen some arbitrary node labels for mathematical encoding later on in the notes.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "source/01-networkrepresentations.html#matrix-representations-of-graphs",
    "href": "source/01-networkrepresentations.html#matrix-representations-of-graphs",
    "title": "1  Networks and their representations",
    "section": "Matrix Representations of Graphs",
    "text": "Matrix Representations of Graphs\nOne reason that graphs are particularly useful mathematical representations of networks is that they can be encoded with matrices. This is a huge advantage, because we’ll be able to leverage a lot of theory we know from linear algebra.\nFor our definitions below, we’ll suppose we have a graph \\(G\\) with \\(n\\) vertices.We take the convention that the directed edge \\((j, i)\\) is an edge from \\(j\\) to \\(i\\). Notice that \\((j, i)\\) is represented in the \\(i\\)th row and the \\(j\\)th column of the adjacency matrix. As with so many things in math, this notation is a choice made for mathematical convenience. This choice also allows us to align with both the Newman (2018) textbook and NetworkX syntax. Be aware that different authors and sources might make a different choice!\n\nAdjacency matrix\n\n\n\n\n\n\n\nDefinition 1.6 The adjacency matrix \\(A\\) of a graph \\(G = (V, E)\\) is an \\(n \\times n\\) matrix where \\(n = \\vert V \\vert\\). Its entries are \\[\\begin{align*}\n    A_{ij} = \\begin{cases}\n    1 & (j, i) \\in E \\,, \\\\\n    0 & \\text{otherwise.}\n    \\end{cases}\n\\end{align*}\\]\n\n\n\n\nWe can make a few observations about how the graph structure relates to the structure of the adjacency matrix.\n\nIf there are no self-edges, the diagonal elements are all zero.\nIf the network is undirected, then an edge between \\(i\\) and \\(j\\) implies the existence of an edge between \\(j\\) and \\(i\\). This means that \\(A_{ji} = A_{ij}\\) and thus the adjacency matrix is symmetric.\nSimilarly, if the adjacency matrix is not symmetric, the network cannot be undirected.\n\n\nExample. The adjacency matrix for the graph in Figure 1.4 is \\[\n\\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}\n\\]\n\nIf we have a graph with self-edges, then \\(\\{i,i\\} \\in E\\) for some \\(i.\\) If the graph is undirected, we represent this in the adjacency matrix by setting \\(A_{ii} = 2.\\) If the graph is directed, we set \\(A_{ii} = 1.\\)This is another convention that will make the mathematics easier. An intuitive way to understand this choice is that, in undirected graphs, every edge “shows up” twice in the adjacency matrix, whereas in directed graphs every edge “shows up” once.\nIf we have a graph with multiedges, then we can set the corresponding matrix element \\(A_{ij}\\) equal to the multiplicity of the edge. For example, a double edge between nodes 2 and 3 in an undirected graph is represented \\(A_{23} = A_{32} = 2.\\)\nBut why stop there? Instead of requiring an integer number of edges between two nodes, we could extend this idea to form weighted networks with real-valued edge weights. Sometimes it is useful to represent edges as having a strength, weight, or value. In this situation, we set the value of \\(A_{ij}\\) equal to the weight of the corresponding edge \\((j, i)\\). For example, weights in an airport network could be used represent a distance between two airports, or weights in a social media network could represent the number of messages sent between two individuals.\n\n\nMany more matrices …\nThere are LOTS of matrices that can be associated to networks. There’s no “right” one — some are more useful than others for certain jobs. Throughout this course, we’ll see examples of matrices that are well-suited to certain specific tasks, like ranking or clustering. If you’re interested in searching around a bit, some other fun matrices are:\n\nThe graph Laplacian matrix and its variants.\nThe nonbacktracking or Hashimoto matrix.\nThe modularity matrix.\nThe random-walk transition matrix.\nThe PageRank matrix.\nThe node-edge incidence matrix.\n\nAnd the list goes on!",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "source/01-networkrepresentations.html#graphs-in-networkx",
    "href": "source/01-networkrepresentations.html#graphs-in-networkx",
    "title": "1  Networks and their representations",
    "section": "Graphs in NetworkX",
    "text": "Graphs in NetworkX\nIn this class, we will also be studying networks from a computational perspective. We will be using the NetworkX package in Python.\n\nCreating networks\nWe can create an empty undirected graph \\(G\\) using G = nx.Graph(). You could instead create an empty directed graph using nx.DiGraph() or an empty multigraph using nx.MultiGraph().\nThere are multiple ways to grow your graph in NetworkX.\n\nAdding nodes or edges manually\nYou can add one node at a time. For example,\n\nG.add_node(1)\n\nwill add a node with the label 1. We use an integer here, but a node can be any hashable Python object. You can add one edge at a time using tuples of nodes\n\nG.add_edge(1, 2)\n\nIf you create an edge connecting to a node that’s not in your graph yet, the node gets created automatically.\nIn most cases it’s pretty inefficient to add one node or edge at a time. Fortunately, you can also add nodes and edges from a list or any other iterable container:\n\nG.add_nodes_from(nodelist)\n\n\nG.add_edges_from(edgelist)\n\nThere are corresponding methods to remove nodes and edges: G.remove_node(), G.remove_edge(), G.remove_nodes_from(), G.remove_edges_from().\n\n\nCreating a graph dictionary\nYou can also build your graph using a dictionary that maps nodes to neighbors. The code below creates a graph with 3 nodes, where nodes 1 and 3 are both connected to node 2, but not to each other.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\ngraph_dict = {1: [2], 2: [1, 3], 3:[2]}\nG = nx.Graph(graph_dict) \nnx.draw(G)\n\n\n\n\n\n\n\n\nFigure 1.5: A network built using a dictionary that maps nodes to neighbors.\n\n\n\n\n\n\n\nUsing an adjacency matrix\nYou can directly create a graph using a numpy array that encodes your adjacency matrix representation for your graph. The three-node example above has the adjacency matrix \\[\n\\begin{pmatrix}\n    0 & 1 & 0 \\\\\n    1 & 0 & 1 \\\\\n    0 & 1 & 0\n\\end{pmatrix}\n\\]\nwhich can be encoded after importing the numpy package as np as follows:\n\nA = np.array([[0, 1, 0], [1, 0, 0], [0, 1, 0]])\n\nYou can then quickly build an undirected graph \\(G\\) with\n\nG = nx.from_numpy_array(A)\n\nFor a directed graph, you need to make two additional changes to the code above. You need specify that you want a directed graph with create_using. Additionally, you need to be very careful about the adjacency matrix convention for in and out edges. The default convention for from_numpy_array points edges in the opposite direction to what we defined above (it assumes \\(A_{ij} = 1\\) if \\((i,j) \\in E\\)), so if you want to match that convention, you’ll need to take the transpose with np.transpose.\n\nG = nx.from_numpy_array(np.transpose(A), create_using = nx.DiGraph)\n\n\n\n\nVisualizing your networks\nYou can use the matplotlib plot interface to draw your network. If you have created a graph \\(G\\), you can quickly visualize it using\n\nnx.draw(G)\n\nThere are lots of fun ways to customize your figure, including changing colors, sizes, adding labels, and more. See the documentation for details.\nPractice with the following exercises:\n\nUsing NetworkX to reproduce Figure 1.2, Figure 1.3, Figure 1.4. Use at least two of the different methods described above.\nDraw the network represented by the adjacency matrix \\[\n\\begin{pmatrix}\n  0 & 1 & 0 & 0 & 1 \\\\\n  1 & 0 & 0 & 1 & 1 \\\\\n  0 & 0 & 0 & 1 & 0 \\\\\n  1 & 0 & 0 & 0 & 0 \\\\\n  1 & 0 & 0 & 0 & 0\n\\end{pmatrix} \\,.\n\\]\nCreate your own network.\n\n\n\nShow code\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n# Use NetworkX to reproduce the figures from the notes.\n# Then, create and visualize the network represented by the given adjacency matrix.\n# Finally, create your own network.\n# This is left as a coding exercise in the live notes.\n\n\n#---\nA = np.array([[0, 1, 0, 0, 1], [1, 0, 0, 1, 1], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0]])\nG = nx.from_numpy_array(np.transpose(A), create_using = nx.DiGraph)\nnx.draw(G, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')\n\n#---\n\n\n\n\n\n\n\n\nFigure 1.6: The network corresponding to the adjacency matrix given in the exercise.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "source/01-networkrepresentations.html#references",
    "href": "source/01-networkrepresentations.html#references",
    "title": "1  Networks and their representations",
    "section": "References",
    "text": "References\n\n\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html",
    "href": "source/02-degree-walks-paths.html",
    "title": "2  Degree, Walks, and Paths",
    "section": "",
    "text": "Degree\nUnsurprisingly, degree is directly related to the number of edges in an undirected network.\nIn the code below we use NetworkX to find the degrees of all the nodes in an undirected network: by definition using the adjacency matrix, and with the built-in function nx.degree().\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is a coding exercise in the live notes.\n\n# Create an unweighted version of the Les Mis network\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG_LesMis = unweight(nx.les_miserables_graph())\n\nnx.draw(G_LesMis)\n\n# Create and print a numpy array of node degrees directly from the adjacency matrix\n\n#---\nA = nx.adjacency_matrix(G_LesMis)\nprint('Degrees computed from adjacency matrix:', np.sum(A, axis = 1))\n#---\n\n# Print the degrees using the built in function in NetworkX. \n\n#---\ndegree_vec = G_LesMis.degree()\nprint('Built-in NetworkX:', degree_vec)\n#----\n\n\nDegrees computed from adjacency matrix: [ 1 10  3  3  1  1  1  1  1  1 36  1  2  1  1  1  7  9  7  7  7  7  7 15\n 11 16 11 17  4  8  2  4  1  2  6  6  6  6  6  3  1 11  3  3  2  1  2  1\n 22  7  2  7  2  1  4 19  2 11 15 11  9 11 13 12 13 12 10  1 10 10 10  9\n  3  2  2  7  7]\nBuilt-in NetworkX: [('Napoleon', 1), ('Myriel', 10), ('MlleBaptistine', 3), ('MmeMagloire', 3), ('CountessDeLo', 1), ('Geborand', 1), ('Champtercier', 1), ('Cravatte', 1), ('Count', 1), ('OldMan', 1), ('Valjean', 36), ('Labarre', 1), ('Marguerite', 2), ('MmeDeR', 1), ('Isabeau', 1), ('Gervais', 1), ('Listolier', 7), ('Tholomyes', 9), ('Fameuil', 7), ('Blacheville', 7), ('Favourite', 7), ('Dahlia', 7), ('Zephine', 7), ('Fantine', 15), ('MmeThenardier', 11), ('Thenardier', 16), ('Cosette', 11), ('Javert', 17), ('Fauchelevent', 4), ('Bamatabois', 8), ('Perpetue', 2), ('Simplice', 4), ('Scaufflaire', 1), ('Woman1', 2), ('Judge', 6), ('Champmathieu', 6), ('Brevet', 6), ('Chenildieu', 6), ('Cochepaille', 6), ('Pontmercy', 3), ('Boulatruelle', 1), ('Eponine', 11), ('Anzelma', 3), ('Woman2', 3), ('MotherInnocent', 2), ('Gribier', 1), ('MmeBurgon', 2), ('Jondrette', 1), ('Gavroche', 22), ('Gillenormand', 7), ('Magnon', 2), ('MlleGillenormand', 7), ('MmePontmercy', 2), ('MlleVaubois', 1), ('LtGillenormand', 4), ('Marius', 19), ('BaronessT', 2), ('Mabeuf', 11), ('Enjolras', 15), ('Combeferre', 11), ('Prouvaire', 9), ('Feuilly', 11), ('Courfeyrac', 13), ('Bahorel', 12), ('Bossuet', 13), ('Joly', 12), ('Grantaire', 10), ('MotherPlutarch', 1), ('Gueulemer', 10), ('Babet', 10), ('Claquesous', 10), ('Montparnasse', 9), ('Toussaint', 3), ('Child1', 2), ('Child2', 2), ('Brujon', 7), ('MmeHucheloup', 7)]\n\n\n\n\n\n\n\n\nFigure 2.1: A network of coappearances of characters in the book Les Miserables by Victor Hugo. Nodes represent characters and edges represent characters who appear within the same chapter.\nWe have to be a little more subtle in how we define degree in a directed network because there is a distinction between in-edges and out-edges in these networks.\nWe will repeat the exercises above for directed networks.\nShow code\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\n\n# import data for our directed network\ndf = pd.read_csv(\"https://philchodrow.github.io/PIC16A/homework/HW3-hamilton-data.csv\", \nnames = [\"mentioner\", \"mentioned\"])\n\ndf = df[df[\"mentioner\"] != df[\"mentioned\"]]\n\nG_Hamilton = nx.from_pandas_edgelist(df, \n                            source = \"mentioner\", \n                            target = \"mentioned\", \n                            edge_attr=None, \n                            create_using=nx.DiGraph())\n\nnx.draw(G_Hamilton)\n\n# This is a coding exercise in the live notes.\n\n# Create and print a numpy array of node in-degrees and out-degrees directly from the adjacency matrix\n\n#---\nA = nx.adjacency_matrix(G_Hamilton)\nprint('In-Degrees computed from adjacency matrix:', np.sum(A, axis = 0))\nprint('Out-Degrees computed from adjacency matrix:', np.sum(A, axis = 1))\n#---\n\n# Print the in- and out-degrees using the built in function in NetworkX. \n\n#---\nout_degree_vec = G_Hamilton.out_degree()\nin_degree_vec = G_Hamilton.in_degree()\nprint('In-degrees built-in NetworkX:', in_degree_vec)\nprint('Out-degrees built-in NetworkX:', out_degree_vec)\n#----\n\n\nIn-Degrees computed from adjacency matrix: [13 14  1  4  1  1  1  1 10  1  6  1  2  2  7  6  4  1  9  9  1  1  2  4\n  1  2  3  1  4  2  1  1  0  0  1  0  0  1  1  1  1  1  1  1  0  0]\nOut-Degrees computed from adjacency matrix: [21 20  0  5  0  0  0  0  9  0  0  0  0  1  7  6  0  0  9  0  0  0  3  2\n  5  2  1  0  2  0  0  0  6 13  0  5  6  0  0  0  0  0  0  0  1  1]\nIn-degrees built-in NetworkX: [('burr', 13), ('hamilton', 14), ('weeks', 1), ('madison', 4), ('jay', 1), ('theodosiaDaughter', 1), ('betsy', 1), ('theodosiaMother', 1), ('washington', 10), ('marthaWashington', 1), ('schuylerSis', 6), ('generalMontgomery', 1), ('philipS', 2), ('peggy', 2), ('angelica', 7), ('eliza', 6), ('reynolds', 4), ('generalMercer', 1), ('jefferson', 9), ('jAdams', 9), ('ness', 1), ('pendleton', 1), ('philipH', 2), ('lafayette', 4), ('laurens', 1), ('mulligan', 2), ('lee', 3), ('conway', 1), ('kingGeorge', 4), ('eacker', 2), ('kingLouis', 1), ('maria', 1), ('ensemble', 0), ('company', 0), ('admiralHowe', 1), ('men', 0), ('women', 0), ('franklin', 1), ('paine', 1), ('rochambeau', 1), ('green', 1), ('knox', 1), ('sAdams', 1), ('sally', 1), ('seabury', 0), ('doctor', 0)]\nOut-degrees built-in NetworkX: [('burr', 21), ('hamilton', 20), ('weeks', 0), ('madison', 5), ('jay', 0), ('theodosiaDaughter', 0), ('betsy', 0), ('theodosiaMother', 0), ('washington', 9), ('marthaWashington', 0), ('schuylerSis', 0), ('generalMontgomery', 0), ('philipS', 0), ('peggy', 1), ('angelica', 7), ('eliza', 6), ('reynolds', 0), ('generalMercer', 0), ('jefferson', 9), ('jAdams', 0), ('ness', 0), ('pendleton', 0), ('philipH', 3), ('lafayette', 2), ('laurens', 5), ('mulligan', 2), ('lee', 1), ('conway', 0), ('kingGeorge', 2), ('eacker', 0), ('kingLouis', 0), ('maria', 0), ('ensemble', 6), ('company', 13), ('admiralHowe', 0), ('men', 5), ('women', 6), ('franklin', 0), ('paine', 0), ('rochambeau', 0), ('green', 0), ('knox', 0), ('sAdams', 0), ('sally', 0), ('seabury', 1), ('doctor', 1)]\n\n\n\n\n\n\n\n\nFigure 2.2: A network of mentions in the musical Hamilton. Nodes represent characters; there is an edge from character \\(i\\) to character \\(j\\) if \\(i\\) mentions \\(j\\).\nSome special cases of regular graphs are lattices (e.g., a square lattice is 4-regular) and the complete graph where every node is connected to every other node (which is \\((n-1)\\)-regular).",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#degree",
    "href": "source/02-degree-walks-paths.html#degree",
    "title": "2  Degree, Walks, and Paths",
    "section": "",
    "text": "Definition 2.1 The degree of a node in an undirected network is the number of edges connected to it. The degree of node \\(i\\) is, equivalently, \\[\n    k_i = \\sum_{j=1}^n A_{ij} \\,.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse degree to calculate the total number of edges in an undirected network.\n\n\n\nSolution. The key here is to notice that degree counts ends of edges (sometimes we call these stubs). This means the total number of stubs will be the sum of the degrees of all the nodes. However, each edge is counted twice (each edge has two stubs), so the number of edges \\(m\\) in an undirected network is\n\\[\n    m = \\frac{1}{2}\\sum_{i=1}^n k_i = \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}\\,.\n\\]\nThis relationship between degree and number of edges is a useful fact!\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the mean degree of a node in an undirected network.\n\n\n\nSolution. Let \\(c\\) represent the mean (or expected) degree of a node in an undirected network. Using the previous exercise, \\[\\begin{align}\n    c &= \\frac{1}{n} \\sum_{i = 1}^n k_i \\,, \\\\\n    &= \\frac{2m}{n} \\,.\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 In a directed network, the in-degree is the number of ingoing edges to a node and the out-degree is the number of outgoing edges. That is, the in-degree is defined to be \\[\n    k_i^{\\text{in}} = \\sum_{j=1}^n A_{ij}\n\\] and the out-degree is \\[\n    k_j^{\\text{out}} = \\sum_{i=1}^n A_{ij}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse degree to calculate the total number of edges in a directed network, and use this to calculate the mean (expected) in-degree and out-degree of a directed network.\n\n\n\nSolution. The number of edges in a directed network is equal to the total number of ingoing (respectively, outgoing) ends of edges, so,\n\\[\n    m = \\sum_{i=1}^n k_i^{\\text{in}} = \\sum_{j=1}^n k_i^{\\text{out}} = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\,.\n\\]\nThis means that the expected in-degree and expected out-degree are also equal:\n\\[\\begin{align}\n    c_{\\text{in}} &= \\frac{1}{n} \\sum_{i=1}^n k_i^{\\text{in}} \\\\\n     &= \\frac{1}{n}\\sum_{j=1}^n k_i^{\\text{out}} \\\\\n    &= c_{\\text{out}} \\\\\n    &= c \\,.\n\\end{align}\\]\nCombining these gives \\(c = \\frac{m}{n}\\).\nNotice that this differs by a factor of 2 from the case of undirected networks.\n\n\n\n\n\n\n\n\n\nDefinition 2.3 A network in which all nodes have the same degree is called a regular graph or regular network. A regular graph where all nodes have degree \\(k\\) is called \\(k\\)-regular.\n\n\n\n\n\n\nDensity and sparsity\n\n\n\n\n\n\n\nDefinition 2.4 The density or connectance \\(\\rho\\) of a simple network is the fraction of possible edges that are actually present. That is,\n\\[\n    \\rho = \\frac{\\text{number of edges}}{\\text{possible edges}} = \\frac{m}{\\binom{n}{2}} \\,.\n\\]\n\n\n\n\nOne way to interpret density is to think of it as a probability that a pair of nodes picked uniformly at random is connected by an edge.\nWe can rewrite density in terms of expected degree using our earlier exercises :This simplification comes from the binomial coefficient formula \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\). Also, we can notice the cool fact that \\(n \\choose 2\\) is equivalent to the sum of the first \\(n-1\\) integers!\n\\[\\begin{align}\n    \\rho &= \\frac{m}{\\binom{n}{2}} \\\\\n    &= \\frac{m}{\\frac{1}{2}n(n-1)} \\\\\n    &= \\frac{2m}{n(n-1)} \\\\\n    &= \\frac{c}{n-1} \\,.\n\\end{align}\\]\nIf a network is sufficiently large, you can approximate the density as \\(\\rho \\approx \\frac{c}{n}.\\)\nLet’s compute density in network below using three different strategies:\n\nCalculating directly using number of edges and number of nodes;\nCalculating directly using mean degree and number of nodes;\nUsing the built-in NetworkX function nx.density().\n\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Create an unweighted version of the Les Mis network\nG = G_LesMis\n\n# This is a coding exercise in the live notes.\n\n# Calculate density directly using number of edges and number of nodes\n\n#---\nn = G.number_of_nodes()\nm = G.number_of_edges()\nprint('Density using nodes and edges: ', 2*m/(n*(n-1)))\n#---\n\n# Calculate density directly using mean degree and number of nodes\n# Hint: you may want to calculate degree from the adjacency matrix so that you can calculate mean using numpy\n\n#---\nA = nx.adjacency_matrix(G)\ndegree = np.sum(A, axis = 1)\nc = np.mean(degree)\nprint('Calculating using mean degree', c/(n-1))\n#----\n\n# Use the built-in NetworkX function nx.density()\n\n#---\ndensity = nx.density(G)\nprint('Density with NetworkX built-in:', density)\n#---\n\n\nDensity using nodes and edges:  0.08680792891319207\nCalculating using mean degree 0.08680792891319207\nDensity with NetworkX built-in: 0.08680792891319207\n\n\nWhile it’s pretty straightforward to calculate the density of a network, it’s more complicated to determine whether a network is dense or sparse. There isn’t a universally agreed upon threshold for density below which a real-world network would be considered sparse. However, we can create a definition which applies for certain theoretical models of networks. If we have a model where we can take a formal limit, then such a network is sparse if \\(\\rho \\to 0\\) as \\(n \\to \\infty\\). In this scenario, the mean degree grows (much) more slowly than the number of nodes.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#walks-and-paths",
    "href": "source/02-degree-walks-paths.html#walks-and-paths",
    "title": "2  Degree, Walks, and Paths",
    "section": "Walks and paths",
    "text": "Walks and paths\nWe may like to know if it it is possible to reach one node from another by traversing edges. For this task, we introduce the notion of a walk.\n\n\n\n\n\n\n\nDefinition 2.5 A walk of length \\(k \\geq 2\\) is a set of edges \\(\\{ (i_1,j_1), (i_2, j_2), \\dots, (i_k, j_k)\\}\\) with the property that \\(i_l = j_{l-1}\\) for each \\(2 \\leq l \\leq k\\). We say this is a walk from node \\(i_1\\) to node \\(j_k.\\)\nThe length of a walk is the number of edges it contains.\nA single edge \\((i,j)\\) is always considered a walk of length 1 from \\(i\\) to \\(j\\).\n\n\n\n\nA walk between two nodes is not always well-defined. Consider the example below, where there is no walk from node 7 to node 3.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ngraph_dict = {1: [2], 2: [1, 3], 3:[2], 4:[5,6], 7: [4]}\nG = nx.Graph(graph_dict) \nnx.draw(G, with_labels = True, font_color = 'white', font_weight = 'bold')\n\n\n\n\n\n\n\n\nFigure 2.3: Not all pairs of nodes in this network have a walk between them.\n\n\n\n\n\nA question that pops up a lot in network analysis is “How many walks of length \\(r\\) exist between nodes \\(i\\) and \\(j\\)?”\nThe adjacency matrix gives a concise way to address this question. First, let’s consider \\(r=1\\). That’s just the number of edges from node \\(j\\) to \\(i\\), which is exactly \\(A_{ij}\\). Said another way,\n\nThe \\(ij\\)th entry of \\({\\bf A}^1\\) counts the number of walks of length 1 from node \\(j\\) to node \\(i\\).\n\nThis observation generalizes by induction.\n\n\n\n\n\n\n\nTheorem 2.1 (Counting Walks) The \\(ij\\)th entry of the matrix \\({\\bf A}^r\\) contains the number of walks of length \\(r\\) from \\(j\\) to \\(i\\).\n\n\n\n\n\nProof. We proceed by induction on walk length \\(r\\). We discussed the base case above: that is, the \\(ij\\)th entry of the adjacency matrix \\({\\bf A}_{ij} =  {\\bf A}_{ij}^1\\) gives us walks of length 1, by definition.\nNow, suppose that \\({\\bf A}^r\\) gives the number of walks of length \\(r\\); we will show that \\({\\bf A}^{r+1}\\) gives the number of walks of length \\({r+1}\\). By definition, \\({\\bf A}^{r+1} = {\\bf A}^{r}{\\bf A}.\\) Thinking about matrix multiplication as an inner product, we see that the \\(ij\\) entry can be written\n\\[\n    A_{ij}^{r+1} = \\sum_{l=1}^n A_{il}^rA_{lj} \\,,\n\\]\nthat is, entry \\(ij\\) comes from summing the componentwise product of the \\(i\\)th row of \\({\\bf A}^r\\) with the \\(j\\)th column of \\({\\bf A}.\\)\nThe number of walks from node \\(j\\) to \\(i\\) of length \\(r+1\\) is equivalent to the number of walks of length \\(r\\) from \\(j\\) to \\(l\\) multiplied by the number of length \\(1\\) walks from \\(l\\) to \\(i\\), which is exactly the quantity we have written above. This completes the proof.\n\n\n\n\n\n\n\n\nDefinition 2.6 A path is a walk that is not self-intersecting. That is, any edge \\((i,j)\\) shows up in a path at most once.\nA geodesic path or shortest path is from \\(i\\) to \\(j\\) is a walk fom \\(i\\) to \\(j\\) of minimum length; i.e. a walk such that no other walk has shorter length.\nThe length of a geodesic path is called the (geodesic) distance between \\(i\\) and \\(j\\) If two nodes are not path-connected, their geodesic distance is undefined.\n\n\n\n\nRemarks\n\nShortest paths are not necessarily unique.\nShortest paths are self-avoiding. This is because if a shortest path intersected itself, this would create a loop which could be removed to create a shorter path.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#cyclic-and-acyclic-graphs",
    "href": "source/02-degree-walks-paths.html#cyclic-and-acyclic-graphs",
    "title": "2  Degree, Walks, and Paths",
    "section": "Cyclic and acyclic graphs",
    "text": "Cyclic and acyclic graphs\n\n\n\n\n\n\n\nDefinition 2.7 A cycle is a path from a node \\(i\\) to itself.\nA network with no cycles is acyclic.\n\n\n\n\nBy our definition, self-edges are cycles of length 1.\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the number of cycles of length \\(r\\) in a network starting and ending at node \\(i\\)?\n\n\n\nSolution. Since a cycle can be represented as a walk from a node \\(i\\) to itself, this is the diagonal element \\(A_{ii}^r\\) from the theorem we proved earlier.\nBe careful: this quantity separately counts cycles where the same nodes are visited in a different order. For example, the cycle \\(1 \\to 2 \\to 3 \\to 1\\) is counted separately as the cycle \\(1 \\to 3 \\to 2 \\to 1.\\) To count distinct cycles, you need to divide by the number of combinations.\n\nWhile the formula above is very useful to look for cycles of a specific length, it could be quite inefficient to use to detect whether we have a cycle of any length (because we may have to check the diagonal entries of \\({\\bf A}^r\\) for all possible cycle lengths \\(r\\)). We can construct a simple algorithm to determine computationally whether a network is cyclic or acyclic.\nUse the sketch below to write this algorithm.\n\nFind a node with no out-edges.\nIf no such node exists, the network is cyclic. Otherwise, remove the node and repeat the process.\nIf all nodes can be removed using the strategy above, the network is acyclic.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Implement the algorithm above to create a function is_cyclic(G). Your function should take a NetworkX graph G as an argument and print out whether a graph G is cyclic or acyclic.\n\n#---\ndef is_cyclic(G):\n    while G.number_of_nodes() &gt; 0:\n        zero_out_degree = [node for node in G.nodes if G.out_degree(node) == 0]\n        if len(zero_out_degree) == 0:\n            print('Network is cyclic')\n            return\n        G.remove_nodes_from(zero_out_degree)\n    else: \n        print('Network is acyclic')\n        return\n#---\n\nprint('Binomial Tree:')\nG_Tree = nx.binomial_tree(4, create_using = nx.DiGraph)\nis_cyclic(G_Tree)\n\nprint('Hamilton Network:')\nis_cyclic(G_Hamilton)\n\n\n\n\n\nBinomial Tree:\nNetwork is acyclic\nHamilton Network:\nNetwork is cyclic\n\n\n\nFigure 2.4\n\n\n\nThis algorithm has a nice mathematical consequence. If we label the nodes in an acyclic network according to the order we removed them, we will end up with an adjacency matrix that is strictly upper triangular.  There exists at least one such labeling for any acyclic network.This is because each node that is removed could only have out-edges that were already removed previously, i.e., nonzero entries of the \\(i\\)th column could only occur between columns 1 and \\(i-1\\).\n\nTrees\n\n\n\n\n\n\n\nDefinition 2.8 A tree is a connected, acyclic, undirected network. \n\n\n\n\nBy “connected”, we mean every node is reachable from every other node by traversing a sequence of edges, i.e., there exists a walk between any two nodesTrees are often drawn as rooted trees with a root node at the top and leaf nodes below.\nA few remarks:\n\nTopologically, the root of a tree is not unique\nAll trees are necessarily simple graphs, because self- and multiedges would create cycles.\n\nTrees play important roles, especially in math and computer science. Trees have several useful properties that we can exploit for network analysis:\n\nBecause trees have no cycles, there is exactly one path between any pair of nodes (as long as we don’t allow ``backtracking’’’). Many calculations on networks with this property are simple(r).\nA tree of \\(n\\) nodes has exactly \\(n-1\\) edges. Furthermore, any connected network with \\(n-1\\) edges and \\(n\\) nodes is a tree.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#references",
    "href": "source/02-degree-walks-paths.html#references",
    "title": "2  Degree, Walks, and Paths",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/03-components-laplacian.html",
    "href": "source/03-components-laplacian.html",
    "title": "3  Components and the Graph Laplacian",
    "section": "",
    "text": "Components\nMany networks have parts that are disconnected from each other. These parts are called components. As we saw in an example from the previous lecture, there is no path between any pair of nodes in different components of a network.\nIf we have nodes that are singletons — that is, nodes that have no edges connecting to them — then we consider that node a connected component.\nAs we saw with degree, the definition of connected components requires a bit more subtlety if we consider directed networks.\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\nDG = nx.DiGraph()\nDG.add_edges_from([(1, 2), (2, 3), (2,5), (2,6), (3, 4), (3, 7), (4, 3), (4, 8), (5, 1), (5, 6), (6, 7), (7, 6), (8, 4), (8, 7)])\n\nnx.draw(DG, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')\n\n\n\n\n\n\n\n\nFigure 3.1: Identify the strongly connected components in this directed network.\nIn directed networks, we can also define individual node properties that describe all nodes that could reach or be reached by our node of interest.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Components and the Graph Laplacian</span>"
    ]
  },
  {
    "objectID": "source/03-components-laplacian.html#components",
    "href": "source/03-components-laplacian.html#components",
    "title": "3  Components and the Graph Laplacian",
    "section": "",
    "text": "Definition 3.1 Two nodes \\(i\\) and \\(j\\) are path-connected if there exists a path between \\(i\\) and \\(j\\). The maximal set of nodes \\(j\\) such that \\(i\\) is path-connected to \\(j\\) is called the connected component of \\(i\\). \nA network is connected if it has only one connected component. Otherwise, we say the network is disconnected.\n\n\n\n\nWe say maximal in this definition because we want a connected component to be the biggest subset with this property. Another way to say this is that there are no additional nodes and edges we could include in the set without breaking the path-connected property.\n\n\n\n\n\n\n\n\nDefinition 3.2 In a directed network, two nodes \\(i\\) and \\(j\\) are strongly connected if there exists a path from \\(i\\) to \\(j\\) and a path from \\(j\\) to \\(i\\). A maximal subset of nodes such that all pairs of nodes are strongly connected is called the strongly connected component.\nIf we relax the requirement to consider the maximal subset of node pairs \\((i,j)\\) such that there exists either a path from \\(i\\) to \\(j\\) or from \\(j\\) to \\(i\\), then this is a weakly connected component.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIdentify the strongly connected components and the weakly connected components in the network below.\n\n\n\n\nSolution. This network has three strongly connected components: \\(\\{1,2,5\\}, \\ \\{3, 4, 8\\},\\) and \\(\\{6, 7\\}\\).\nThe network has only one weakly connected component (this set contains all the nodes). Thus we would say the entire network is weakly connected.\n\n\n\n\n\n\n\n\n\nDefinition 3.3 An in-component of node \\(i\\) is the set of nodes \\(j\\) such that there is a directed path from \\(j\\) to \\(i\\).\nAn out-component of node \\(i\\) is the set of nodes \\(j\\) such that there is a directed path from \\(i\\) to \\(j\\).\nWe include the node \\(i\\) itself as a member of its own in- and out-components.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Components and the Graph Laplacian</span>"
    ]
  },
  {
    "objectID": "source/03-components-laplacian.html#the-graph-laplacian",
    "href": "source/03-components-laplacian.html#the-graph-laplacian",
    "title": "3  Components and the Graph Laplacian",
    "section": "The Graph Laplacian",
    "text": "The Graph Laplacian\nThe graph Laplacian  is another important matrix representation of a network. It’s useful in studying random walks and dynamics, for clustering and data analysis, for graph visualization, partitioning, and more!There are mutiple matrices that use this name; the one we introduce here is sometimes called the Combinatorial Graph Laplacian.\n\n\n\n\n\n\n\nDefinition 3.4 The (combinatorial) graph Laplacian \\(L\\) of an undirected graph with adjacency matrix \\(A\\) is\n\\[\n{\\bf L} = {\\bf D} - {\\bf A} \\,\n\\]\nwhere \\({\\bf D}\\) is the diagonal matrix whose diagonal entries \\(D_{ii} = k_i\\) contain the degree of node \\(i\\).\n\n\n\n\nThe definition given above generalizes in a straightforward way for weighted networks with positive weights. There are also variants for directed graphs (including using in-degree or out-degree matrices to build an in-degree Laplacian or an out-degree Laplacian), but some properties may not be preserved with this approach.\n\n\n\n\n\n\n\nTheorem 3.1 (Properties of the graph Laplacian) Consider the combinatorial graph Laplacian \\({\\bf L}\\) as defined above for an undirected network. Let \\({\\bf 1}\\) be the vector containing all ones. The matrix \\({\\bf L}\\) has the following properties:\n\n\\({\\bf L}\\) is real and symmetric.\n\\({\\bf L}{\\bf 1} = {\\bf 0}.\\) That is, every row sums to 0.\nThe eigenvalues of \\({\\bf L}\\) are real and nonnegative.\nThe Laplacian always has at least one zero eigenvalue with corresponding eigenvector \\({\\bf 1}.\\)\nThe Laplacian is not invertible.\nA network has \\(c\\) components if and only if its graph Laplacian has exactly \\(c\\) zero eigenvalues (that is, the eigenvalue \\(\\lambda = 0\\) has algebraic multiplicity \\(c\\).)\n\n\n\n\n\nWe will save the proofs of these properties for homework.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Components and the Graph Laplacian</span>"
    ]
  },
  {
    "objectID": "source/03-components-laplacian.html#the-graph-laplacian-as-a-diffusion-operator",
    "href": "source/03-components-laplacian.html#the-graph-laplacian-as-a-diffusion-operator",
    "title": "3  Components and the Graph Laplacian",
    "section": "The Graph Laplacian as a Diffusion Operator",
    "text": "The Graph Laplacian as a Diffusion Operator\nOne of the many important properties of the graph Laplacian is that it describes many spreading or diffusion processes that take place on networks. Here’s an example: suppose that we “heat up” a single node on the network, and then allow heat to flow along the network edges. The Laplacian matrix gives a concise description of how this heat spreads over the network. Let \\(\\mathbf{x} \\in \\mathbb{R}^n\\) be the vector whose \\(i\\)th entry gives the amount of heat currently on node \\(i\\). Then, the vector \\(\\delta \\mathbf{x} = -\\mathbf{Lx}\\) is proportional to rate of change of heat at each node. If we imagine that heat moves in discrete time, our update would be\n\\[\n\\begin{aligned}\n\\mathbf{x} \\gets \\mathbf{x} -\\alpha\\mathbf{Lx} \\,,\n\\end{aligned}\n\\]\nwhere \\(\\alpha\\) is some constant that describes the rate of heat transfer. Let’s see how this looks:\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\n\nfig, axarr = plt.subplots(1, 4, figsize = (8, 1.7))\n\n# create a network for visualization and set up a layout\nn = 50\nrad = 0.25\nG = nx.random_geometric_graph(n, rad, seed = 1234)\n\nlayout = nx.kamada_kawai_layout(G)\n\n# construct the Laplacian matrix\nA = nx.to_numpy_array(G)\nD = np.diag(np.sum(A, axis = 1))\nL = D - A\n\n# rate of heat transfer\nrate = 0.05\n\n# initial condition: all heat on a single node\nx = np.zeros(n)\nx[20] = 1\n\n# main loop\nfor i, ax in enumerate(axarr.flatten()):\n    nx.draw(G, ax = ax, node_size = 20, edge_color = 'gray', node_color = np.log(x + 1e-4), width = 0.5, pos = layout, cmap = \"coolwarm\", vmax = 0.5)\n    ax.set_title(\"$t = \" + str(i) + \"$\")\n\n    # Laplacian dynamical update\n    x -= rate*L@x\n\n\n\n\n\nSnapshots of heat diffusion on a network. Colors are shown on a logarithmic scale for visualization purposes.\n\n\n\n\nThe Laplacian operator also has many other applications in network science, many of which we will study later in these notes.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Components and the Graph Laplacian</span>"
    ]
  },
  {
    "objectID": "source/03-components-laplacian.html#references",
    "href": "source/03-components-laplacian.html#references",
    "title": "3  Components and the Graph Laplacian",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Components and the Graph Laplacian</span>"
    ]
  },
  {
    "objectID": "source/04-centrality.html",
    "href": "source/04-centrality.html",
    "title": "4  Centrality",
    "section": "",
    "text": "Degree-based Centrality Measures",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "source/04-centrality.html#degree-based-centrality-measures",
    "href": "source/04-centrality.html#degree-based-centrality-measures",
    "title": "4  Centrality",
    "section": "",
    "text": "Degree Centrality\nOne natural definition of importance would be to suppose that important nodes have lots of connections to other nodes. Conveniently, we already have a way to measure this quantity: this is captured by degree!\nIf \\(k_i\\) is the degree of node \\(i\\) and we have a network with \\(n\\) nodes, then we can define the \\(n \\times 1\\) vector \\({\\bf c}_{deg} = \\left(k_1, k_2, \\dots, k_n \\right)\\) to contain the centralities of each node. Using what we know about degree, that means we can calculate centrality directly from the adjacency matrix \\(A\\):\n\\[\n{\\bf c}_{deg} = {\\bf A}{\\bf 1}\n\\]\nwhere \\({\\bf 1}\\) is the vector containing all ones.\nFor a directed network, we could use either in- or out-degree as centrality measures, depending on what is useful for the context or application.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG = unweight(nx.karate_club_graph())\n\n## Degree\n#---\ndeg = nx.degree_centrality(G)\ndeg = np.array([deg[i] for i in deg.keys()])\nnx.draw(G, with_labels = True, node_size = 1000*deg, node_color = deg, cmap = \"Blues\", font_size = 8, edge_color = \"gray\", edgecolors = \"black\", pos=nx.kamada_kawai_layout(G))\n#---\n\n\n\n\n\n\n\n\nFigure 4.1: Visualiation of degree centrality. Node size and color are proportional to centrality: larger nodes in darker blue have higher centrality.\n\n\n\n\n\n\nAdvantages and Disadvantages\nDegree centrality is quick to calculate and interpret, which makes it an attractive choice of centrality measure. The link between network structure and centrality is very clear.\nHowever, we might miss a key feature of relative importance using this simple measure. Perhaps what matters is not only how many connections a node has, but whether or not it is connected to other important nodes. For example, if I have two friends, you may view me as more important if my two friends are Beyonc'{e} and Taylor Swift than if they were two people you had never heard of.\n\n\n\nEigenvector Centrality\nEigenvector centrality accounts for the phenomenon described above by assuming that a node with an important connection is more important. The idea is to “weight” each connection for a node by the centrality of the adjacent neighbor. In this way, high centrality is achieved either by having lots of connections, or by having a few important connections.\nSuppose we have an undirected network with \\(n\\) nodes. We calculate the centrality \\(c_i\\) of node \\(i\\) by summing the centralities of its neighbor, using some proportionality constant \\(\\frac{1}{r}.\\) This gives the equation\n\\[\\begin{align}\n    c_i &= \\frac{1}{r}\\sum_{j \\in \\text{neighbors of } i} c_j \\, \\\\\n    &= \\frac{1}{r} \\sum_{j=1}^n A_{ij}c_j \\,.\n\\end{align}\\]\nLet \\({\\bf c}_{eig}\\) be our new centrality vector. Writing the formula above in matrix notation gives\n\\[\n    {\\bf c}_{eig} = \\frac{1}{r}{\\bf A}{\\bf c}_{eig} \\implies r{\\bf c}_{eig} = {\\bf A}{\\bf c}_{eig} \\,.\n\\]\nNow the name of this centrality measure is very clear: \\({\\bf c}_{eig}\\) is an eigenvector of \\({\\bf A}\\) with associated eigenvalue \\(r\\)!\nThis leads us to a challenge: which eigenvector should we choose? We have up to \\(n\\) linearly independent eigenvectors to choose from, as well as linear combinations of these, so our task of making a meaningful choice seems quite daunting. One constraint we’d like to impose is that we’d like our centrality scores to be nonnegative. Will we always be able to find such an eigenvector for any graph?\nFortunately, we have a powerful theorem that can help us with this task.\n\nPerron–Frobenius Theorem\n\n\n\n\n\n\n\nTheorem 4.1 (Perron–Frobenius Theorem) A nonnegative matrix \\(A\\) has a nonnegative eigenvector with corresponding positive eigenvalue. Furthermore, if the matrix is an adjacency matrix for a (strongly) connected network, then the eigenvector is unique and strictly positive, and the corresponding eigenvalue is the largest of all eigenvalues of the matrix.\n\n\n\n\nWe won’t reproduce the entire theorem here. Horn and Johnson (2012) provides a comprehensive discussion of the proof of this theorem. Keener (1993) also provides a concise proof and interesting applications to ranking.\nFrom the Perron–Frobenius theorem, we know we are guaranteed to have at least one nonnegative eigenvector for \\(A\\). This is good news! For the case where we have a strongly connected graph, then we have a nice unique answer to our problem. We should choose an eigenvector associated with the largest eigenvalue (i.e., the leading eigenvalue). Notice that an scalar multiple of this eigenvector will also work, as the relative rankings of nodes is still preserved. Many people will choose to use a normalized eigenvector for convenience.\n\n\n\n\n\n\n\nDefinition 4.1 The eigenvector centrality \\({\\bf c}_{eig}\\) satisfies\n\\[\n    r{\\bf c}_{eig} = {\\bf A}{\\bf c}_{eig} \\,\n\\]\nwhere \\(r\\) is the leading eigenvalue of \\(A\\). That is, the eigenvector centrality of node \\(i\\) is the \\(i\\)th element of the leading eigenvector of the adjacency matrix.\n\n\n\n\nIf you have multiple strongly connected components, the second statement of the theorem will not hold. However, you can still calculate the eigenvector centrality of each component separately, as each strongly connected component satisfies all conditions for the theorem.\nLet’s implement eigenvector centrality for the same network\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG = unweight(nx.karate_club_graph())\n\n## Eigenvector\n#---\neig = nx.eigenvector_centrality(G)\neig = np.array([eig[i] for i in eig.keys()])\nnx.draw(G, with_labels = True, node_size = 1000*eig, node_color = eig, cmap = \"Blues\", font_size = 8, edge_color = \"gray\", edgecolors = \"black\", pos = nx.kamada_kawai_layout(G))\n#---\n\n\n\n\n\n\n\n\nFigure 4.2: Visualiation of eigevector centrality. Node size and color are proportional to centrality: larger nodes in darker blue have higher centrality. Compare and contrast with the visualization for degree centrality in the same network. What do you notice?\n\n\n\n\n\n\n\nComplications with directed networks\n\nShould this centrality use in-edges or out-edges? This depends on the context! In-edges correspond to right eigenvectors; and out-edges correspond to left eigenvectors.\nOnly nodes that are in a strongly connected component of two or more nodes, or in the out-component of such a strongly connected component, have nonzero eigenvector centrality.\n\nWe will see this challenge in the network below. Calculate the in-degree eigenvector centrality \\(x_i\\) of each node, using the component-wise formula:\n\\[\n    x_i = \\frac{1}{r} \\sum_j A_{ij} x_j \\,.\n\\]\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\nDG = nx.DiGraph()\nDG.add_edges_from([(1,2), (1, 4), (2, 3), (2, 4), (3, 4), (3, 5), (4, 5)])\n\nnx.draw(DG, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')\n\n\n\n\n\n\n\n\nFigure 4.3: Calculate the eigenvector centrality of this network by hand. Can you see a potential problem with eigenvector centrality in directed networks?\n\n\n\n\n\n\n\n\nKatz Centrality\nIt would be nice to be able to generalize eigenvector centrality while being able to avoid some of the issues that arose with nodes having zero centrality if they have zero in-degree.\nWe could try to introduce an intuitive fix by giving each node some centrality “for free.” That is,\n\\[\n    c_i = \\alpha \\sum_j A_{ij}c_j + \\beta \\,\n\\]\nwhere \\(\\alpha, \\beta &gt;0\\) are constant. The first term follows the form we derived for eigenvector centrality, and the second is the baseline level of centrality (the “free centrality”).\nWriting in matrix-vector form, we arrive at a centrality measure \\({\\bf c}_{Katz}\\) due to Katz (1953):\n\\[\n    {\\bf c}_{Katz} = \\alpha A {\\bf c}_{Katz} + \\beta {\\bf 1} \\,.\n\\]\nIf \\(I-\\alpha A\\) is invertible, then we will be able to write a nice expression for \\({\\bf c}_{Katz}\\). We know this matrix is not invertible when \\(det(I-\\alpha A) = 0\\), which is equivalent to the scalar multiple \\(det(\\frac{1}{\\alpha}I - A) = 0\\). We deduce that this occurs when \\(\\lambda = \\frac{1}{\\alpha}\\), where \\(\\lambda\\) are the eigenvalues of the adjacency matrix.\nThus, if we want to be safe and guarantee convergence of our centrality measure, then we should choose \\(\\alpha &lt; \\frac{1}{\\lambda_1}\\), where \\(\\lambda_1\\) is the largest (most positive) eigenvalue of \\(A\\).\n\n\n\n\n\n\n\nDefinition 4.2 Let \\({\\bf A}\\) be the \\(n \\times n\\) adjacency matrix, \\({\\bf 1}\\) be the \\(n \\times 1\\) vector containing all ones, and \\(\\beta &gt; 0\\) constant. Katz centrality \\({\\bf c}_{Katz}\\) is \\[\n    {\\bf c}_{Katz} = \\beta \\left({\\bf I}-\\alpha {\\bf A}\\right)^{-1} {\\bf 1} \\,.\n\\] We are guaranteed convergence for \\(0 &lt; \\alpha &lt; \\frac{1}{\\lambda_1}\\), where \\(\\lambda_1\\) is the leading eigenvalue of \\({\\bf A}.\\)\n\n\n\n\nOften, we will choose to set \\(\\beta = 1\\), since it doesn’t have any impact on the relative ordering of our centrality scores.\nNotice that within this constraint, \\(\\alpha\\) acts like a tunable parameter: As \\(\\alpha \\to 0\\), all the nodes have the same centrality. As \\(\\alpha \\to \\frac{1}{\\lambda_1}\\), we recover eigenvector centrality.\nNow let’s implement Katz centrality in NetworkX. Calculate an appropriate range for \\(\\alpha\\), and then explore how varying \\(\\alpha\\) changes the centrality scores.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG = unweight(nx.karate_club_graph())\n\n## Katz centrality\n#---\nbeta = 1\nA = nx.to_numpy_array(G)\nalpha_max = min(np.abs(1/np.linalg.eig(A)[0]))\nkatz = nx.katz_centrality(G, 0.5*alpha_max, beta)\nkatz = np.array([katz[i] for i in katz.keys()])\nnx.draw(G, with_labels = True, node_size = 1000*katz, node_color = katz, cmap = \"Blues\", font_size = 8, edge_color = \"gray\", edgecolors = \"black\", pos = nx.kamada_kawai_layout(G))\n#---\n\n\n\n\n\n\n\n\nFigure 4.4: Visualiation of Katz centrality. Node size and color are proportional to centrality: larger nodes in darker blue have higher centrality. We can again compare and contrast with our previous centrality measures.\n\n\n\n\n\n\nAdvantages and disadvantages\nKatz centrality keeps several of the nice features of eigenvector centrality while avoiding the zero-centrality pitfalls. It’s also relatively quick to calculate.\nHowever, if a node with high Katz centrality points to many other nodes in a directed network, then they will all “inherit” this high centrality as well, which may be undesirable.\n\n\n\nPageRank\nNeither eigenvector nor Katz centrality measures penalize high-centrality nodes with a large number of edges. Suppose we wanted to think more of centrality like currency: each node has an allotted amount that it may divide among its edges, so that if you were sharing with more edges, you would have to give a smaller amount to each. This idea would essentially “dilute” centrality based on the number of out-edges. This might be relevant in the example of webpages: Just because a page is linked from a very popular site (say, Wikipedia) does not mean that linked page is itself important. Wikipedia links to many, many, many pages!\nWe implement this idea with a small modification to Katz centrality, where we divide by out-degree of each node.\n\\[\n    x_i = \\alpha \\sum_j A_{ij} \\frac{x_j}{k_{j}^{out}} + \\beta\n\\]\nwhere we define \\(k_j^{out} = 1\\) for nodes that have no out-edges to make our mathematical expression well-defined. Defining \\(k_j^{out}=1\\) in this ad hoc way might seem shady, but in fact it is an equivalent expression to the original desired system because \\(A_{ij}=0\\) if a node j has no out-edges.\nFollowing the same arguments as for Katz centrality, this means we can write our PageRank centrality \\({\\bf c}_{PR}\\) as \\[\n    {\\bf c}_{PR} = \\alpha A D^{-1} {\\bf c}_{PR} + \\beta {\\bf 1} \\,,\n\\]\nwhere \\(D\\) is the diagonal matrix with diagonal elements \\(D_{ii} = \\max\\{k_i^{out}, 1\\}\\). If we set \\(\\beta = 1\\) and as long as we have chosen \\(\\alpha\\) appropriately (using similar arguments as before), we can write PageRank centrality in closed form.\n\n\n\n\n\n\n\nDefinition 4.3 Let \\({\\bf A}\\) be the \\(n \\times n\\) adjacency matrix, \\({\\bf D}\\) is the \\(n \\times n\\) diagonal matrix with diagonal elements \\(D_{ii} = \\max\\{k_i^{out}, 1\\}\\), and \\({\\bf 1}\\) be the \\(n \\times 1\\) vector containing all ones. PageRank centrality \\({\\bf c}_{PR}\\) is \\[\n    {\\bf c}_{PR} = \\left({\\bf I}-\\alpha {\\bf A} {\\bf D}^{-1}\\right)^{-1} {\\bf 1} \\,,\n\\] where \\(\\alpha\\) is a parameter chosen so that \\({\\bf I}-\\alpha {\\bf A} {\\bf D}^{-1}\\) is invertible.\n\n\n\n\nThe name PageRank comes from Google, who used this idea as a basis of their original web search algorithm. See Langville and Meyer (2005) for a survey on PageRank and related methods. We will explore PageRank further in future lectures.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG = unweight(nx.karate_club_graph())\n\n## PageRank\n#---\nA = nx.to_numpy_array(G)\nD_inv = np.diag(1/np.sum(A, axis = 1))\nalpha_max = min(np.abs(1/np.linalg.eig(np.dot(A,D_inv))[0]))\npr = nx.pagerank(G, 0.85*alpha_max)\npr = np.array([pr[i] for i in pr.keys()])\nnx.draw(G, with_labels = True, node_size = 1000*pr, node_color = pr, cmap = \"Blues\", font_size = 8, edge_color = \"gray\", edgecolors = \"black\", pos = nx.kamada_kawai_layout(G))\n#---\n\n\n\n\n\n\n\n\nFigure 4.5: Visualiation of PageRank centrality. Node size and color are proportional to centrality: larger nodes in darker blue have higher centrality. We can again compare and contrast with our previous centrality measures.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "source/04-centrality.html#summary",
    "href": "source/04-centrality.html#summary",
    "title": "4  Centrality",
    "section": "Summary",
    "text": "Summary\nWe have derived a family of centrality measures that are all centered around degree. We could provide a neat summary of these four measures we’ve seen so far in the table below.\n\n\n\n\n\n\n\n\n\nWith Constant\nWithout Constant\n\n\n\n\nDivide by out-degree\nPageRank: \\({\\bf c} = \\left(I-\\alpha D^{-1}\\right)^{-1}{\\bf 1}\\)\nDegree: \\({\\bf c} = AD^{-1} {\\bf c}\\)\n\n\nNo division\nKatz: \\({\\bf c} = (I-\\alpha A)^{-1}{\\bf 1}\\)\nEigenvector: \\({\\bf c} = r A {\\bf c}\\)",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "source/04-centrality.html#path-based-centrality-measures",
    "href": "source/04-centrality.html#path-based-centrality-measures",
    "title": "4  Centrality",
    "section": "Path-based Centrality Measures",
    "text": "Path-based Centrality Measures\nAll of the centrality measures we’ve explored thus far are built off variations on the theme of scoring importance based on the number of adjacent nodes. However, an alternate way to think about importance is through a node’s impact on network connectivity through paths.\n\nCloseness Centrality\nOne way to encode this type of importance would be to start with the assumption that a node should have high centrality if it was a small distance to many other nodes. This might be important in transportation or geographic networks, for example.\nConsider a connected graph \\(G\\). Suppose \\(d_{ij}\\) is the shortest (geodesic) distance from node \\(i\\) to node \\(j\\) (that is, the walk of minimum length from \\(i\\) to \\(j\\)). Then, the mean shortest distance \\(l_i\\) from node \\(i\\) to any other node in the network is\n\\[\n    l_i = \\frac{1}{n-1} \\sum_{j=1}^n d_{ij} \\,.\n\\]\nNotice that the qualitative behavior of this quantity is the opposite of what we might usually define for centrality: it has small values for nodes that are separated by others only a short distance (on average) and larger values for longer average distances. If we want our centrality score to be larger for nodes that are close to many other nodes, one way to achieve this is to take the reciprocal. This strategy gives the closeness centrality \\(c_i = \\frac{1}{l_i}\\) of node \\(i\\). This centrality measure dates back to (at least) 1950 from Bavelas (1950).\n\n\n\n\n\n\n\nDefinition 4.4 Consider a (strongly) connected network. Let \\(d_{ij}\\) be the geodesic distance between node \\(i\\) and node \\(j\\). We define the closeness centrality of node \\(i\\) as \\[\n    c_i = \\frac{n-1}{\\sum_{j\\neq i} d_{ij}}\\,.\n\\]\n\n\n\n\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG = unweight(nx.karate_club_graph())\n\n## Closeness\n#---\ncloseness = nx.closeness_centrality(G)\ncloseness = np.array([closeness[i] for i in closeness.keys()])\nnx.draw(G, with_labels = True, node_size = 1000*closeness, node_color = closeness, cmap = \"Blues\", font_size = 8, edge_color = \"gray\", edgecolors = \"black\", pos = nx.kamada_kawai_layout(G))\n#---\n\n\n\n\n\n\n\n\nFigure 4.6: Visualiation of Closeness centrality. Node size and color are proportional to centrality: larger nodes in darker blue have higher centrality. What do you notice about this centrality measure?\n\n\n\n\n\nThis centrality measure has a clear disadvantage for directed or disconnected networks, as it requires the geodesic distance to be defined for all nodes. One simple strategy to fix this would be to compute closeness by only summing geodesic distance to nodes in the same (strongly) connected component. However, be aware that you may not be able to compare centralities between components if you use this strategy: nodes in smaller components will tend to have higher closeness than they would in larger components.\nAnother strategy is to compute the reciprocal of the harmonic mean of the distances, which is a modification by Beauchamp (1965) sometimes referred to as harmonic centrality:\n\\[\n    c_i = \\frac{1}{n-1} \\sum_{j \\neq i, \\ d_{ij}&lt;\\infty} \\frac{1}{d_{ij}}\n\\] where we take the convention \\(\\frac{1}{d_{ij}} = 0\\) if \\(i\\) and \\(j\\) are not path-connected.\n\n\nBetweenness Centrality\nAnother possibility in using paths to measure importance is to encode the idea that a node is important if it lies on paths between many other nodes. Nodes like this might be important because their removal could disrupt paths. Depending on the application, nodes that lie on many paths may have information, goods, data, etc. that pass through frequently.\nLet’s start by considering an undirected network with at most one shortest path between nodes. Let \\(n_{st}^i = 1\\) if node \\(i\\) lies on the shortest path from node \\(s\\) to node \\(t\\) and 0 otherwise. Then, we could sum the number of these unique shortest paths for node \\(i\\) as\n\\[\n    x_i = \\sum_s \\sum_t n_{st}^i \\,.\n\\] Notice that this counts the path from \\(s\\) to \\(t\\) and the path from \\(t\\) to \\(s\\) as two separate paths. In undirected networks, this doesn’t make a difference in the centrality score because it’s only the relative ranking that matters. It also applies as written to directed networks.\nAccounting for the fact that shortest paths may not be unique gives us our definition of betweenness centrality below.\n\n\n\n\n\n\n\nDefinition 4.5 Let \\(n_{st}^i\\) be the number of shortest paths from \\(s\\) to \\(t\\) that pass through \\(i\\), and let \\(g_{st} = \\max\\{1,\\ \\text{number of shortest paths from } s \\text{ to } t\\}.\\) Then the betweenness centrality of node \\(i\\) is\n\\[\n    c_i = \\sum_s \\sum_t \\frac{n_{st}^i}{g_{st}^i} \\,.\n\\]\n\n\n\n\nDirection of travel is accounted for in this definition, and so this can be used without modification in directed networks.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n    return G\n\nG = unweight(nx.karate_club_graph())\n\n## Betweenness\n#---\nbetweenness = nx.betweenness_centrality(G)\nbetweenness = np.array([betweenness[i] for i in betweenness.keys()])\nnx.draw(G, with_labels = True, node_size = 1000*betweenness, node_color = betweenness, cmap = \"Blues\", font_size = 8, edge_color = \"gray\", edgecolors = \"black\", pos = nx.kamada_kawai_layout(G))\n#---\n\n\n\n\n\n\n\n\nFigure 4.7: Visualiation of Betweenness centrality. Node size and color are proportional to centrality: larger nodes in darker blue have higher centrality. What do you notice about this centrality measure?\n\n\n\n\n\nThis centrality measure seems to have originated from Freeman (1977). There are many additional variants and generalizations of betweenness centrality that can be found in the literature.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "source/04-centrality.html#references",
    "href": "source/04-centrality.html#references",
    "title": "4  Centrality",
    "section": "References",
    "text": "References\n\n\n\n\nBavelas, Alex. 1950. “Communication Patterns in Task-Oriented Groups.” The Journal of the Acoustical Society of America 22 (6): 725–30.\n\n\nBeauchamp, Murray A. 1965. “An Improved Index of Centrality.” Behavioral Science 10 (2): 161–63.\n\n\nBoldi, Paolo, and Sebastiano Vigna. 2014. “Axioms for Centrality.” Internet Mathematics 10 (3-4): 222–62.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40: 35–41.\n\n\nHorn, Roger A, and Charles R Johnson. 2012. Matrix Analysis. Cambridge university press.\n\n\nKatz, Leo. 1953. “A New Status Index Derived from Sociometric Analysis.” Psychometrika 18 (1): 39–43.\n\n\nKeener, James P. 1993. “The Perron–Frobenius Theorem and the Ranking of Football Teams.” SIAM Review 35 (1): 80–93.\n\n\nLangville, Amy N, and Carl D Meyer. 2005. “A Survey of Eigenvector Methods for Web Information Retrieval.” SIAM Review 47 (1): 135–61.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "source/05-viz.html",
    "href": "source/05-viz.html",
    "title": "5  Visualizing Networks, and Why You Shouldn’t",
    "section": "",
    "text": "Network Drawing as Minimizing Mean Edge Lengths\nIn Section 6.14.2, Newman (2018) gives some heuristic reasons to suggest that our friend the Laplacian matrix can give a good guide to the quality of a network drawing. This suggests a natural idea: why not just use the graph Laplacian itself to draw the network? Laplacian spectral embedding gives us a way to do this. To implement Laplacian spectral embedding, we:\nThis elegant algorithm is important enough to be implemented in NetworkX: it’s nx.spectral_layout. But we can implement a full version (for connected graphs) of it ourselves.\n#---\nA = nx.to_numpy_array(G)\nD = np.diag(np.sum(A, axis = 1))\nL = D - A\nE = tuple(np.linalg.eig(L)) # compatibility with numpy 2.0.0\n\n_, i, j = np.argsort(E[0])[:3]\n\nv_1 = E[1][:, i]\nv_2 = E[1][:, j]\n\nv = np.column_stack([v_1, v_2])\n#---\nHow’d we do?\nnx.draw(G, pos = {i: v[i] for i in range(G.number_of_nodes())}, **draw_kwargs)\nOh dear, that doesn’t look very good at all! This is a good mini-lesson in applied mathematics: sometimes the most elegant solution is outstanding, and sometimes it just doesn’t work very well in computational practice.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Networks, and Why You Shouldn't</span>"
    ]
  },
  {
    "objectID": "source/05-viz.html#network-drawing-as-minimizing-mean-edge-lengths",
    "href": "source/05-viz.html#network-drawing-as-minimizing-mean-edge-lengths",
    "title": "5  Visualizing Networks, and Why You Shouldn’t",
    "section": "",
    "text": "Form the Laplacian matrix \\(\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\\).\nFind the second and third smallest eigenvalues of the Laplacian.. Let \\(\\mathbf{v}_2\\) and \\(\\mathbf{v}_3\\) be their corresponding eigenvectors.\nPosition the nodes in \\(\\mathbb{R}^2\\) according to the values of these eigenvectors: the first coordinate is given by the entries of \\(\\mathbf{v}_2\\), and the second coordinate is given by the entries of \\(\\mathbf{v}_3\\).\n\nRecall that, if \\(G\\) is connected, then the very smallest eigenvalue of the Laplacian is always equal to 0 and corresponds to an eigenvector with constant entries.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Networks, and Why You Shouldn't</span>"
    ]
  },
  {
    "objectID": "source/05-viz.html#network-drawing-as-isometry-approximation",
    "href": "source/05-viz.html#network-drawing-as-isometry-approximation",
    "title": "5  Visualizing Networks, and Why You Shouldn’t",
    "section": "Network Drawing as Isometry Approximation",
    "text": "Network Drawing as Isometry Approximation\nLet’s try a better approach. We’re going to view network drawing primarily as the problem of placing the nodes. This corresponds to the following problem: For drawing on a computer screen or chalkboard, \\(d = 2\\). The general task of placing nodes in Euclidean space is often called graph embedding, and has applications in deep learning for much higher values of \\(d\\).\n\nTo each node \\(i \\in N\\), assign a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\).\n\nIf we collect all the nodes together, we aim to find a matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the number of nodes in the graph, such that the \\(i\\)th row of \\(\\mathbf{X}\\) is the vector \\(\\mathbf{x}_i\\) assigned to node \\(i\\).\nOf course, we don’t want to just find any old matrix \\(\\mathbf{X}\\); that’s what we did on the random visualization above and the result was not very impressive. Instead, we want to find a matrix \\(\\mathbf{X}\\) that reflects the structure of the graph in some useful way. There are multiple ways to\nThere are multiple ways to approach this problem, but in these notes, we are going to follow the approach of Kamada and Kawai (1989). Their idea is simple to state:\n\nChoose \\(\\mathbf{X}\\) so that the Euclidean distance \\(d^e_{ij} = \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert\\) is approximately the same as the graph geodesic distance \\(d^g_{ij}\\) between nodes \\(i\\) and \\(j\\), for all pairs of nodes \\((i,j)\\).\n\nThis approach is an expression of the geometric idea of isometry: we are looking to map the nodes from one metric space (the graph) into another metric space (Euclidean space) in a way that approximately preserves distance.\nTo make this actionable, let’s define an optimization objective that measures how far apart the Euclidean distances are from the graph geodesic distances:\n\\[\n\\begin{aligned}\n    f(\\mathbf{X}, G) &= \\sum_{(i,j) \\in \\binom{N}{2}} \\left( d^e_{ij} - d^g_{ij} \\right)^2    \\\\\n                     &= \\sum_{(i,j) \\in \\binom{N}{2}} \\left( \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert - d^g_{ij} \\right)^2\\;.\n\\end{aligned}\n\\]\nThis objective function is also sometimes called an “energy” of the problem. Let’s implement a function that computes the energy, given a matrix of distances \\(\\mathbf{D}\\) and a matrix of node positions \\(\\mathbf{X}\\).\n\n#---\ndef energy(X, D):\n    n = X.shape[0]\n    return np.sum([(np.linalg.norm(X[i] - X[j]) - D[i, j])**2 for i in range(n) for j in range(n)])\n\ndef partial_energy(x, D, k):\n    return np.sum([(np.linalg.norm(x - X[j]) - D[k, j])**2 for j in range(G.number_of_nodes())])\n#---\n\nNow we’ll implement an algorithm that attempts to minimize the energy by moving the position of each node, one at a time. Here’s how a single step works:\n\nWe select a node \\(k\\).\nWe minimize \\(k\\)’s contribution to the energy, which is \\(\\sum_{j \\in N\\setminus k} \\left( \\lVert \\mathbf{x}_k - \\mathbf{x}_j \\rVert - d^g_{kj} \\right)^2\\).\n\nWe cycle through all the nodes, updating each of them, and then we repeat the process for some specified number of iterations. First, let’s visualize how this looks. Then, we’ll implement this algorithm ourselves.\n\n\n\n\n\n\n\n\n\nNote that the energy very quickly shrinks after the first iteration. The structure of the visualization is resolved fairly clearly after just a small number of iterations.\nOk, time to implement!\n\n#---\ndef optimization_layout(G, n_iter = 100): \n    D = nx.floyd_warshall_numpy(G) # matrix of Euclidean distances\n    X = random_layout(G) # initialization\n\n    for _ in range(n_iter): # specified number of iterations\n        print(f\"Current energy : {energy(X, D)}\")\n\n        for k in range(G.number_of_nodes()):\n\n            def partial_energy(x, D, k):\n                return np.sum([(np.linalg.norm(x - X[j]) - D[k, j])**2 for j in range(G.number_of_nodes()) if j != k])\n            \n            # minimize the partial energy with respect to the position of node k\n            res = minimize(partial_energy, X[k], args = (D, k), method = 'BFGS')\n\n            # update the position of node k\n            X[k] = res.x\n    \n    return X\n#---\n\nLet’s try it out:\n\n#---\nX = optimization_layout(G, n_iter = 10)\nnx.draw(G, pos = {i: X[i] for i in range(G.number_of_nodes())}, **draw_kwargs)\n#---\n\nCurrent energy : 5008.911265163744\nCurrent energy : 479.1119001476208\nCurrent energy : 332.6430015306248\nCurrent energy : 318.6387699044556\nCurrent energy : 307.3010951083751\nCurrent energy : 303.81948048585843\nCurrent energy : 303.6680015521836\nCurrent energy : 303.6589031275522\nCurrent energy : 303.65811737158\nCurrent energy : 303.65800304142624\n\n\n\n\n\n\n\n\n\nOur result looks pretty reasonable. Nodes that “should” be close to each other do indeed appear to be drawn close together in the graph.\nThe results are not always reliable; if we check the Les Miserables graph, we might argue that a lot of the nodes in the dense center of the graph are a little too squished together:\n\nG = nx.convert_node_labels_to_integers(nx.les_miserables_graph())\nX = optimization_layout(G, n_iter = 10)\nnx.draw(G, pos = {i: X[i] for i in range(G.number_of_nodes())}, **draw_kwargs)\n\nCurrent energy : 146994.73201419896\nCurrent energy : 10775.523958369178\nCurrent energy : 6777.5013476300965\nCurrent energy : 6298.197325534889\nCurrent energy : 6214.343262474495\nCurrent energy : 6204.265664623094\nCurrent energy : 6202.948574547777\nCurrent energy : 6202.76369873904\nCurrent energy : 6202.7292869110115\nCurrent energy : 6202.722256101726\n\n\n\n\n\n\n\n\n\n\nAn Adjustment\nWe can compensate for this by weighting pairs of nodes differently. In the final form of the Kamada-Kawai algorithm, we modify the objective function by incorporating an inverse-square distance weighting. The modified objective function is\n\\[\n\\begin{aligned}\n    f(\\mathbf{X}, G)  &= \\sum_{(i,j) \\in \\binom{N}{2}} \\frac{1}{\\left(d^g_{ij}\\right)^2}\\left( \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert - d^g_{ij} \\right)^2\\;.\n\\end{aligned}\n\\]\nThe implementation here is very similar to our unweighted implementation:\n\ndef energy_weighted(X, D):\n    n = X.shape[0]\n    return np.sum([(np.linalg.norm(X[i] - X[j]) - D[i, j])**2/D[i, j]**2 for i in range(n) for j in range(n) if j != i])\n\ndef optimization_layout_weighted(G, n_iter = 100): \n    D = nx.floyd_warshall_numpy(G) # matrix of Euclidean distances\n    X = random_layout(G) # initialization\n\n    for _ in range(n_iter): # specified number of iterations\n        print(f\"Current energy : {energy(X, D)}\")\n\n        for k in range(G.number_of_nodes()):\n\n            def partial_energy(x, D, k):\n                return np.sum([(np.linalg.norm(x - X[j]) - D[k, j])**2 / D[k,j]**2 for j in range(G.number_of_nodes()) if j != k])\n            \n            # minimize the partial energy with respect to the position of node k\n            res = minimize(partial_energy, X[k], args = (D, k), method = 'BFGS')\n\n            # update the position of node k\n            X[k] = res.x\n    \n    return X\n\nX = optimization_layout_weighted(G, n_iter = 10)\nnx.draw(G, pos = {i: X[i] for i in range(G.number_of_nodes())}, **draw_kwargs)\n\nCurrent energy : 145981.4445187683\nCurrent energy : 20055.04332755151\nCurrent energy : 11679.08054919471\nCurrent energy : 11112.278492742982\nCurrent energy : 10979.893198067035\nCurrent energy : 10861.995804570088\nCurrent energy : 10755.349269139639\nCurrent energy : 10629.245049289957\nCurrent energy : 10508.820963925262\nCurrent energy : 10382.517001175722\n\n\n\n\n\n\n\n\n\nThis version resolves the nodes in the dense core of the network somewhat more cleanly.\n\n\nA few caveats related to this algorithm:\n\nThe energy function is nonconvex, and so in general it is possible for the optimization routine to get stuck at a suboptimal local minimum.\nAny rotation or reflection of the node positions \\(\\mathbf{X}\\) yields a new layout with the same energy.\nThe computation of the complete matrix of pairwise geodesic distances is in general quite expensive on networks of even moderate size.\n\nThere are many other ways to draw networks, several of which are implemented as built-in methods in NetworkX. See the documentation for various other possibilities.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Networks, and Why You Shouldn't</span>"
    ]
  },
  {
    "objectID": "source/05-viz.html#why-you-shouldnt-draw-networks",
    "href": "source/05-viz.html#why-you-shouldnt-draw-networks",
    "title": "5  Visualizing Networks, and Why You Shouldn’t",
    "section": "Why You Shouldn’t Draw Networks",
    "text": "Why You Shouldn’t Draw Networks\nMaking attractive visualizations of large networks is a very fun and satisfying thing to do, and some software packages like Gephi are specifically designed for this task. We encourage you to put network visualizations on your phone wallpaper, t-shirts, posters, websites, etc. Indeed, Phil Chodrow, one of the coauthors of these notes, made a graph visualization as the image background of his website.\nSo, what do we mean when we say that you shouldn’t draw networks? In general, it’s very difficult to extract reliable structural insights about networks by eye. This means that the one place drawings of networks usually don’t belong is in scientific papers. It’s just too easy for the eye to be drawn to structural features that may or may not actually be present in the data. For this reason, node-edge visualizations of large networks have been called names like “ridiculograms” and hairballs by prominent network scientists.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Networks, and Why You Shouldn't</span>"
    ]
  },
  {
    "objectID": "source/05-viz.html#what-you-should-do-instead",
    "href": "source/05-viz.html#what-you-should-do-instead",
    "title": "5  Visualizing Networks, and Why You Shouldn’t",
    "section": "What You Should Do Instead",
    "text": "What You Should Do Instead\nThe right way to visualize the structure of your network is very context-dependent, and there are many, many possibilities. Here we’ll point out just one: the adjacency matrix. This is a common strategy for visualizing networks that separate into one or more distinct clusters, sometimes also called “communities.” For example, simply inspecting the adjacency matrix of the Les Miserables graph can reveal a lot about its structure:\n\nG = nx.les_miserables_graph()\nG = unweight(G)\nG = nx.convert_node_labels_to_integers(G)\nA = nx.to_numpy_array(G)\nplt.imshow(A, cmap = \"Greys\", vmax = 1.2)\n\n\n\n\n\n\n\n\nThe adjacency matrix allows to easily see the presence of dense clusters (indeed, several cliques) in the graph, as well as a few nodes who seem to interact with almost all of the other ones.\nIn this case, the adjacency matrix was already sorted by node in a way that made the structure clear. In more complicated cases, we may need to use a community detection algorithm to find a way to sort the nodes that reveals useful structure. This is a complicated (and thorny) topic which we’ll touch on later in this course.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Networks, and Why You Shouldn't</span>"
    ]
  },
  {
    "objectID": "source/05-viz.html#references",
    "href": "source/05-viz.html#references",
    "title": "5  Visualizing Networks, and Why You Shouldn’t",
    "section": "References",
    "text": "References\n\n\n\n\nKamada, Tomihisa, and Satoru Kawai. 1989. “An Algorithm for Drawing General Undirected Graphs.” Information Processing Letters 31 (1): 7–15.\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Networks, and Why You Shouldn't</span>"
    ]
  },
  {
    "objectID": "source/06-modularity.html",
    "href": "source/06-modularity.html",
    "title": "6  Homophily, assortativity, and modularity",
    "section": "",
    "text": "Assortative mixing and modularity\nSuppose we have a network where nodes are classified by a finite set of descriptive values.\nThis definition gives us some inspiration on a possible strategy on how to assortativity. Assortativity is defined by a comparison to a counterfactual version (or versions) of the network that has the same degree distribution but edges are positioned at random:\n\\[\n\\left[ \\text{fraction of same-type edges} \\right] - \\left[ \\text{expected fraction of same-type edges}\\right] \\,.\n\\]\nLet’s formalize this notation. Let \\(Z\\) be the set of possible group labels. For example, if there are \\(g\\) groups, then we have \\(z_i \\in \\{z_1, \\dots, z_g\\}\\) representing the type of node \\(i\\). To count the total number of same-type edges, we’d like a way to encode 1 every time an edge is between same-type edges.\nWe now have nice mathematical notation to count the total number of same-type edges that are present in our network: \\[\n    \\frac{1}{2} \\sum_i \\sum_j A_{ij}\\delta_{z_iz_j} \\,.\n\\tag{6.1}\\]\nNext we need to calculate the expected number of same-type edges. Let’s suppose that for this comparison we are keeping some important structural properties of our network the same: we want to make sure that the number of edges \\(m\\) is preserved, as well as the degrees \\(k_i\\) of each of the nodes.\nWe need to know the probability that an edge from any node connects to an edge from node \\(j\\):\n\\[\n    P((i,j) \\in E) = \\frac{k_j}{2m-1} \\approx \\frac{k_j}{2m} \\,.\n\\]\n\\[\n    \\frac{1}{2} \\sum_i \\sum_j \\frac{k_ik_j}{2m}\\delta_{z_iz_j} \\,.\n\\tag{6.2}\\]\nWe can combine Equation 6.1 and Equation 6.2 to get a count of the difference between the actual and expected number of same-type edges: \\[\\begin{align}\n    \\frac{1}{2} \\sum_i \\sum_j A_{ij}\\delta_{z_iz_j} - \\frac{1}{2} \\sum_i \\sum_j \\frac{k_ik_j}{2m}\\delta_{z_iz_j} \\\\\n    = \\frac{1}{2}\\sum_i \\sum_j \\left( A_{ij} -\\frac{k_ik_j}{2m}\\right)\\delta_{z_iz_j} \\,.\n\\end{align}\\]\nIf we divide by our total number of edges \\(m\\) we achieve our desired result. This quantity that measures assortativity in a network is called modularity.\nIn some sense modularity measures the extent to which same-type nodes are connected in a network.\nThis expression highlights two things:",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homophily, assortativity, and modularity</span>"
    ]
  },
  {
    "objectID": "source/06-modularity.html#assortative-mixing-and-modularity",
    "href": "source/06-modularity.html#assortative-mixing-and-modularity",
    "title": "6  Homophily, assortativity, and modularity",
    "section": "",
    "text": "Definition 6.1 A network is assortative if nodes of the same type are more likely to be connected to each other than what would be expected if nodes connected to each other randomly. \nA network is disassortative if nodes of different types are more likely to be connected to each other than what would be expected if nodes connected to each other randomly.\n\n\n\n\nWhat do we mean by “randomly?” We mean that if you were to take existing edges and ‘rewire’ them by reassigning new sources and targets uniformly at random, while preserving the degree distribution. This is an important topic that we’ll explore more when we talk about models of networks.\n\n\n\n\n\n\n\n\n\nDefinition 6.2 The Kronecker delta is a mapping \\(\\delta_{ij}: \\mathbb{N}\\times \\mathbb{N} \\to \\{0, 1\\}\\) that satisfies\n\\[\n    \\delta_{ij} = \\begin{cases}\n        0 & \\text{if } i \\neq j \\,, \\\\\n        1 & \\text{if } i = j \\,.\n    \\end{cases}\n\\]\nThat is, it returns 1 if the variables are equal, and 0 otherwise.\n\n\n\n\n\n\n\n\n With this, we can calculate the expected number of edges between node \\(i\\) and node \\(j\\) is approximately \\(k_i\\frac{k_j}{2m}\\). Summing over all possible \\(i,j\\) combinations and again using our Kronecker delta to only count same type edges gives us an approximation for the expected number of edges between same-type node pairsThe approximation above simplifies calculations, and is quite a reasonable choice provided that we are working with a network that has a sufficiently large number of edges.\n\n\n\n\n\n\n\n\n\n\nDefinition 6.3 Let \\(G(V,E)\\) be a network with a set \\(Z\\) of node label types. The modularity \\(Q\\) of a network is defined to be \n\\[\n    Q(G, \\mathbf{z}) \\triangleq \\frac{1}{2m}\\sum_{i,j \\in V}\\left[A_{ij} - \\frac{k_ik_j}{2m}\\right]\\delta_{z_iz_j} \\,.\n\\]\n\n\n\n\nFor simplicity of notation, we shift from explicitly writing \\(\\sum_i \\sum_j\\) in favor of writing \\(\\sum_{i,j \\in V}\\). These mean the same thing!\n\n\nFirst, we are comparing the actual adjacency matrix \\(\\mathbf{A}\\) of the graph to the expected adjacency matrix with entries \\(k_ik_j/2m\\).\nSecond, we are performing this comparison only on the edges in which \\(\\delta_{z_iz_j}=1\\). These are the edges on which \\(z_i = z_j\\); i.e. the edge joins two nodes in the same group.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homophily, assortativity, and modularity</span>"
    ]
  },
  {
    "objectID": "source/06-modularity.html#another-perspective-on-modularity",
    "href": "source/06-modularity.html#another-perspective-on-modularity",
    "title": "6  Homophily, assortativity, and modularity",
    "section": "Another Perspective on Modularity",
    "text": "Another Perspective on Modularity\nThe idea of comparing to a random graph where certain properties are preserved is a pretty useful way to think about the modularity, but there are also others. Let’s take another point of view.This derivation follows Newman eq. 7.55 through 7.58\nLet \\(Z\\) be the set of possible group labels. For example, \\(Z = \\{z_1,z_2,\\ldots,z_g \\}\\) for some \\(g\\). For each label \\(\\ell \\in Z\\), define \\[\ne_\\ell \\triangleq \\frac{1}{2m}\\sum_{i,j\\in V}A_{ij}\\delta_{z_i, \\ell}\\delta_{z_j, \\ell} \\quad \\text{and} \\quad f_\\ell \\triangleq \\frac{1}{2m}\\sum_{i\\in V} k_i \\delta_{z_i, \\ell}\\;.\n\\]\nWe’re going to find copies of these expressions in \\(Q\\). The “trick” is to note that we can do fancy things with the \\(\\delta\\)-function, like this: \\[\n\\delta_{z_i, z_j} = \\sum_{\\ell \\in Z}\\delta_{z_i,\\ell}\\delta_{z_j,\\ell}\n\\tag{6.3}\\] Inserting Equation 6.3 and doing some algebra, we find \\[\n\\begin{aligned}\nQ(G, \\mathbf{z}) &= \\frac{1}{2m}\\sum_{i,j \\in V}\\left[A_{ij} - \\frac{k_ik_j}{2m}\\right]\\delta_{z_i, z_j} \\\\\n&= \\frac{1}{2m}\\sum_{i,j \\in V}\\left[A_{ij} - \\frac{k_ik_j}{2m}\\right]\\sum_{\\ell \\in Z}\\delta_{z_i,\\ell}\\delta_{z_j,\\ell} \\\\\n&= \\frac{1}{2m}\\sum_{\\ell \\in Z}\\sum_{i,j \\in V}\\left[A_{ij}\\delta_{z_i,\\ell}\\delta_{z_j,\\ell} - \\frac{k_ik_j}{2m}\\delta_{z_i,\\ell}\\delta_{z_j,\\ell}\\right] \\\\\n&= \\sum_{\\ell \\in Z}\\left[e_\\ell - \\frac{1}{(2m)^2}\\sum_{i,j \\in V}k_i\\delta_{z_i,\\ell}k_j\\delta_{z_j,\\ell}\\right] \\\\\n&= \\sum_{\\ell \\in Z}\\left[e_\\ell - \\frac{1}{(2m)^2}\\sum_{i \\in V}k_i\\delta_{z_i,\\ell}\\sum_{j \\in V}k_j\\delta_{z_j,\\ell}\\right] \\\\\n&= \\sum_{\\ell \\in Z}\\left[e_\\ell - f_\\ell^2\\right]\\;. \\\\\n\\end{aligned}\n\\tag{6.4}\\] This compact expression for the modularity helps us interpret the expression in a new way. Remember that we consider the network to be assortative when \\(Q(G, \\mathbf{z})\\) is large, i.e., when \\(e_\\ell\\) is large and \\(f_\\ell\\) small.  What does it mean here?Take a minute to figure out why this is true.This is a pretty common kind of balancing act in many optimization settings.\nWell, \\(\\sum_{\\ell \\in Z} e_\\ell\\) is the fraction of all edges that join nodes in the same group. One extreme case is when every node is in the same group: Then, \\(\\sum_{\\ell \\in Z} e_\\ell = 1\\). The term \\(f_\\ell\\) tell us the fraction of ends of edges attached to nodes of type \\(\\ell\\). Because of this, \\(\\sum_{\\ell}f_\\ell = 1.\\) This means that \\(\\sum_{\\ell} f_\\ell^2\\) can be reasonably small in the case where groups have approximately equal sizes. This is related to the optimization problem \\[\n\\min_{\\mathbf{x}\\in \\mathbb{R}^k} \\sum_{\\ell\\in Z}x_\\ell^2 \\ \\text{ such that } \\ \\sum_{\\ell \\in Z}x_\\ell = 1\\;.\n\\] We can solve this problem using Lagrange multipliers, obtaining the solution \\(x_\\ell = 1/k\\) for each \\(\\ell\\). We will explore the idea of optimization and modularity more deeply in a few lectures when we discuss community detection.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homophily, assortativity, and modularity</span>"
    ]
  },
  {
    "objectID": "source/06-modularity.html#references",
    "href": "source/06-modularity.html#references",
    "title": "6  Homophily, assortativity, and modularity",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homophily, assortativity, and modularity</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html",
    "href": "source/07-real-world.html",
    "title": "7  Structure of Empirical Networks",
    "section": "",
    "text": "Introduction\nIt’s all well and good to study the theoretical properties of hypothetical networks. In this set of notes, we’ll start addressing an important empirical question:\nOf course, there’s no simple answer to this question: we observe network data sets across a wide variety of domains, and many of them have different properties. For our purposes today, we’ll look at a sample of four networks:\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\nimport numpy as np\nfrom scipy.special import factorial\nimport pandas as pd\nimport random\n\n# acquire twitch data\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ZHTW_edges.csv\"\nedges = pd.read_csv(url)\nG_twitch = nx.from_pandas_edgelist(edges, \"from\", \"to\", create_using=nx.Graph)\n\n# acquire chameleon data\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/chameleon_edges.csv\"\nedges = pd.read_csv(url)\nG_chameleon = nx.from_pandas_edgelist(edges, \"id1\", \"id2\", create_using=nx.Graph)\n\n# two built-in networkx data sets. \nG_karate = nx.karate_club_graph()\nG_les_mis = nx.les_miserables_graph()\nA fundamental principle in measuring networks is to compare. If we say that a network has a high value of some measurement \\(X\\), then the correct reply is:\nThe following code constructs a random synthetic counterpart graph for each of our empirical graphs. It then adds all of these to a dictionary so that we can easily access both the real and synthetic graphs later.\ndef unweight(G):\n    for source, target in G.edges():\n        G[source][target]['weight'] = 1\n\ndef random_counterpart(G):\n    degrees = [deg for (node, deg) in G.degree()]\n    # G_random = nx.configuration_model(degrees, create_using=nx.Graph)\n    G_random = nx.expected_degree_graph(degrees, selfloops=False)\n    G_random.remove_edges_from(nx.selfloop_edges(G_random))\n    return G_random\n\ndef add_to_dataset_dict(dataset_dict, G, name):\n    unweight(G)\n    dataset_dict[name] = {\n        \"graph\" : G,\n        \"random\" : random_counterpart(G)\n    }\n    \ndataset_dict = {}\nadd_to_dataset_dict(dataset_dict, G_twitch, \"twitch\")\nadd_to_dataset_dict(dataset_dict, G_chameleon, \"chameleon\")\nadd_to_dataset_dict(dataset_dict, G_karate, \"karate\")\nadd_to_dataset_dict(dataset_dict, G_les_mis, \"les_mis\")\nTo help us compute and compare measurements on these graphs, we’ll define the following function which will manage these computations and organize the result as a table. This function takes as an argument a function fun which accepts a graph as an input and returns a scalar value.\ndef compute_metric(fun = lambda x: 0, compare = True):\n    print(\"Data Set\" + \" \" * 10 + \"Real\", end = \"\")\n    if compare: \n            print(\" \" * 10 + \"Random\")\n    else: \n        print()\n    print(\"-\" * 22, end = \"\")\n    if compare: \n        print(\"-\"*18)\n    else: \n        print()\n    for data_set in dataset_dict:\n        print(data_set + \" \" * (14 - len(data_set)) + f\"{fun(dataset_dict[data_set]['graph']):&gt;8.2f}\", end = \"\")\n        if compare:\n            print(\" \" * (8) + f\"{fun(dataset_dict[data_set]['random']):&gt;8.2f}\")\n        else: \n            print()",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#introduction",
    "href": "source/07-real-world.html#introduction",
    "title": "7  Structure of Empirical Networks",
    "section": "",
    "text": "What are real networks like?\n\n\n\ntwitch: A network of mutual friends on the Twitch streaming platform. The data set was collected by Rozemberczki, Allen, and Sarkar (2021).\nchameleon: A network of Wikipedia pages on topics related to chameleons (yes, the animal). An edge exists between two nodes if the corresponding Wikipedia pages link to each other. The data set was collected by Rozemberczki, Allen, and Sarkar (2021).\nkarate: The Zachary Karate Club social network (Zachary 1977), which is packaged with NetworkX.\nles_mis: A network of character interactions in the novel Les Miserables by Victor Hugo, also packaged with NetworkX.\n\n\n\n\nHigh compared to what?\n\nThere are many reasonable answers to this question, and we’ll explore several of them when we come to the study of random graphs. For now, we are going to compare each of our real networks to a synthetic random graph with a similar degree sequence. Technically, we are using a model that reproduces the degree sequence approximately and in expectation. This model is due to Chung and Lu (2002).",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#node-and-edge-counts",
    "href": "source/07-real-world.html#node-and-edge-counts",
    "title": "7  Structure of Empirical Networks",
    "section": "Node and Edge Counts",
    "text": "Node and Edge Counts\nLet’s start with something simple: how many nodes and edges are in each graph?\n\nprint(\"Number of nodes\")\ncompute_metric(lambda x: x.number_of_nodes())\nprint(\"\\nNumber of edges\")\ncompute_metric(lambda x: x.number_of_edges())\n\nNumber of nodes\nData Set          Real          Random\n----------------------------------------\ntwitch         2772.00         2772.00\nchameleon      2277.00         2277.00\nkarate           34.00           34.00\nles_mis          77.00           77.00\n\nNumber of edges\nData Set          Real          Random\n----------------------------------------\ntwitch        63462.00        61625.00\nchameleon     31421.00        30996.00\nkarate           78.00           69.00\nles_mis         254.00          252.00\n\n\nAlthough the number of nodes agree exactly in the real and random networks, there are some small discrepancies in the edge counts. This is due to the fact that our procedure for constructing random graphs (a) only preserves the degrees in expectation rather than exactly and (b) can create some self-loops, which get discarded.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#clustering-coefficient",
    "href": "source/07-real-world.html#clustering-coefficient",
    "title": "7  Structure of Empirical Networks",
    "section": "Clustering Coefficient",
    "text": "Clustering Coefficient\nLet’s move on to something more complex. Take a moment and think of two of your friends, whom we’ll call \\(A\\) and \\(B\\). Are \\(A\\) and \\(B\\) themselves friends with each other? If they do, then we say that there is a triad or triangle in the network.\n\n\n\n\n\n\nA triangle in a social network\n\n\n\nA stylized fact about many networks—especially social networks—is that triangles like these are common. In order to validate this stylized fact, we need to (a) determine how to measure the prevalence of triangles and (b) compare the value of this measure on our real networks to that of their random counterparts.\nThere are many possible measures of the prevalence of triangles, but here we will use the transitivity: the fraction of all possible triangles that are present in the network. The formula for transitivity is\n\\[\n\\begin{aligned}\n    T(G) = \\frac{\\mathrm{trace}(\\mathbf{A}^3)}{\\sum_{i} k_i(k_i - 1)}\n\\end{aligned}\n\\]\nHere, \\(\\mathbf{A}\\) is the adjacency matrix of \\(G\\) and \\(k_i\\) is the degree of node \\(i\\).\nThe numerator of this expression is proportional to the number of triangles in the network (technically, it is off by a factor of 6) and the denominator is proportional to the number of paths of length two in the network. You can think of a triplet as a possible triangle: just add one more edge and a triangle forms. bn\n\n\n\n\n\n\nA triplet in a social network centered on you\n\n\n\nLet’s write a function to compute the transitivity of a graph.\n\n#---\ndef my_transitivity(G):\n    A = nx.adjacency_matrix(G).toarray()\n    # A = 1*(A &gt;= 1) # convert to unweighted form\n\n    # numerator\n    num_triangles = np.trace(A @ A @ A)\n\n    # denominator\n    degrees = A.sum(axis = 0)\n    num_triplets = np.sum(degrees * (degrees - 1))\n\n    return num_triangles / num_triplets\n#---\n\nLet’s compare our function to the built-in function supplied by NetworkX.\n\n#---\nprint(my_transitivity(G_karate))\nprint(nx.transitivity(G_karate))\n#---\n\n0.2556818181818182\n0.2556818181818182\n\n\nLooks good! We’ll move forward with the NetworkX version, as it is substantially faster on larger graphs.\n\n#---\ncompute_metric(nx.transitivity)\n#---\n\nData Set          Real          Random\n----------------------------------------\ntwitch            0.12            0.15\nchameleon         0.31            0.12\nkarate            0.26            0.27\nles_mis           0.50            0.24\n\n\nWe observe that the chameleon and les_mis graphs appear to have substantially greater transitivity than their random counterparts, while both karate and twitch have similar transitivity to their random counterparts. Under this comparison, some networks indeed display very high transitivity.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#connected-components",
    "href": "source/07-real-world.html#connected-components",
    "title": "7  Structure of Empirical Networks",
    "section": "Connected Components",
    "text": "Connected Components\nWhat about the number of connected components in the network?\n\n#---\ncompute_metric(lambda x: len(list(nx.connected_components(x))))\n#---\n\nData Set          Real          Random\n----------------------------------------\ntwitch            1.00           65.00\nchameleon         1.00           60.00\nkarate            1.00            5.00\nles_mis           1.00           10.00\n\n\nRecall that we’ve engineered all of our real networks to have only one connected component, filtering if necessary. On the other hand, the random networks tend to have multiple connected components.\nWould it be fair to say that real networks are more connected than would be expected at random? Some caution is required here. Many researchers collect network data using methods that are especially likely to produce connected networks. For example, snowball sampling in study design refers to the method of recruiting participants for a survey or other instrument by asking people to recommend their friends. Since they can’t recommend people they don’t know, the snowball sample collected from an individual is always connected. Similarly, data sets like the chameleon data set are constructed by following links from one Wikipedia page to another. This method always produces a connected network as well. So, while it is true that many network data sets contain a single connected component, this is often an artifact of data collection rather than a fundamental property of the network.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#degree-degree-correlations",
    "href": "source/07-real-world.html#degree-degree-correlations",
    "title": "7  Structure of Empirical Networks",
    "section": "Degree-Degree Correlations",
    "text": "Degree-Degree Correlations\nWe have constructed random counterpart networks that have similar degree sequences to the real networks we are studying. Networks, however, can have interesting degree structures beyond just the degree sequence. One such structure is the degree-assortativity. The degree assortativity measures the extent to which nodes of similar degree are connected to each other. There are several ways to measure degree assortativity, but the most common one (due to Newman (2018)) has formula\n\\[\n\\begin{aligned}\n    C = \\frac{\\sum_{(u,v) \\in G} k_u k_v - \\frac{1}{m}\\left(\\sum_{(u,v) \\in G}  k_u\\right)^2}{\\sum_{(u,v) \\in G} k_u^2 - \\frac{1}{m}\\left(\\sum_{(u,v) \\in G}  k_u\\right)^2}\\;.\n\\end{aligned}\n\\]\nIf you are familiar with probability and statistics, this formula is equivalent to \\(C = \\frac{\\mathrm{cov}(K_1,K_2)}{\\sqrt{\\mathrm{var}(K_1)\\mathrm{var}(K_2)}}\\), where \\(K_1\\) and \\(K_2\\) are the degrees of the nodes at the ends of an edge selected uniformly at random from \\(G\\). This is also the Pearson correlation coefficient between \\(K_1\\) and \\(K_2\\).\nAn assortative network (with high assortativity) is one in which nodes of high degree tend to connect to each other frequently. A disassortative network (with negative assortativity) is one in which nodes of high degree tend to connect to nodes of low degree. Let’s take a look at the assortativity values in our networks:\n\n#---\ncompute_metric(nx.degree_assortativity_coefficient)\n#---\n\nData Set          Real          Random\n----------------------------------------\ntwitch           -0.23           -0.10\nchameleon        -0.20           -0.06\nkarate           -0.48           -0.22\nles_mis          -0.17           -0.08\n\n\nIt looks like all of our networks are disassortative, and somewhat moreso than their random counterparts. Disassortativity is a common feature of many networks, and it is often attributed to the presence of hubs in the network. Hubs are nodes with very high degree, and they tend to connect to many other nodes. Since there are only a few hubs, they are more likely to connect to nodes of low degree than to other hubs.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#shortest-path-lengths",
    "href": "source/07-real-world.html#shortest-path-lengths",
    "title": "7  Structure of Empirical Networks",
    "section": "Shortest Path Lengths",
    "text": "Shortest Path Lengths\nIn a famous study, Stanley Milgram (1967) 1 asked participants to ensure that a letter reached a target person, whom they did not know, in a US city. However, the participants were only allowed to send the letter to someone they knew on a first-name basis. That person could then send the letter to another person they knew on a first-name basis, and so on, until the letter was delivered (or lost). Perhaps surprisingly, many participants were able to reach the target person in only a few steps, on average. This experiment is the origin of the famous phrase six degrees of separation: in many social networks, most individuals are separated by relatively few links, even when the network is very large.\n1 Yes, that Milgram (Milgram 1963).To test this in our networks, we’ll compute the length of the shortest path between a pair of nodes, averaged across all possible pairs. This quantity isn’t defined for the random counterpart networks that have multiple disconnected components (why?), and so we’ll stick to calculating it on the real-world networks.\n\n#---\ncompute_metric(nx.average_shortest_path_length, compare = False)\n#---\n\nData Set          Real\n----------------------\ntwitch            2.43\nchameleon         3.56\nkarate            2.41\nles_mis           2.64\n\n\nIndeed, despite some of these networks having thousands of nodes and edges, the average shortest path length does not exceed 4 links. We’ll consider some theoretical models that aim to explain this phenomenon later in the course.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/07-real-world.html#references",
    "href": "source/07-real-world.html#references",
    "title": "7  Structure of Empirical Networks",
    "section": "References",
    "text": "References\n\n\n\n\nChung, Fan, and Linyuan Lu. 2002. “Connected Components in Random Graphs with Given Expected Degree Sequences.” Annals of Combinatorics 6 (2): 125–45.\n\n\nMilgram, Stanley. 1963. “Behavioral Study of Obedience.” The Journal of Abnormal and Social Psychology 67 (4): 371.\n\n\n———. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67.\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.\n\n\nRozemberczki, Benedek, Carl Allen, and Rik Sarkar. 2021. “Multi-Scale Attributed Node Embedding.” Journal of Complex Networks 9 (2).\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html",
    "href": "source/08-power-laws.html",
    "title": "8  Power Law Degree Distributions",
    "section": "",
    "text": "Introduction\nLast time, we studied several properties of real-world networks and compared them to randomized models of those networks in which the degrees were held constant. That is, we were looking at aspects of the structure of real-world networks that are not captured by the degree distribution alone. But what about the degree distribution itself? Do real world networks have degree distributions that are especially interesting? What models can account for the degree distributions that we observe?\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\nimport numpy as np\nfrom scipy.special import factorial\nimport pandas as pd\nimport random\nTo observe a degree distribution, let’s take a look at a data set collected from the streaming platform Twitch by Rozemberczki, Allen, and Sarkar (2021). Nodes are users on Twitch. An edge exists between them if they are mutual friends on the platform. The authors collected data sets for users speaking several different languages; we’ll use the network of English-speaking users. Let’s now download the data (as a Pandas data frame) and convert it into a graph using Networkx.\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ENGB_edges.csv\"\nedges = pd.read_csv(url)\nG = nx.from_pandas_edgelist(edges, \"from\", \"to\", create_using=nx.Graph)\n\nnum_nodes = G.number_of_nodes()\nnum_edges = G.number_of_edges()\n\nprint(f\"This graph has {num_nodes} nodes and {num_edges} edges. The mean degree is {2*num_edges/num_nodes:.1f}.\")\n\nThis graph has 7126 nodes and 35324 edges. The mean degree is 9.9.\ndef degree_sequence(G):\n    degrees = nx.degree(G)\n    degree_sequence = np.array([deg[1] for deg in degrees])\n    return degree_sequence\n\ndef log_binned_histogram(degree_sequence, interval = 5, num_bins = 20):\n    hist, bins = np.histogram(degree_sequence, bins = min(int(len(degree_sequence)/interval), num_bins))\n    bins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n    hist, bins = np.histogram(degree_sequence, bins = bins)\n    binwidths = bins[1:] - bins[:-1]\n    hist = hist / binwidths\n    p = hist/hist.sum()\n\n    return bins[:-1], p\n\ndef plot_degree_distribution(G, **kwargs):\n\n    deg_seq = degree_sequence(G)\n    x, p = log_binned_histogram(deg_seq, **kwargs)\n    plt.scatter(x, p,  facecolors='none', edgecolors =  'cornflowerblue', linewidth = 2, label = \"Data\")\n    plt.gca().set(xlabel = \"Degree\", xlim = (0.5, x.max()*2))\n    plt.gca().set(ylabel = \"Density\")\n    plt.gca().loglog()\n    plt.legend()\n    return plt.gca()\nLet’s use this function to inspect the degree distrubtion of the data:\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nThere are a few things to notice about this degree distribution. First, most nodes have relatively small degrees, fewer tham the mean of 9.9. However, there are a small number of nodes that have degrees which are much larger: almost two orders of magnitude larger! You can think of these nodes as “super stars” or “hubs” of the network; they may correspond to especially popular or influential accounts.\nRecall that, from our discussion of the Erdős–Rényi model \\(G(n,p)\\), the degree distribution of a \\(G(n,p)\\) model with mean degree \\(\\bar{d}\\) is approximately Poisson with mean \\(\\bar{d}\\). Let’s compare this Poisson distribution to the data.\ndeg_seq = degree_sequence(G)\nmean_degree  = deg_seq.mean()\nd_      = np.arange(1, 30, 1)\npoisson = np.exp(-mean_degree)*(mean_degree**d_)/factorial(d_)\n\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nax.plot(d_, poisson,  linewidth = 1, label = \"Poisson fit\", color = \"grey\", linestyle = \"--\")\nax.legend()\nComparing the Poisson fit predicted by the Erdős–Rényi model to the actual data, we see that the the Poisson places much higher probability mass on degrees that are close to the mean of 9.9. The Poisson would predict that almost no nodes would have degree higher than \\(10^2\\), while in the data there are several.\nWe often say that the Poisson has a “light right tail” – the probability mass allocated by the Poisson dramatically drops off as we move to the right of the mean. In contrast, the data itself appears to have a “heavy tail”: there is substantial probability mass even far to the right from the mean.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html#introduction",
    "href": "source/08-power-laws.html#introduction",
    "title": "8  Power Law Degree Distributions",
    "section": "",
    "text": "Let’s now define some helper functions to extract and visualize the degree distribution of a graph. Our first function extracts the degree distribution for easy computation, while the second creates a variable-width histogram in which each bin has the same width when plotted on a logarithmic horizontal axis. This is called logarithmic binning.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html#power-laws-as-models-of-heavy-tailed-distributions",
    "href": "source/08-power-laws.html#power-laws-as-models-of-heavy-tailed-distributions",
    "title": "8  Power Law Degree Distributions",
    "section": "Power Laws As Models of Heavy-Tailed Distributions",
    "text": "Power Laws As Models of Heavy-Tailed Distributions\nThere are many probability distributions that have heavy tails. By far the most important (and controversial) in the history of network science is the power law degree distribution.\n\n\n\n\n\n\n\nDefinition 8.1 (Power Law Distribution) A random variable \\(D\\) has a discrete power law distribution with cutoff \\(d_*\\) and exponent \\(\\gamma &gt; 1\\) if its probability mass function is has the form\n\\[\n\\begin{aligned}\n    p_d \\triangleq \\mathbb{P}[D = d] = C d^{-\\gamma}\\;,\n\\end{aligned}\n\\tag{8.1}\\]\nfor all \\(d &gt; d_*\\). Here, \\(C\\) is a normalizing constant that ensures that the distribution sums to \\(1\\). The entries of the distribution for \\(d \\leq d_*\\) are are arbitrary.\n\n\n\n\nAn important intuitive insight about power law distributions is that they are linear on log-log axes. To see this, we can take the logarithm of both sides in Equation 8.1:\n\\[\n\\begin{aligned}\n    \\log p_d = \\log C - \\gamma \\log d\\;.\n\\end{aligned}\n\\]\nSo, \\(\\log p_d\\) is a linear function of \\(\\log d\\) with slope \\(-\\gamma\\).\nLet’s try plotting such a distribution against the data. Since the power law distribution is defined for all \\(d &gt; d_*\\), we’ll need to choose a cutoff \\(d_*\\) and an exponent \\(\\gamma\\). For now, we’ll do this by eye. If we inspect the plot of the data above, it looks like linear behavior takes over somewhere around \\(d_* = 10^\\frac{3}{2} \\approx 30\\).\n\ndeg_seq = degree_sequence(G)\ncutoff  = 30\nd_      = np.arange(cutoff, deg_seq.max(), 1)\ngamma   = 2.7\npower_law = 18*d_**(-gamma)\n\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nax.plot(d_, power_law,  linewidth = 1, label = fr\"Power law: $\\gamma = {gamma}$\" , color = \"grey\", linestyle = \"--\")\nax.legend()\n\n\n\n\n\n\n\n\nThe power law appears to be a much better fit than the Poisson to the tail of the distribution, making apparently reasonable predictions about the numbers of nodes with very high degrees. Where do the parameters for the power law come from? Here we performed a fit “by eye”, but we discuss some systematic approaches in Section 8.5\nThe claim that a given network “follows a power law” is a bit murky: like other models, power laws are idealizations that no real data set matches exactly. The idea of a power law is also fundamentally asymptotic in nature: the power law that we fit to the data also predicts that we should see nodes of degree \\(10^3\\), \\(10^4\\), or \\(10^5\\) if we were to allow the network to keep growing. Since the network can’t keep growing (it’s data, we have a finite amount of it), we have to view the power law’s predictions about very high-degree nodes as extrapolations toward an idealized, infinite-size data set to which we obviously do not have access.\n\nStatistical Properties of Power Laws\nPower law distributions are much more variable than, say, Poisson distributions. Indeed, when \\(\\gamma \\leq 3\\), the variance of a power law distribution is infinite. To show this, we calculate the second moment of the distribution:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[D^2] &= \\sum_{d = 1}^\\infty p_d d^2 \\\\\n                    &= \\sum_{d = 1}^{d_*} p_d d^2 + \\sum_{d = d_* + 1}^\\infty d^2 C d^{-\\gamma} \\\\\n                    &= K + C \\sum_{d = d_*}^\\infty d^{2-\\gamma}\\;,\n\\end{aligned}\n\\]\nwhere \\(K\\) is a (finite) constant. Now we draw upon a fact from calculus: the sequence \\(\\sum_{x = 1}^\\infty x^{-p}\\) converges if and only if \\(p &gt; 1\\). This means that the sum \\(\\sum_{d = d_*}^\\infty d^{2-\\gamma}\\) converges if and only if \\(2 - \\gamma &lt; -1\\), which requires \\(\\gamma &gt; 3\\). If \\(\\gamma \\leq 3\\), the second moment (and therefore the variance) is undefined. Heuristically, this means that the distribution can generate samples which are are arbitrarily far from the mean, much as the data above includes samples which are orders of magnitude higher than the mean.\n\n\nPurported Universality of Power Laws\nBarabási and Albert (1999) made the first published that power-law degree distributions are very common in real-world networks. This initial paper has since been cited over 46,000 times (as of August 2024). Many subsequent studies have fit power-law tails to degree distributions in empirical networks across social, technological, and biological domains.  The idea that power laws are common in real-world networks is sometimes called the “scale-free hypothesis.” Part of the appeal of this hypothesis comes from statistical physics and complexity science, where power law distributions are common signatures of self-organizing systems.Clauset, Shalizi, and Newman (2009), Broido and Clauset (2019) and Holme (2019) provide some review and references for these claims.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html#the-preferential-attachment-model-a-generative-model-for-power-law-degrees",
    "href": "source/08-power-laws.html#the-preferential-attachment-model-a-generative-model-for-power-law-degrees",
    "title": "8  Power Law Degree Distributions",
    "section": "The Preferential Attachment Model: A Generative Model for Power Law Degrees",
    "text": "The Preferential Attachment Model: A Generative Model for Power Law Degrees\nWhy would power-law degree distributions be common in empirical networks? A common way to answer questions like this is to propose a generative model. A generative model is a random network model that is intended to produce some kind of realistic structure.  If the mechanism proposed by the generative model is plausible as a real, common mechanism, then we might expect that the large-scale structure generated by that model would be commonly observed.Generative models contrast with null models like the \\(G(n,p)\\) model, which are usually used to contrast with real networks. Generative models are models of what the data is like; null models are models of what the data is not like.\nBarabási and Albert (1999) are responsible for popularizing the claim that many empirical networks are scale-free. Alongside this claim, they offered a generative model called preferential attachment. The preferential attachment model offers a simple mechanism of network growth which leads to power-law degree distributions. Their model is closely related to the regrettably much less famous models of Yule (1925), Simon (1955), and Price (1976).\nHere’s how the Yule-Simon-Price-Barabási-Albert model works. First, we start off with some initial graph \\(G_0\\). Then, in each timestep \\(t=1,2,\\ldots\\), we: The model of Barabási and Albert (1999) did not include a uniform selection mechanism, which corresponds to the case \\(\\alpha = 1\\).\n\nFlip a coin with probability of heads equal to \\(\\alpha\\). If this coin lands heads, then:\n\nChoose a node \\(u\\) from \\(G_{t-1}\\) with probability proportional to its degree.\n\nOtherwise, if the coin lands tails, choose a node \\(u\\) from \\(G_{t-1}\\) uniformly at random.\nAdd a node \\(v\\) to \\(G_{t-1}\\).\nAdd edge \\((u,v)\\) to \\(G_{t-1}\\).\n\nWe repeat this process as many times as desired. Intuitively, the preferential attachment model expresses the idea that “the rich get richer”: nodes that already have many connections are more likely to receive new connections.\nHere’s a quick implementation. Note: this implementation of preferential attachment is useful for illustrating the mathematics and operations with Networkx. It is, however, not efficient.\n\n# initial condition\nG = nx.Graph() \nG.add_edge(0, 1) \n\nalpha = 3/5 # proportion of degree-based selection steps\n\n# main loop\nfor _ in range(10000):\n    degrees = nx.degree(G)\n\n    # determine u using one of two mechanisms\n    if np.random.rand() &lt; alpha: \n        deg_seq = np.array([deg[1] for deg in degrees])\n        degree_weights = deg_seq / deg_seq.sum()\n        u = np.random.choice(np.arange(len(degrees)), p = degree_weights)\n    else: \n        u = np.random.choice(np.arange(len(degrees)))\n\n    # integer index of new node v\n    v = len(degrees)\n\n    # add new edge to graph    \n    G.add_edge(u, v)\n\nLet’s go ahead and plot the result. We’ll add a visualization of the exponent \\(\\gamma\\) as well. How do we know the right value of \\(\\gamma\\)? It turns out that there is a theoretical estimate based on \\(\\alpha\\) which we’ll derive in the next section.\n\ndeg_seq = degree_sequence(G)\ncutoff  = 10\nd_      = np.arange(cutoff, deg_seq.max(), 1)\ngamma   = (2 + alpha) / alpha\npower_law = 10*d_**(-gamma)\n\nax = plot_degree_distribution(G, interval = 2, num_bins = 30)\nax.plot(d_, power_law,  linewidth = 1, label = fr\"Power law: $\\gamma = {gamma:.2f}$\" , color = \"grey\", linestyle = \"--\")\nax.legend()\n\n\n\n\n\n\n\n\nThis fit is somewhat noisy, reflecting the fact that we simulated a relatively small number of preferential attachment steps.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html#analyzing-preferential-attachment",
    "href": "source/08-power-laws.html#analyzing-preferential-attachment",
    "title": "8  Power Law Degree Distributions",
    "section": "Analyzing Preferential Attachment",
    "text": "Analyzing Preferential Attachment\nLet’s now see if we can understand mathematically why the preferential attachment model leads to networks with power-law degree distributions. There are many ways to demonstrate this fact, including both “casual” and highly rigorous techniques. Here, we’ll use a “casual” argument from Mitzenmacher (2004).\nLet \\(p_d^{(t)}\\) be the proportion of nodes of degree \\(d \\geq 2\\) after algorithmic timestep \\(t\\). Suppose that at this timestep there are \\(n\\) nodes and \\(m\\) edges. Then, the total number of nodes of degree \\(d\\) is \\(n_d^{(t)} = np_d^{(t)}\\). Suppose that we do one step of the preferential attachment model. Let’s ask: what will be the new expected value of \\(n_d^{(t+1)}\\)?\nWell, in the previous timestep there were \\(n_d^{(t)}\\) nodes of degree \\(d\\). How could this quantity change? There are two processes that could make \\(n_d^{(t+1)}\\) different from \\(n_d^{(t)}\\). If we selected a node \\(u\\) with degree \\(d-1\\) in the model update, then this node will become a node of degree \\(d\\) (since it will have one new edge attached to it), and will newly count towards the total \\(n_d^{(t+1)}\\). On the other hand, if we select a node \\(u\\) of degree \\(d\\), then this node will become a node of degree \\(d+1\\), and therefore no longer count for \\(n_d^{(t+1)}\\).\nSo, we can write down our estimate for the expected value of \\(n_d^{(t+1)}\\).\n\\[\n\\begin{aligned}\n    \\mathbb{E}\\left[n_d^{(t+1)}\\right] - n_d^{(t)} = \\mathbb{P}[d_u = d-1] - \\mathbb{P}[d_u = d]\\;.\n\\end{aligned}\n\\]\nLet’s compute the probabilities appearing on the righthand side. With probability \\(\\alpha\\), we select a node from \\(G_t\\) proportional to its degree. This means that, if a specific node \\(u\\) has degree \\(d-1\\), the probability of picking \\(u\\) is\n\\[\n\\begin{aligned}\n    \\mathbb{P}[u \\text{ is picked}] = \\frac{d-1}{\\sum_{w \\in G} d_w} = \\frac{d-1}{2m^{(t)}}\\;.\n\\end{aligned}\n\\]\nOf all the nodew we could pick, \\(p_{d-1}^{(t)}n\\) of them have degree \\(d-1\\). So, the probability of picking a node with degree \\(d-1\\) is \\(n p_{d-1}^{(t)}\\frac{d-1}{2m}\\). On the other hand, if we flipped a tails (with probability \\(1-\\alpha\\)), then we pick a node uniformly at random; each one is equally probable and \\(p_{d-1}^{(t)}\\) of them have degree \\(d-1\\). So, in this case the probability is simply \\(p_{d-1}^{(t)}\\). Combining using the law of total probability, we have\n\\[\n\\begin{aligned}\n    \\mathbb{P}[d_u = d-1] &= \\alpha n p_{d-1}^{(t)}\\frac{d-1}{2m^{(t)}} + (1-\\alpha)p_{d-1}^{(t)} \\\\\n                          &= \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)}\\;.\n\\end{aligned}\n\\]\nA similar calculation shows that\n\\[\n\\begin{aligned}\n    \\mathbb{P}[d_u = d] &= \\alpha n p_{d}^{(t)}\\frac{d}{2m^{(t)}} + (1-\\alpha)p_{d}^{(t)} \\\\\n                          &= \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;,\n\\end{aligned}\n\\]\nso our expectation is\n\\[\n\\begin{aligned}\n    \\mathbb{E}\\left[n_d^{(t+1)}\\right] - n_d^{(t)} = \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nUp until now, everything has been exact: no approximations involved. Now we’re going to start making approximations and assumptions. These can all be justified by rigorous probabilistic arguments, but we won’t do this here.\n\nWe’ll assume that \\(n_d^{(t+1)}\\) is equal to its expectation.\nIn each timestep, we add one new node and one new edge. This means that, after enough timesteps, the number of nodes \\(n\\) and number of edges \\(m\\) should be approximately equal. We’ll therefore assume that \\(t\\) is sufficiently large that \\(\\frac{n}{m} \\approx 1\\).\nStationarity: we’ll assume that, for sufficiently large \\(t\\), \\(p_d^{(t)}\\) is a constant: \\(p_d^{(t)} = p_d^{(t+1)} \\triangleq p_d\\).\n\nTo track these assumptions, we’ll use the symbol \\(\\doteq\\) to mean “equal under these assumptions.”\nWith these assumptions, we can simplify. First, we’ll replace \\(\\mathbb{E}\\left[n_d^{(t+1)}\\right]\\) with \\(n_d^{(t+1)}\\), which we’ll write as \\((n+1)p_d^{(t+1)}\\)\n\\[\n\\begin{aligned}\n    (n+1)p_d^{(t+1)} - np_d^{(t)} \\doteq \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nNext, we’ll assume \\(\\frac{n}{m} \\approx 1\\):\n\\[\n\\begin{aligned}\n    (n+1)p_d^{(t+1)} - np_d^{(t)} \\doteq \\left[\\alpha  \\frac{d-1}{2} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha \\frac{d}{2} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nFinally, we’ll assume stationarity:\n\\[\n\\begin{aligned}\n    (n+1)p_d - np_d \\doteq \\left[\\alpha  \\frac{d-1}{2} + (1-\\alpha)\\right]p_{d-1} - \\left[\\alpha \\frac{d}{2} + (1-\\alpha)\\right]p_{d}\\;.\n\\end{aligned}\n\\]\nAfter a long setup, this looks much more manageable! Our next step is to solve for \\(p_d\\), from which we find\n\\[\n\\begin{aligned}\n    p_d &\\doteq \\frac{\\alpha  \\frac{d-1}{2} + (1-\\alpha)}{1 + \\alpha \\frac{d}{2} + (1-\\alpha)} p_{d-1} \\\\\n    &= \\frac{2(1-\\alpha) + (d-1)\\alpha}{2(1-\\alpha) + 2 + d\\alpha }p_{d-1} \\\\\n    &= \\left(1 - \\frac{2 + \\alpha }{2(1-\\alpha) + 2 + d\\alpha }\\right)p_{d-1}\\;.\n\\end{aligned}\n\\] When \\(d\\) grows large, this expression is approximately \\[\n\\begin{aligned}\n    p_d \\simeq \\left(1 - \\frac{1}{d}\\frac{2+\\alpha}{\\alpha}\\right) p_{d-1}\\;.\n\\end{aligned}\n\\]\nNow for a trick “out of thin air.” As \\(d\\) grows large,\n\\[\n\\begin{aligned}\n    1 - \\frac{1}{d}\\frac{2+\\alpha}{\\alpha} \\rightarrow \\left(\\frac{d-1}{d} \\right)^{\\frac{2 + \\alpha}{\\alpha}}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nJustify the approximation above. To do this, Taylor-expand the function \\(f(x) = x^{\\gamma}\\) to first order around the point \\(x_0 = 1\\) and use this expansion to estimate the value of \\(1 - \\frac{1}{d}\\).\n\n\n\nSolution. We will use a Taylor expansion to find a first-order approximation about \\(x_0 = 1\\):\nThe Taylor expansion to first order is \\[\nf(x) \\approx f(x_0) + f'(x_0)(x-x_0).\n\\]\nLet \\(f(x) = x^{\\gamma}\\). Then \\(f'(x) = \\gamma x ^{\\gamma - 1}.\\) This means that\n\\[\nf(x) \\approx 1 +\\gamma (x-1).\n\\]\nFurthermore, note that \\(\\frac{1}{d} &lt;&lt;1\\) for \\(d\\) large. Substituting in \\(x = 1-\\frac{1}{d}\\) and \\(\\gamma = \\frac{2+\\alpha}{\\alpha}\\) gives \\[\nf(1-\\frac{1}{d}) \\approx 1-\\frac{1}{d} \\frac{2+\\alpha}{\\alpha}.\n\\]\nThis justifies the approximation: the expression we derived could be thought of as a first-order approximation of the function \\(\\left(\\frac{d - 1}{d}\\right)^{\\frac{2+\\alpha}{\\alpha}}.\\)\n\nApplying this last approximation, we have shown that, for sufficiently large \\(d\\),\n\\[\n\\begin{aligned}\n    p_d \\simeq \\left(\\frac{d-1}{d} \\right)^{\\frac{2 + \\alpha}{\\alpha}} p_{d-1}\\;.\n\\end{aligned}\n\\]\nThis recurrence relation, if it were exact, would imply that \\(p_d = C d^{-\\frac{2+\\alpha}{\\alpha}}\\), as shown by the following exercise:\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that \\(p_d\\) is a probability distribution with the property that, for some \\(d_*\\) and for all \\(d &gt; d_*\\), it holds that\n\\[\n\\begin{aligned}\n    p_d = \\left(\\frac{d-1}{d}\\right)^{\\gamma} p_{d-1}\\;.\n\\end{aligned}\n\\]\nProve using induction that \\(p_d = C d^{-\\gamma}\\) for some constant \\(C\\), and explain how to compute \\(C\\).\n\n\n\nSolution. Our proof is by induction.\nBase Case: Since the statement is claimed to hold for all \\(d &gt; d_*\\), our base case is \\(d = d_* + 1\\). In this case we have \\(p_d = \\left(\\frac{d_*}{d}\\right)^{\\gamma} p_{d_*} = \\left[p_{d_*}d_*^{\\gamma}\\right]d^{-\\gamma}\\). We’ll let \\(C = p_{d_*}d_*^\\gamma\\).\nInductive Step: Suppose that the statement holds for some arbitrary \\(d = \\hat{d} &gt; d_*\\). We’ll show that the statement also holds for \\(d = \\hat{d} + 1\\). We can calculate directly\n\\[\n\\begin{aligned}\n    p_{\\hat{d} +  1} &= \\left(\\frac{\\hat{d}}{\\hat{d} + 1}\\right)^{\\gamma} p_{\\hat{d}} \\\\\n    &= C \\left(\\frac{\\hat{d}}{\\hat{d} + 1}\\right)^{\\gamma} \\hat{d}^{-\\gamma} \\\\\n    &= C (\\hat{d} + 1)^{-\\gamma}\\;.\n\\end{aligned}\n\\]\nThis completes the inductive step and the proof.\n\nThis concludes our argument. Although this argument contains many approximations, it is also possible to reach the same conclusion using fully rigorous probabilistic arguments (Bollobás et al. 2001).",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html#sec-power-law-estimation",
    "href": "source/08-power-laws.html#sec-power-law-estimation",
    "title": "8  Power Law Degree Distributions",
    "section": "Estimating Power Laws From Data",
    "text": "Estimating Power Laws From Data\nAfter the publication of Barabási and Albert (1999), there was a proliferation of papers purporting to find power-law degree distributions in empirical networks. For a time, the standard method for estimating the exponent \\(\\gamma\\) was to use the key visual signature of power laws – power laws are linear on log-log axes. This suggests performing linear regression in log-log space; the slope of the regression line is the estimate of \\(\\gamma\\). This approach, however, is badly flawed: errors can be large, and uncertainty quantification is not reliably available. Clauset, Shalizi, and Newman (2009) discuss this problem in greater detail, and propose an alternative scheme based on maximum likelihood estimation and goodness-of-fit tests. Although the exact maximum-likelihood estimate of \\(\\gamma\\) is the output of a maximization problem and is not available in closed form, the authors supply a relatively accurate approximation:\n\\[\n\\begin{aligned}\n    \\hat{\\gamma} = 1 + n\\left(\\sum_{i=1}^n \\log \\frac{d_i}{d_*}\\right)^{-1}\\;.\n\\end{aligned}\n\\]\nAs they show, this estimate and related methods are much more reliable estimators of \\(\\gamma\\) than the linear regression method.\nAn important cautionary note: the estimate \\(\\hat{\\gamma}\\) can be formed regardless of whether or not the power law is a good descriptor of the data. Supplementary methods such as goodness-of-fit tests are necessary to determine whether a power law is appropriate at all. Clauset, Shalizi, and Newman (2009) give some guidance on such methods as well.\n\nPreferential Attachment in Growing Graphs\nWhat if we are able to observe more than the degree distribution of a network? What if we could also observe the growth of the network, and actually know which edges were added at which times? Under such circumstances, it is possible to estimate more directly the extent to which a graph might grow via the preferential attachment mechanism, possibly alongside additional mechanisms. Overgoor, Benson, and Ugander (2019) supply details on how to estimate the parameters of a general class of models, including preferential attachment, from observed network growth.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/08-power-laws.html#are-power-laws-good-descriptors-of-real-world-networks",
    "href": "source/08-power-laws.html#are-power-laws-good-descriptors-of-real-world-networks",
    "title": "8  Power Law Degree Distributions",
    "section": "Are Power Laws Good Descriptors of Real-World Networks?",
    "text": "Are Power Laws Good Descriptors of Real-World Networks?\nAre power laws really that common in empirical data? Broido and Clauset (2019) controversially claimed that scale free networks are rare. In a bit more detail, the authors compare power-law distributions to several competing distributions as models of real-world network degree sequences. The authors find that that the competing models—especially lognormal distributions, which also have heavy tails—are often better fits to observed data than power laws. This paper stirred considerable controversy, which is briefly documented by Holme (2019).\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12. https://doi.org/10.1126/science.286.5439.509.\n\n\nBollobás, Bela, Oliver Riordan, Joel Spencer, and Gábor Tusnády. 2001. “The Degree Sequence of a Scale-Free Random Graph Process.” Random Structures & Algorithms 18 (3): 279–90. https://doi.org/10.1002/rsa.1009.\n\n\nBroido, Anna D., and Aaron Clauset. 2019. “Scale-Free Networks Are Rare.” Nature Communications 10 (1): 1017. https://doi.org/10.1038/s41467-019-08746-5.\n\n\nClauset, Aaron, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. “Power-Law Distributions in Empirical Data.” SIAM Review 51 (4): 661–703. https://doi.org/10.1137/070710111.\n\n\nHolme, Petter. 2019. “Rare and Everywhere: Perspectives on Scale-Free Networks.” Nature Communications 10 (1): 1016. https://doi.org/10.1038/s41467-019-09038-8.\n\n\nMitzenmacher, Michael. 2004. “A Brief History of Generative Models for Power Law and Lognormal Distributions.” Internet Mathematics 1 (2): 226–51. https://doi.org/10.1080/15427951.2004.10129088.\n\n\nOvergoor, Jan, Austin Benson, and Johan Ugander. 2019. “Choosing to Grow a Graph: Modeling Network Formation as Discrete Choice.” In The World Wide Web Conference, 1409–20. San Francisco CA USA: ACM. https://doi.org/10.1145/3308558.3313662.\n\n\nPrice, Derek de Solla. 1976. “A General Theory of Bibliometric and Other Cumulative Advantage Processes.” Journal of the American Society for Information Science 27 (5): 292–306.\n\n\nRozemberczki, Benedek, Carl Allen, and Rik Sarkar. 2021. “Multi-Scale Attributed Node Embedding.” Journal of Complex Networks 9 (2).\n\n\nSimon, Herbert A. 1955. “On a Class of Skew Distribution Functions.” Biometrika 42 (3-4): 425–40. https://doi.org/10.1093/biomet/42.3-4.425.\n\n\nYule. 1925. “A Mathematical Theory of Evolution, Based on the Conclusions of Dr. J. C. Willis, F. R. S.” Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character 213 (402-410): 21–87. https://doi.org/10.1098/rstb.1925.0002.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/09-random-graphs.html",
    "href": "source/09-random-graphs.html",
    "title": "9  Random Graphs: Erdős–Rényi",
    "section": "",
    "text": "Why Random Graphs?\nWhy should we even study random graphs? There are a few reasons!",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Graphs: Erdős–Rényi</span>"
    ]
  },
  {
    "objectID": "source/09-random-graphs.html#why-random-graphs",
    "href": "source/09-random-graphs.html#why-random-graphs",
    "title": "9  Random Graphs: Erdős–Rényi",
    "section": "",
    "text": "Random Graphs as Insightful Models\nRandom graphs allow us to build our intuition and skills in the study of networks. In many simple random graphs, quantities of interest (clustering coefficients, diameters, etc) can be calculated with pencil and paper. This allows us to build mathematical insight into the structure of many real-world models, without the need for detailed simulations.\n\n\nRandom Graphs as Mathematical Puzzles\nMany properties of even simple random graphs are still under investigation by research mathematicians. While some properties can be calculated simply, others require extremely sophisticated machinery in order to understand. There are many mathematicians who spend their careers studying random graphs, and their cousins, random matrices.\n\n\nRandom Graphs as Null Hypotheses\nSuppose that you compute the global clustering coefficient of a graph and find it to be \\(0.31\\). How do we interpret that? Is that high? Low? In this case, we should ask: compared to what? Random graphs allow us one way to make a comparison: that clustering coefficient is “high” if it’s larger than the clustering coefficient in a suitably chosen, comparable random graph. How to choose a comparable random graph is an important question, and the answer is not always clear!\nIn this way, random graphs often serve as the null hypothesis, in exactly the same way you might have heard of the null hypothesis for other kinds of statistical tests.\n\n\nRandom Graphs and Statistical Inference\nMany statistical algorithms for tasks like graph clustering, ranking, and prediction come from random graph models. The idea, generally speaking, is to imagine that we observe a graph, and then try to make the best educated guess possible about the model that generated that graph. This is classical statistical inference.",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Graphs: Erdős–Rényi</span>"
    ]
  },
  {
    "objectID": "source/09-random-graphs.html#the-gnp-model-erdősrényi",
    "href": "source/09-random-graphs.html#the-gnp-model-erdősrényi",
    "title": "9  Random Graphs: Erdős–Rényi",
    "section": "The \\(G(n,p)\\) Model (Erdős–Rényi)",
    "text": "The \\(G(n,p)\\) Model (Erdős–Rényi)\nThe model of Erdős and Rényi (1960) is the simplest and most fundamental model of a random graph. Our primary interest in the ER model is for mathematical insight and null modeling. The ER model is mostly understood in its major mathematical properties, and it’s almost never used in statistical inference. The ER model is, however, a building block of models that are used in statistical inference. The stochastic blockmodel, for example, is often used for graph clustering and community detection. In its simplest form, it’s a bunch of ER graphs glued together.\n\n\n\n\n\n\n\nDefinition 9.1 (Erdős–Rényi Random Graph) An Erdős–Rényi random graph with \\(n\\) nodes and connection probability \\(p\\), written \\(G(n,p)\\), is a random graph constructed by placing an edge with probability \\(p\\) between each pair of distinct nodes.\n\n\n\n\nWe can imagine visiting each possible pair of edges \\((i,j)\\) and flipping a coin with probability of heads \\(p\\). If heads, we add \\((i,j) \\in E\\); otherwise, we don’t.\n\n\n\n\n\n\nExercise: Let \\(i\\) be a fixed node in a \\(G(n,p)\\) graph, and let \\(K_i\\) be its (random) degree. Show that \\(K_i\\) has binomial distribution with success probability \\(p\\) and \\(n-1\\) trials.\n\n\n\n\nSolution. A given node \\(u\\) is connected to each of the other \\(n-1\\) nodes (independently) with probability \\(p\\). The probability of being connected to a particular set of \\(k\\) nodes is\n\\[\\begin{align}\n    \\mathbb{P}[\\text{connected to $k$ given nodes}] &= \\mathbb{P}[\\text{$k$ successes}]\\mathbb{P}[\\text{$n-1-k$ failures}] \\\\\n    &= p^k (1-p)^{n-1-k} \\,.\n\\end{align}\\]\nBuilding on this, the probability of being connected to \\(k\\) other nodes in any way is\n\\[\\begin{align}\n    \\mathbb{P}[\\text{connected to $k$ nodes}] &= \\left( \\text{number of ways to choose $k$ nodes }\\right) \\mathbb{P}[\\text{$k$ successes}]\\mathbb{P}[\\text{$n-1-k$ failures}] \\\\ \\\\\n    &= {n-1 \\choose k} p^k (1-p)^{n-1-k} \\,.\n\\end{align}\\]\nIndeed, this is exactly the binomial degree distribution, as required.\n\n\n\n\n\n\n\nExercise: Show that \\(\\mathbb{E}[K_i] = p(n-1)\\).\n\n\n\n\nSolution. Using our previous calculations, we know \\(\\mathbb{E}[K_i] = \\mathbb{E}[\\frac{2m}{n}].\\) However, \\(n\\) is fixed for the \\(G(n,p)\\) model, so we have \\(\\mathbb{E}[K_i] = \\frac{2}{n}\\mathbb{E}[m],\\) where \\(\\mathbb{E}[m]\\) is the expected number of edges. This quantity will be equal to the probability that an edge exists between a pair of nodes multiplied by the number of possible pairs, which is \\({n \\choose 2}p = \\frac{n(n-1)}{2}p\\). Thus \\(\\mathbb{E}[K_i] = (n-1)p\\), as required.\n\n\nClustering Coefficient\nBoth local and global clustering coefficients are defined in terms of a ratio of realized triangles to possible triangles. Analyzing ratios using probability theory can get tricky, but we can get a pretty reliable picture of things by computing the expectations of the numerator and denominator separately.\nLet’s take the global clustering coefficient. For this, we need to compute the total number of triangles, and the total number of wedges.\nHow many triangles are there? Well, there are \\(\\binom{n}{3}\\) ways to choose 3 nodes from all the possibilities, and the probability that all three edges exist to form the triangle is \\(p^3\\). So, in expectation, there are \\(\\binom{n}{3}p^3\\) triangles in the graph.\nHow many wedges are there? Well, each triple of nodes contains three possible wedges, and the probability of any given wedge existing is \\(p^2\\). So, the expected number of wedges is \\(\\binom{n}{3}p^2\\). Our estimate for the expected clustering coefficient is that it should be about \\(p\\), although we have been fast and loose with several mathematical details to arrive at this conclusion.",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Graphs: Erdős–Rényi</span>"
    ]
  },
  {
    "objectID": "source/09-random-graphs.html#sparsity",
    "href": "source/09-random-graphs.html#sparsity",
    "title": "9  Random Graphs: Erdős–Rényi",
    "section": "Sparsity",
    "text": "Sparsity\nRecall that it is possible to define sparsity more explicitly when we have a theoretical model (where it makes sense to take limits). Indeed, the sparse Erdős–Rényi model is very useful. Intuitively, the idea of sparsity is that there are not very many edges in comparison to the number of nodes.\n\n\n\n\n\n\n\nDefinition 9.2 We say that a \\(G(n,p)\\) graph is sparse when \\(p = c/(n-1)\\) for some constant \\(c\\).\n\n\n\n\nA consequence of sparsity is that \\(\\mathbb{E}[K_i] = c\\); i.e. the expected degree of a node in sparse \\(G(n,p)\\) is constant. When studying sparse \\(G(n,p)\\), we are almost always interested in the case \\(n\\rightarrow \\infty\\).\n\nClustering\nWhat does this imply for our estimation of the global clustering coefficient from before? Well, we expect the global clustering coefficient to be about \\(p\\), and if \\(p = c/(n-1)\\), then \\(p \\rightarrow 0\\) as \\(n \\to \\infty\\) for sparse \\(G(n,p)\\).\nThe need to move beyond the ER model to develop sparse graphs with clustering coefficients was part of the motivation of Watts and Strogatz (1998), a famous paper that introduced the “small world model.”\n\nSparse Erdős–Rényi graphs have vanishing clustering coefficients.\n\nFigure 9.1 shows how the global clustering coefficient of a sparse Erdős–Rényi random graph decays as we increase \\(n\\). Although the estimate that the global clustering coefficient should be equal to \\(p\\) was somewhat informal, experimentally it works quite well.\n\n\nShow code\nimport networkx as nx\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nc = 5\nN = np.repeat(2**np.arange(5, 15), 10)\n\ndef global_clustering(n, c):\n    G = nx.fast_gnp_random_graph(n, c/(n-1))\n    return nx.transitivity(G)\n\nT = [global_clustering(n, c) for n in N]\n\nfig, ax = plt.subplots(1)\n\ntheory = ax.plot(N, c/(N-1), color = \"black\", label = \"Estimate\")\n\nexp = ax.scatter(N, T, label = \"Experiments\")\nsemil = ax.semilogx()\nlabels = ax.set(xlabel = \"Number of nodes\", \n       ylabel = \"Global clustering coefficient\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9.1\n\n\n\n\n\n\n\nCycles and Local Tree-Likeness\nRecall the following definition:\n\n\n\n\n\n\n\nDefinition 9.3 A cycle is a walk that does not repeat edges and ends at the same node that it begins.\n\n\n\n\nA triangle is an example of a cycle of length \\(3\\).\n\n\n\n\n\n\n\nTheorem 9.1 In the sparse \\(G(n,p)\\) model, for any length \\(k\\), the probability that there exists a cycle of length \\(k\\) attached to node \\(i\\) shrinks to 0 as \\(n \\rightarrow \\infty\\).\n\n\n\n\nYou’ll prove a generalization of Theorem 9.1 in an upcoming homework problem.\nGraphs in which cycles are very rare are often called locally tree-like. A tree is a graph without cycles; if cycles are very rare, then we can often use techniques that are normally guaranteed to only work on trees without running into (too much) trouble.\n\n\nPath Lengths\nHow far apart are two nodes in \\(G(n,p)\\)? Again, exactly computing the length of geodesic paths involves some challenging mathematical detail.  However, we can get a big-picture view of the situation by asking a slightly different question:See Riordan and Wormald (2010) and references therein.\n\nGiven two nodes \\(i\\) and \\(j\\), what is the expected number of paths of length \\(k\\) between them?\n\nLet \\(R(k)\\) denote the number of \\(k\\)-paths between \\(i\\) and \\(j\\). Let \\(r(k) = \\mathbb{E}[R(k)]\\). Let’s estimate \\(r(k)\\).\nFirst, we know that \\(r(1) = p\\). For higher values of \\(k\\), we’ll use the following idea: in order for there to be a path of length \\(k\\) from \\(i\\) to \\(j\\), there must be a node \\(\\ell\\) such that:\n\nThere exists a path from \\(i\\) to \\(\\ell\\) of length \\(k-1\\). In expectation, there are \\(r(k-1)\\) of these.\nThere exists a path from \\(\\ell\\) to \\(j\\) of length \\(1\\). This happens with probability \\(p\\).\n\nThere are \\(n-2\\) possibilities for \\(\\ell\\) (excluding \\(i\\) and \\(j\\)), and so we obtain the approximate relation\n\\[\nr(k) \\approx (n-2)r(k-1)p\\;.\n\\]\n\n\n\n\n\n\nWhy is this an approximation? Well, some of the paths between \\(i\\) and \\(\\ell\\) that are counted in \\(r(k-1)\\) could actually include the edge \\((j, \\ell)\\) already. An example is \\((i,j), (j,\\ell)\\). In this case, the presence of edge \\((j,\\ell)\\) is not independent of the presence of the path between \\(i\\) and \\(\\ell\\). The derivation above implicitly treats these two events as independent. Again, because cycles are rare in large, sparse ER, this effect is small when \\(k\\) is small.\n\n\n\nProceeding inductively and approximating \\(n-2 \\approx n-1\\) for \\(n\\) large, we have the relation\n\\[\nr(k) \\approx (n-1)^{k-1}p^{k-1}r(1) = c^{k-1}p\n\\tag{9.1}\\]\nin the sparse ER model.\nUsing this result, let’s ask a new question:\n\nWhat path length \\(k\\) do I need to allow to be confident that there’s a path between nodes \\(i\\) and \\(j\\)?\n\nWell, suppose we want there to be \\(q\\) paths. Then, we can solve \\(q = c^{k-1}p\\) for \\(k\\), which gives us:\n\\[\n\\begin{aligned}\nq &= c^{k-1}p \\\\\n\\log q &= (k-1)\\log c + \\log p \\\\\n\\log q &= (k-1)\\log c + \\log c - \\log n \\\\\n\\frac{\\log q + \\log n}{\\log c} &= k\n\\end{aligned}\n\\]\nHere, I’ve also approximated \\(\\log n-1 \\approx \\log n\\) for \\(n\\) large.\nSo, supposing that I want there to be at least one path in expectation (\\(q = 1\\)), I need to allow \\(k = \\frac{\\log n}{\\log c}\\). This is pretty short, actually! For example, the population of the world is about \\(8\\times 10^9\\), and Newman estimates that an average individual knows around 1,000 other people; that is, \\(c = 10^3\\) in the world social network. The resulting value of \\(k\\) here is around 3.3.\nIn other words, this calculation suggests that, if the world were an ER network, it would be the case that any two individuals would be pretty likely to have at least one path between them of length no longer than \\(4\\).\nMore formal calculations regarding the diameter of the ER graph confirm that the diameter of the ER graph grows slowly as a function of \\(n\\), even in relatively sparse cases.\n\n\nA Caveat\nIf you spend some time looking at Equation 9.1, you might find yourself wondering:\n\nHey, what happens if \\(c \\leq 1\\)?\n\nIndeed, something very interesting happens here. Let’s assume \\(c &lt; 1\\) (i.e. we’re ignoring the case \\(c = 1\\)), and estimate the expected number of paths between \\(i\\) and \\(j\\) of any length. Using Equation 9.1, we get\n\\[\n\\mathbb{E}\\left[\\sum_{k = 1}^{\\infty} R(k)\\right] = \\sum_{k = 1}^\\infty c^{k-1}p = \\sum_{k = 0}^\\infty c^kp = \\frac{p}{1-c}\\;.\n\\]\nIf we now use Markov’s inequality,  we find that the probability that there is a path of any length between nodes \\(i\\) and \\(j\\) is no larger than \\(\\frac{p}{1-c}.\\) In the sparse regime, we can substitute \\(p = \\frac{c}{n-1}\\) to see thatMarkov’s inequality states that \\(\\mathbb{P}(X \\geq a) \\leq \\frac{\\mathbb{E}(X)}{a}\\).\n\\[\n\\frac{c}{(1-c)(n-1)}\\rightarrow 0 \\text{ as } n \\to \\infty\\;.\n\\]\nSo, this suggests that, if \\(c &lt; 1\\), any two nodes are likely to be disconnected! On the other hand, if \\(c &gt; 1\\), we’ve argued that we can make \\(k\\) large enough to have high probability of a path of length \\(k\\) between those nodes.\nSo, what’s special about \\(c = 1\\)? This question brings us to one of the first and most beautiful results in the theory of random graphs. To get there, let’s study in a bit more detail the sizes of the connected components of the ER graph.",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Graphs: Erdős–Rényi</span>"
    ]
  },
  {
    "objectID": "source/09-random-graphs.html#component-sizes-and-the-branching-process-approximation",
    "href": "source/09-random-graphs.html#component-sizes-and-the-branching-process-approximation",
    "title": "9  Random Graphs: Erdős–Rényi",
    "section": "Component Sizes and the Branching Process Approximation",
    "text": "Component Sizes and the Branching Process Approximation\nWe’re now going to ask ourselves about the size of a “typical” component in the Erdős–Rényi model. In particular, we’re going to be interested in whether there exists a component that fills up “most” of the graph, or whether components tend to be vanishingly small in relation to the overall graph size.\nOur first tool for thinking about this question is the branching process approximation. Informally, a branching process is a process of random generational growth. We’ll get to a formal mathematical definition in a moment, but the easiest way to get insight is to look at a diagram:\n\n\n\n\n\n\n\nFigure 9.2: Image source.\n\n\n\nWe start with a single entity, \\(X_0\\). Then, \\(X_0\\) has a random number of “offspring”: \\(X_1\\) in total. Then, each of those \\(X_1\\) offspring has some offspring of their own; the total number of these offspring is \\(X_2\\). The process continues infinitely, although there is always a chance that at some point no more offspring are produced. In this case, we often say that the process “dies out.”\n\n\nSome of this exposition in this section draws on these notes by David Aldous.\n\n\n\n\n\n\n\nDefinition 9.4 (Branching Process) Let \\(p\\) be a probability distribution on \\(\\mathbb{Z}\\), called the offspring distribution.\nA branching process with distribution \\(p\\) is a sequence of random variables \\(X_0, X_1,,X_2\\ldots\\) such that \\(X_0 = 1\\) and, for \\(t \\geq 1\\),\n\\[\nX_t = \\sum_{i = 1}^{X_{t-1}} Y_i\\;,\n\\]\nwhere each \\(Y_i\\) is distributed i.i.d. according to \\(p\\).\n\n\n\n\n\n\nTechnically, this is a Galton-Watson branching process, named after the two authors who first proposed it (Watson and Galton 1875).   History note: Galton, one of the founders of modern statistics, was a eugenicist. The cited paper is explicit about its eugenicist motivation: the guiding question was about whether certain family names associated with well-to-do aristocrats were giving way to less elite surnames.\n\nApplication to Erdős–Rényi\nBranching processes create trees – graphs without cycles. The reason that branching processes are helpful when thinking about Erdős–Rényi models is that cycles are rare in Erdős–Rényi random graphs. So, if we can understand the behavior of branching processes, then we can learn something about the Erdős–Rényi random graph as well.\nHere’s the particular form of the branching process approximation that we will use:\n\n\n\n\n\n\n\nDefinition 9.5 (Branching Process Approximation for ER Component Sizes) Sample a single node \\(j\\) at random from a large, sparse ER graph with mean degree \\(c\\), and let \\(S\\) be the size (number of nodes) of the component in which \\(j\\) lies. Note that \\(S\\) is random: it depends both on \\(j\\) and on the realization of the ER graph.\nThen, \\(S\\) is distributed approximately as \\(T\\), where \\(T = \\sum_{i = 0}^{\\infty}X_t\\) is the total number of offspring in a GW branching process with offspring distribution \\(\\text{Poisson}(c)\\).\n\n\n\n\nThe idea behind this approximation is:\n\nWe start at \\(j\\), whose number of neighbors \\(\\sim \\text{Poisson}(c)\\).\nEach of these neighbors has a number of new neighbors \\(\\sim \\text{Poisson}(c)\\), and so on.\nWe keep visiting new neighbors until we run out, and add up the number of neighbors we’ve visited to obtain \\(S\\).\n\nSince cycles are rare in ER, we are unlikely to double-count any nodes (doing so would create a cycle), and so this whole process also approximately describes \\(T\\) in a branching process with a \\(\\text{Poisson}(c)\\) offpsring distribution.\n\n\n\n\nThe Subcritical Case\nThe mean of a \\(\\text{Poisson}(c)\\) random variable is again \\(c\\). As you’ll show in homework, this implies that \\(X_t\\), the number of offspring in generation \\(t\\), satisfies \\(\\mathbb{E}[X_t] = c^{t}\\). It follows that, when \\(c &lt; 1\\), \\(\\mathbb{E}[T] = \\frac{1}{1-c}\\).\nNow using Markov’s inequality, we obtain the following results:\n\n\n\n\n\n\nIn a \\(\\text{Poisson}(c)\\) branching process with \\(c &lt; 1\\), \\[\\mathbb{P}(X_t &gt; 0) \\leq c^t\\;.\\]\n\n\n\nSo, the probability that the branching process hasn’t yet “died out” decays exponentially with timestep \\(t\\). In other words, the branching process becomes very likely to die out very quickly.\n\n\n\n\n\n\nIn a \\(\\text{Poisson}(c)\\) branching process with \\(c &lt; 1\\), \\[\\mathbb{P}(T \\geq a) \\leq \\frac{1}{a}\\frac{1}{1-c}\\]\n\n\n\nIn particular, for \\(a\\) very large, we are guaranteed that \\(\\mathbb{P}(T &gt; a)\\) is very small.\nSumming up, when \\(c &lt; 1\\), the GW branching process dies out quickly and contains a relatively small number of nodes: \\(\\frac{1}{1-c}\\) in expectation. In this setting, the branching process is called subcritical.\n\nBack to ER\nIf we now translate back to the Erdős–Rényi random graph, the branching process approximation now suggests the following heuristic:\n\n\n\n\n\n\nHeuristic: In a sparse ER random graph with mean \\(c &lt; 1\\), the expected size of a component containing a randomly selected node is roughly \\(\\frac{1}{1-c}\\).\nIn particular, since this quantity is independent of \\(n\\), we find that the fraction of the graph occupied by this component is \\(\\frac{1}{n}\\frac{1}{1-c}\\) and therefore vanishes as \\(n\\rightarrow \\infty\\).\nWe can also turn this into a statement about probabilities: Markov’s inequality implies that, if \\(S\\) is the size of a component containing a randomly selected node,\n\\[\n\\mathbb{P}(S/n &gt; a) \\rightarrow 0\n\\]\nfor any constant \\(a &gt; 0\\). In other words, for large \\(n\\), the largest component is always vanishingly small in relation to the graph as a whole.\n\n\n\nLet’s check this experimentally. The following code block computes the size of the component in an ER graph containing a random node, and averages the result across many realizations. The experimental result is quite close to the theoretical prediction.\n\n\nShow code\nimport networkx as nx\nimport numpy as np\n\ndef component_size_of_node(n, c):\n    G = nx.fast_gnp_random_graph(n, c/(n-1))\n    return len(nx.node_connected_component(G, 1))\n\nc = 0.8\nsizes = [component_size_of_node(5000, c) for i in range(1000)]\n\nout = f\"\"\"\nAverage over experiments is {np.mean(sizes):.2f}.\\n\nTheoretical expectation is {1/(1-c):.2f}.\n\"\"\"\n\nprint(out)\n\n\n\nAverage over experiments is 5.05.\n\nTheoretical expectation is 5.00.\n\n\n\nNote that the expected (and realized) component size is very small, even though the graph contains 5,000 nodes!\nFor this reason, we say that subcritical ER contains only small connected components, in the sense that each component contains approximately 0% of the graph as \\(n\\) grows large.\nThis explains our result from earlier about path lengths. The probability that any two nodes have a path between them is the same as the probability that they are on the same connected component. But if every connected component is small, then the probability that two nodes occupy the same one is vanishes.\n\n\n\nThe Giant Component\n\n\n\n\n\n\n\nDefinition 9.6 (Giant Component) We say that \\(G(n,p)\\) has a giant component if \\[\n\\mathbb{P}(S/n &gt; a) \\rightarrow b\n\\] for some constant \\(b &gt; 0\\).\nIntuitively, this means that there is a possibility of a connected component that takes up a nonzero fraction of the graph.\n\n\n\n\nSo far, we’ve argued using the branching process approximation that there is no giant component in the Erdős–Rényi model with \\(c &lt; 1\\). The theory of branching processes also suggests to us that there could be a giant component when \\(c &gt; 1\\).\nThe proof of this fact is usually done in terms of generating functions and is beyond our scope, but you can check Wikipedia for an outline.\n\n\n\n\n\n\nFact: when \\(c &gt; 1\\), there is a nonzero probability that the \\(\\text{Poisson}(c)\\) branching process continues forever; that is, never goes extinct.\n\n\n\nUsing our correspondence between components of the ER model and branching processes, this suggests that, if we pick a random node, the component it is in has the potential to be very large. In fact (and this requires some advanced probability to prove formally), when \\(c &gt; 1\\), there is a giant component. This is our first example of a phase transition. It is also possible to prove that, with high probability, there is only one giant component; Newman does this in 11.5.1.\n\n\n\n\n\n\n\nDefinition 9.7 (Phase Transition) A phase transition is a qualitative change in response to a small variation in a quantitative parameter.\n\n\n\n\nExamples of phase transitions include freezing, in which a liquid undergoes a qualitative change into a solid in response to a small variation in temperature.\nFigure 9.3 shows two sparse ER random graphs on either side of the \\(c = 1\\) transition. We observe an apparent change in qualitative behavior between the two cases.\n\n\nShow code\nimport networkx as nx\nfrom matplotlib import pyplot as plt\n\nfig, axarr = plt.subplots(1, 2)\n\nn = 500\nc = [0.7, 1.3]\n\nfor i in range(2):\n    G = nx.fast_gnp_random_graph(n, c[i]/(n-1))\n    nx.draw(G, ax = axarr[i], node_size = 1)\n    axarr[i].set(title = f\"c = {c[i]}\")\n\n\n\n\n\n\n\n\nFigure 9.3\n\n\n\n\n\n\nSize of the Giant Component\nPerhaps surprisingly, while it’s difficult to prove that there is a giant component, it’s not hard at all to estimate its size. This argument is reproduced from Newman, pages 349-350\nLet \\(S\\) be the size of the giant component in an Erdős–Rényi random graph, assuming there is one. Then, \\(s = S/n\\) is the probability that a randomly selected node is in the giant component. Let \\(u = 1 - s\\) be the probability that a given node is not in the giant component.\nLet’s take a random node \\(i\\), and ask it the probability that it’s in the giant component. Well, one answer to that question is just “\\(u\\).” On the other hand, we can also answer that question by looking at \\(i\\)’s neighbors. If \\(i\\) is not in the giant component, then it can’t be connected to any node that is in the giant component. So, for each other node \\(j\\neq i\\), it must be the case that either:\n\n\\(i\\) is not connected to \\(j\\). This happens with probability \\(1-p\\).\n\\(i\\) is connected to \\(j\\), but \\(j\\) is not in the giant component either. \\(i\\) is connected to \\(j\\) with probability \\(p\\), and \\(j\\) is not in the giant component with probability \\(u\\).\n\nThere are \\(n-1\\) nodes other than \\(i\\), and so the probability that \\(i\\) is not connected to any other node in the giant component is \\((1 - p + pu)^{n-1}\\). We therefore have the equation\n\\[\nu = (1 - p + pu)^{n-1}\\;.\n\\]\nLet’s take the righthand side and use \\(p = c/(n-1)\\): \\[\n\\begin{aligned}\n    u &= (1 - p(1-u))^{n-1} \\\\\n      &= \\left(1 - \\frac{c(1-u)}{n-1}\\right)^{n-1}\\;.\n\\end{aligned}\n\\] This is a good time to go back to precalculus and remember the limit definition of the function \\(e^x\\): \\[\ne^x = \\lim_{n \\rightarrow \\infty}\\left(1 + \\frac{x}{n}\\right)^{n}\\;.\n\\] Since we are allowing \\(n\\) to grow large in our application, we approximate\n\\[\nu \\approx e^{-c(1-u)}\\;.\n\\] So, now we have a description of the fraction of nodes that aren’t in the giant component. We can get a description of how many nodes are in the giant component by substituting \\(s = 1-u\\), after which we get the equation we’re really after: \\[\ns = 1- e^{-cs}\n\\tag{9.2}\\]\nThis equation doesn’t have a closed-form solution for \\(s\\), but we can still plot it and compare the result to simulations (Figure 9.4). Not bad!\n\n\nShow code\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport networkx as nx\n\n# experiment: compute the size of the largest connected \n# component as a function of graph size for a range of mean degrees. \n\ndef largest_component(n, p):\n    G = nx.fast_gnp_random_graph(n, p)\n    S = max(nx.connected_components(G), key=len)\n    return len(S) / n\n\nn = 50000\nC = np.repeat(np.linspace(0.5, 1.5, 11), 10)\nU = np.array([largest_component(n, c/(n-1)) for c in C])\n\n# theory: prediction based on Newman 11.16\n\nS = np.linspace(-.001, .6, 101)\nC_theory = -np.log(1-S)/S\n\n# plot the results to compare\n\nplt.plot(C_theory, \n         S, \n         color = \"black\", \n         label = \"Theoretical prediction\")\n\nplt.scatter(C, \n            U, \n            label = \"Experiment\")\n\nplt.gca().set(xlabel = \"Mean degree\", \n              ylabel = \"Proportion of graph in largest component\")\n\nplt.legend()\n\n\n\n\n\n\n\n\nFigure 9.4\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nErdős, Paul, and Alfréd Rényi. 1960. “On the Evolution of Random Graphs.” Publ. Math. Inst. Hung. Acad. Sci 5 (1): 17–60.\n\n\nRiordan, Oliver, and Nicholas Wormald. 2010. “The Diameter of Sparse Random Graphs.” Combinatorics, Probability and Computing 19 (5-6): 835–926.\n\n\nWatson, Henry William, and Francis Galton. 1875. “On the Probability of the Extinction of Families.” The Journal of the Anthropological Institute of Great Britain and Ireland 4: 138–44.\n\n\nWatts, Duncan J, and Steven H Strogatz. 1998. “Collective Dynamics of ‘Small-World’networks.” Nature 393 (6684): 440–42.",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Random Graphs: Erdős–Rényi</span>"
    ]
  },
  {
    "objectID": "source/10-configuration-model.html",
    "href": "source/10-configuration-model.html",
    "title": "10  Configuration models",
    "section": "",
    "text": "Types of configuration models\nSimilar to \\(G(n,p)\\) models, we can understand a configuration model by understanding what is fixed (and what is random).",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Configuration models</span>"
    ]
  },
  {
    "objectID": "source/10-configuration-model.html#references",
    "href": "source/10-configuration-model.html#references",
    "title": "10  Configuration models",
    "section": "References",
    "text": "References\n\n\n\n\nBollobás, Béla. 1980. “A Probabilistic Proof of an Asymptotic Formula for the Number of Labelled Regular Graphs.” European Journal of Combinatorics 1 (4): 311–16.\n\n\nFosdick, Bailey K, Daniel B Larremore, Joel Nishimura, and Johan Ugander. 2018. “Configuring Random Graph Models with Fixed Degree Sequences.” Siam Review 60 (2): 315–55.",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Configuration models</span>"
    ]
  },
  {
    "objectID": "source/10-configuration-model.html#types-of-configuration-models",
    "href": "source/10-configuration-model.html#types-of-configuration-models",
    "title": "10  Configuration models",
    "section": "",
    "text": "Fixing a degree sequence\nOne way to set up a configuration model is to fix a degree sequence \\(\\{k_1, \\dots, k_n\\}\\). Note that this also fixes the number of edges: \\[\n    m = \\frac{1}{2} \\sum_j k_j \\,.\n\\]\n\n\n\n\n\n\n\nDefinition 10.1 Consider a graph with \\(n\\) nodes. Let \\({\\bf k} \\in \\mathbb{N}^n\\) be the degree sequence, where \\(k_i\\) is the degree of node \\(i\\).\nA configuration model is a uniform distribution over graphs with degree sequence \\({\\bf k}.\\)\n\n\n\n\nThis strategy is particularly useful as a degree-preserving null model for an empirical graph.\nThe classic algorithmic strategy for sampling from such a configuration model is through stub-matching. Recall that each edge has two ends of edges, or stubs. This means that node \\(i\\) has \\(k_i\\) stubs.\n\nChoose two stubs uniformly at random and connect these stubs to form an edge.\nChoose another pair from the remaining unmatched stubs; repeat the process until all stubs are matched.\n\nThe stub-matching algorithm can produce multiedges and self-loops, which can cause the graph to not be simple. However, Bollobás (1980) proved that, when the graph is sparse, the expected number of multi-edges and self-loops does not grow with network size. One can, as a result, show these structures are rare, and can often be ignored in arguments. Another challenge with this algorithm is we require an even number of stubs to get a graph realization that exactly matches the degree sequence.\nFosdick et al. (2018) discuss the subtleties that arise due to whether we allow or disallow multiedges and self-loops and whether we choose to label stubs distinctly (as opposed to only labeling vertices). These choices result in different spaces of graphs from which we are sampling.\n\n\nFixing a degree distribution\nWhen making mathematical arguments, it may be important to sample from a particular degree distribution \\(p_k\\) instead of sampling graphs with a particular degree sequence \\(k\\).",
    "crumbs": [
      "Models of Networks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Configuration models</span>"
    ]
  }
]