[
  {
    "objectID": "source/02-degree-walks-paths.html",
    "href": "source/02-degree-walks-paths.html",
    "title": "3  Degree, Walks, and Paths",
    "section": "",
    "text": "Degree\nUnsurprisingly, degree is directly related to the number of edges in an undirected network.\nIn the code below we\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is a coding exercise in the live notes.\n\nG = nx.florentine_families_graph()\n\nnx.draw(G, with_labels = True)\n\n# Create and print a numpy array of node degrees directly from the adjacency matrix\n\n#---\nA = nx.adjacency_matrix(G)\n\nprint(np.sum(A, axis = 1))\n\n#---\n\n# Print the degrees using the built in function in NetworkX. \n\n#---\ndegree_vec = nx.degree(G)\n\nprint(degree_vec)\n\n#----\n\n[1 6 3 3 4 2 3 3 3 2 1 3 4 1 1]\n[('Acciaiuoli', 1), ('Medici', 6), ('Castellani', 3), ('Peruzzi', 3), ('Strozzi', 4), ('Barbadori', 2), ('Ridolfi', 3), ('Tornabuoni', 3), ('Albizzi', 3), ('Salviati', 2), ('Pazzi', 1), ('Bischeri', 3), ('Guadagni', 4), ('Ginori', 1), ('Lamberteschi', 1)]\n\n\n\n\n\n\n\n\nFigure 3.1: A graph of the marriage relations of fifteenth-century Florentine families ((breiger1968cumulated?)).\nWe have to be a little more subtle in how we define degree in a directed network because there is a distinction between in-edges and out-edges in these networks.\nWe will repeat the exercises above for directed networks.\nSome special cases of regular graphs are lattices (e.g., a square lattice is 4-regular) and the complete graph where every node is connected to every other node (which is \\((n-1)\\)-regular).",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#degree",
    "href": "source/02-degree-walks-paths.html#degree",
    "title": "3  Degree, Walks, and Paths",
    "section": "",
    "text": "Definition 3.1 The degree of a node in an undirected network is the number of edges connected to it. The degree of node \\(i\\) is, equivalently, \\[\n    k_i = \\sum_{j=1}^n A_{ij} \\,.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse degree to calculate the total number of edges in an undirected network.\n\n\n\nSolution. The key here is to notice that degree counts ends of edges (sometimes we call these stubs). This means the total number of stubs will be the sum of the degrees of all the nodes. However, each edge is counted twice (each edge has two stubs), so the number of edges \\(m\\) in an undirected network is\n\\[\n    m = \\frac{1}{2}\\sum_{i=1^n}k_i = \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}\\,.\n\\]\nThis relationship between degree and number of edges is a useful fact!\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the mean degree of a node in an undirected network.\n\n\n\nSolution. Let \\(c\\) represent the mean (or expected) degree of a node in an undirected network. Using the previous exercise, \\[\\begin{align}\n    c &= \\frac{1}{n} \\sum_{i = 1}^n k_i \\,, \\\\\n    &= \\frac{2m}{n} \\,.\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.2 In a directed network, the in-degree is the number of ingoing edges to a node and the out-degree is the number of outgoing edges. That is, the in-degree is defined to be \\[\n    k_i^{\\text{in}} = \\sum_{j=1}^n A_{ij}\n\\] and the out-degree is \\[\n    k_j^{\\text{out}} = \\sum_{i=1}^n A_{ij}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse degree to calculate the total number of edges in a directed network, and use this to calculate the mean (expected) in-degree and out-degree of a directed network.\n\n\n\nSolution. The number of edges in a directed network is equal to the total number of ingoing (respectively, outgoing) ends of edges, so,\n\\[\n    m = \\sum_{i=1}^n k_i^{\\text{in}} = \\sum_{j=1}^n k_i^{\\text{out}} = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\,.\n\\]\nThis means that the expected in-degree and expected out-degree are also equal:\n\\[\\begin{align}\n    c_{\\text{in}} &= \\frac{1}{n} \\sum_{i=1}^n k_i^{\\text{in}} \\\\\n     &= \\frac{1}{n}\\sum_{j=1}^n k_i^{\\text{out}} \\\\\n    &= c_{\\text{out}} \\\\\n    &= c \\,.\n\\end{align}\\]\nCombining these gives \\(c = \\frac{m}{n}\\).\nNotice that this differs by a factor of 2 from the case of undirected networks.\n\n\nCode to calculate degrees in directed networks\n\n\n\n\n\n\n\n\nDefinition 3.3 A network in which all nodes have the same degree is called a regular graph or regular network. A regular graph where all nodes have degree \\(k\\) is called \\(k\\)-regular.\n\n\n\n\n\n\nDensity and sparsity\n\n\n\n\n\n\n\nDefinition 3.4 The density or connectance \\(\\rho\\) of a simple network is the fraction of possible edges that are actually present. That is,\n\\[\n    \\rho = \\frac{\\text{number of edges}}{\\text{possible edges}} = \\frac{m}{\\binom{n}{2}} \\,.\n\\]\n\n\n\n\nOne way to interpret density is to think of it as a probability that a pair of nodes picked uniformly at random is connected by an edge.\nWe can rewrite density in terms of expected degree using our earlier exercises :This simplification comes from the binomial coefficient formula \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\). Also, we can notice the cool fact that \\(n \\choose 2\\) is equivalent to the sum of the first \\(n-1\\) integers!\n\\[\\begin{align}\n    \\rho &= \\frac{m}{\\binom{n}{2}} \\\\\n    &= \\frac{m}{\\frac{1}{2}n(n-1)} \\\\\n    &= \\frac{2m}{n(n-1)} \\\\\n    &= \\frac{c}{n-1} \\,.\n\\end{align}\\]\nIf a network is sufficiently large, you can approximate the density as \\(\\rho \\approx \\frac{c}{n}.\\)\n\nCode to calculate density\n\nWhile it’s pretty straightforward to calculate the density of a network, it’s more complicated to determine whether a network is dense or sparse. There isn’t a universally agreed upon threshold for density below which a real-world network would be considered sparse. However, we can create a definition which applies for certain theoretical models of networks. If we have a model where we can take a formal limit, then such a network is sparse if \\(\\rho \\to 0\\) as \\(n \\to \\infty\\). In this scenario, the mean degree grows (much) more slowly than the number of nodes.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#references",
    "href": "source/02-degree-walks-paths.html#references",
    "title": "3  Degree, Walks, and Paths",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html",
    "href": "source/31-power-laws.html",
    "title": "5  Power Law Degree Distributions",
    "section": "",
    "text": "Introduction\nLast time, we studied several properties of real-world networks and compared them to randomized models of those networks in which the degrees were held constant. That is, we were looking at aspects of the structure of real-world networks that are not captured by the degree distribution alone. But what about the degree distribution itself? Do real world networks have degree distributions that are especially interesting? What models can account for the degree distributions that we observe?\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\nimport numpy as np\nfrom scipy.special import factorial\nimport pandas as pd\nimport random\nTo observe a degree distribution, let’s take a look at a data set collected from the streaming platform Twitch by Rozemberczki, Allen, and Sarkar (2021). Nodes are users on Twitch. An edge exists between them if they are mutual friends on the platform. The authors collected data sets for users speaking several different languages; we’ll use the network of English-speaking users. Let’s now download the data (as a Pandas data frame) and convert it into a graph using Networkx.\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ENGB_edges.csv\"\nedges = pd.read_csv(url)\nG = nx.from_pandas_edgelist(edges, \"from\", \"to\", create_using=nx.Graph)\n\nnum_nodes = G.number_of_nodes()\nnum_edges = G.number_of_edges()\n\nprint(f\"This graph has {num_nodes} nodes and {num_edges} edges. The mean degree is {2*num_edges/num_nodes:.1f}.\")\n\nThis graph has 7126 nodes and 35324 edges. The mean degree is 9.9.\ndef degree_sequence(G):\n    degrees = nx.degree(G)\n    degree_sequence = np.array([deg[1] for deg in degrees])\n    return degree_sequence\n\ndef log_binned_histogram(degree_sequence, interval = 5, num_bins = 20):\n    hist, bins = np.histogram(degree_sequence, bins = min(int(len(degree_sequence)/interval), num_bins))\n    bins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n    hist, bins = np.histogram(degree_sequence, bins = bins)\n    binwidths = bins[1:] - bins[:-1]\n    hist = hist / binwidths\n    p = hist/hist.sum()\n\n    return bins[:-1], p\n\ndef plot_degree_distribution(G, **kwargs):\n\n    deg_seq = degree_sequence(G)\n    x, p = log_binned_histogram(deg_seq, **kwargs)\n    plt.scatter(x, p,  facecolors='none', edgecolors =  'cornflowerblue', linewidth = 2, label = \"Data\")\n    plt.gca().set(xlabel = \"Degree\", xlim = (0.5, x.max()*2))\n    plt.gca().set(ylabel = \"Density\")\n    plt.gca().loglog()\n    plt.legend()\n    return plt.gca()\nLet’s use this function to inspect the degree distrubtion of the data:\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nThere are a few things to notice about this degree distribution. First, most nodes have relatively small degrees, fewer tham the mean of 9.9. However, there are a small number of nodes that have degrees which are much larger: almost two orders of magnitude larger! You can think of these nodes as “super stars” or “hubs” of the network; they may correspond to especially popular or influential accounts.\nRecall that, from our discussion of the Erdős–Rényi model \\(G(n,p)\\), the degree distribution of a \\(G(n,p)\\) model with mean degree \\(\\bar{d}\\) is approximately Poisson with mean \\(\\bar{d}\\). Let’s compare this Poisson distribution to the data.\ndeg_seq = degree_sequence(G)\nmean_degree  = deg_seq.mean()\nd_      = np.arange(1, 30, 1)\npoisson = np.exp(-mean_degree)*(mean_degree**d_)/factorial(d_)\n\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nax.plot(d_, poisson,  linewidth = 1, label = \"Poisson fit\", color = \"grey\", linestyle = \"--\")\nax.legend()\nComparing the Poisson fit predicted by the Erdős–Rényi model to the actual data, we see that the the Poisson places much higher probability mass on degrees that are close to the mean of 9.9. The Poisson would predict that almost no nodes would have degree higher than \\(10^2\\), while in the data there are several.\nWe often say that the Poisson has a “light right tail” – the probability mass allocated by the Poisson dramatically drops off as we move to the right of the mean. In contrast, the data itself appears to have a “heavy tail”: there is substantial probability mass even far to the right from the mean.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html#introduction",
    "href": "source/31-power-laws.html#introduction",
    "title": "5  Power Law Degree Distributions",
    "section": "",
    "text": "Let’s now define some helper functions to extract and visualize the degree distribution of a graph. Our first function extracts the degree distribution for easy computation, while the second creates a variable-width histogram in which each bin has the same width when plotted on a logarithmic horizontal axis. This is called logarithmic binning.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html#power-laws-as-models-of-heavy-tailed-distributions",
    "href": "source/31-power-laws.html#power-laws-as-models-of-heavy-tailed-distributions",
    "title": "5  Power Law Degree Distributions",
    "section": "Power Laws As Models of Heavy-Tailed Distributions",
    "text": "Power Laws As Models of Heavy-Tailed Distributions\nThere are many probability distributions that have heavy tails. By far the most important (and controversial) in the history of network science is the power law degree distribution.\n\n\n\n\n\n\n\nDefinition 5.1 (Power Law Distribution) A random variable \\(D\\) has a discrete power law distribution with cutoff \\(d_*\\) and exponent \\(\\gamma &gt; 1\\) if its probability mass function is has the form\n\\[\n\\begin{aligned}\n    p_d \\triangleq \\mathbb{P}[D = d] = C d^{-\\gamma}\\;,\n\\end{aligned}\n\\tag{5.1}\\]\nfor all \\(d &gt; d_*\\). Here, \\(C\\) is a normalizing constant that ensures that the distribution sums to \\(1\\). The entries of the distribution for \\(d \\leq d_*\\) are are arbitrary.\n\n\n\n\nAn important intuitive insight about power law distributions is that they are linear on log-log axes. To see this, we can take the logarithm of both sides in Equation 5.1:\n\\[\n\\begin{aligned}\n    \\log p_d = \\log C - \\gamma \\log d\\;.\n\\end{aligned}\n\\]\nSo, \\(\\log p_d\\) is a linear function of \\(\\log d\\) with slope \\(-\\gamma\\).\nLet’s try plotting such a distribution against the data. Since the power law distribution is defined for all \\(d &gt; d_*\\), we’ll need to choose a cutoff \\(d_*\\) and an exponent \\(\\gamma\\). For now, we’ll do this by eye. If we inspect the plot of the data above, it looks like linear behavior takes over somewhere around \\(d_* = 10^\\frac{3}{2} \\approx 30\\).\n\ndeg_seq = degree_sequence(G)\ncutoff  = 30\nd_      = np.arange(cutoff, deg_seq.max(), 1)\ngamma   = 2.7\npower_law = 18*d_**(-gamma)\n\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nax.plot(d_, power_law,  linewidth = 1, label = fr\"Power law: $\\gamma = {gamma}$\" , color = \"grey\", linestyle = \"--\")\nax.legend()\n\n\n\n\n\n\n\n\nThe power law appears to be a much better fit than the Poisson to the tail of the distribution, making apparently reasonable predictions about the numbers of nodes with very high degrees. Where do the parameters for the power law come from? Here we performed a fit “by eye”, but we discuss some systematic approaches in Section 5.5\nThe claim that a given network “follows a power law” is a bit murky: like other models, power laws are idealizations that no real data set matches exactly. The idea of a power law is also fundamentally asymptotic in nature: the power law that we fit to the data also predicts that we should see nodes of degree \\(10^3\\), \\(10^4\\), or \\(10^5\\) if we were to allow the network to keep growing. Since the network can’t keep growing (it’s data, we have a finite amount of it), we have to view the power law’s predictions about very high-degree nodes as extrapolations toward an idealized, infinite-size data set to which we obviously do not have access.\n\nStatistical Properties of Power Laws\nPower law distributions are much more variable than, say, Poisson distributions. Indeed, when \\(\\gamma \\leq 3\\), the variance of a power law distribution is infinite. To show this, we calculate the second moment of the distribution:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[D^2] &= \\sum_{d = 1}^\\infty p_d d^2 \\\\\n                    &= \\sum_{d = 1}^{d_*} p_d d^2 + \\sum_{d = d_* + 1}^\\infty d^2 C d^{-\\gamma} \\\\\n                    &= K + C \\sum_{d = d_*}^\\infty d^{2-\\gamma}\\;,\n\\end{aligned}\n\\]\nwhere \\(K\\) is a (finite) constant. Now we draw upon a fact from calculus: the sequence \\(\\sum_{x = 1}^\\infty x^{-p}\\) converges if and only if \\(p &gt; 1\\). This means that the sum \\(\\sum_{d = d_*}^\\infty d^{2-\\gamma}\\) converges if and only if \\(2 - \\gamma &lt; -1\\), which requires \\(\\gamma &gt; 3\\). If \\(\\gamma \\leq 3\\), the second moment (and therefore the variance) is undefined. Heuristically, this means that the distribution can generate samples which are are arbitrarily far from the mean, much as the data above includes samples which are orders of magnitude higher than the mean.\n\n\nPurported Universality of Power Laws\nBarabási and Albert (1999) made the first published that power-law degree distributions are very common in real-world networks. This initial paper has since been cited over 46,000 times (as of August 2024). Many subsequent studies have fit power-law tails to degree distributions in empirical networks across social, technological, and biological domains.  The idea that power laws are common in real-world networks is sometimes called the “scale-free hypothesis.” Part of the appeal of this hypothesis comes from statistical physics and complexity science, where power law distributions are common signatures of self-organizing systems.Clauset, Shalizi, and Newman (2009), Broido and Clauset (2019) and Holme (2019) provide some review and references for these claims.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html#the-preferential-attachment-model-a-generative-model-for-power-law-degrees",
    "href": "source/31-power-laws.html#the-preferential-attachment-model-a-generative-model-for-power-law-degrees",
    "title": "5  Power Law Degree Distributions",
    "section": "The Preferential Attachment Model: A Generative Model for Power Law Degrees",
    "text": "The Preferential Attachment Model: A Generative Model for Power Law Degrees\nWhy would power-law degree distributions be common in empirical networks? A common way to answer questions like this is to propose a generative model. A generative model is a random network model that is intended to produce some kind of realistic structure.  If the mechanism proposed by the generative model is plausible as a real, common mechanism, then we might expect that the large-scale structure generated by that model would be commonly observed.Generative models contrast with null models like the \\(G(n,p)\\) model, which are usually used to contrast with real networks. Generative models are models of what the data is like; null models are models of what the data is not like.\nBarabási and Albert (1999) are responsible for popularizing the claim that many empirical networks are scale-free. Alongside this claim, they offered a generative model called preferential attachment. The preferential attachment model offers a simple mechanism of network growth which leads to power-law degree distributions. Their model is closely related to the regrettably much less famous models of Yule (1925), Simon (1955), and Price (1976).\nHere’s how the Yule-Simon-Price-Barabási-Albert model works. First, we start off with some initial graph \\(G_0\\). Then, in each timestep \\(t=1,2,\\ldots\\), we: The model of Barabási and Albert (1999) did not include a uniform selection mechanism, which corresponds to the case \\(\\alpha = 1\\).\n\nFlip a coin with probability of heads equal to \\(\\alpha\\). If this coin lands heads, then:\n\nChoose a node \\(u\\) from \\(G_{t-1}\\) with probability proportional to its degree.\n\nOtherwise, if the coin lands tails, choose a node \\(u\\) from \\(G_{t-1}\\) uniformly at random.\nAdd a node \\(v\\) to \\(G_{t-1}\\).\nAdd edge \\((u,v)\\) to \\(G_{t-1}\\).\n\nWe repeat this process as many times as desired. Intuitively, the preferential attachment model expresses the idea that “the rich get richer”: nodes that already have many connections are more likely to receive new connections.\nHere’s a quick implementation. Note: this implementation of preferential attachment is useful for illustrating the mathematics and operations with Networkx. It is, however, not efficient.\n\n# initial condition\nG = nx.Graph() \nG.add_edge(0, 1) \n\nalpha = 3/5 # proportion of degree-based selection steps\n\n# main loop\nfor _ in range(10000):\n    degrees = nx.degree(G)\n\n    # determine u using one of two mechanisms\n    if np.random.rand() &lt; alpha: \n        deg_seq = np.array([deg[1] for deg in degrees])\n        degree_weights = deg_seq / deg_seq.sum()\n        u = np.random.choice(np.arange(len(degrees)), p = degree_weights)\n    else: \n        u = np.random.choice(np.arange(len(degrees)))\n\n    # integer index of new node v\n    v = len(degrees)\n\n    # add new edge to graph    \n    G.add_edge(u, v)\n\nLet’s go ahead and plot the result. We’ll add a visualization of the exponent \\(\\gamma\\) as well. How do we know the right value of \\(\\gamma\\)? It turns out that there is a theoretical estimate based on \\(\\alpha\\) which we’ll derive in the next section.\n\ndeg_seq = degree_sequence(G)\ncutoff  = 10\nd_      = np.arange(cutoff, deg_seq.max(), 1)\ngamma   = (1 + alpha) / alpha\npower_law = .15*d_**(-gamma)\n\nax = plot_degree_distribution(G, interval = 2, num_bins = 30)\nax.plot(d_, power_law,  linewidth = 1, label = fr\"Power law: $\\gamma = {gamma:.2f}$\" , color = \"grey\", linestyle = \"--\")\nax.legend()\n\n\n\n\n\n\n\n\nThis fit is somewhat noisy, reflecting the fact that we simulated a relatively small number of preferential attachment steps.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html#analyzing-preferential-attachment",
    "href": "source/31-power-laws.html#analyzing-preferential-attachment",
    "title": "5  Power Law Degree Distributions",
    "section": "Analyzing Preferential Attachment",
    "text": "Analyzing Preferential Attachment\nLet’s now see if we can understand mathematically why the preferential attachment model leads to networks with power-law degree distributions. There are many ways to demonstrate this fact, including both “casual” and highly rigorous techniques. Here, we’ll use a “casual” argument from Mitzenmacher (2004).\nLet \\(p_d^{(t)}\\) be the proportion of nodes of degree \\(d \\geq 2\\) after algorithmic timestep \\(t\\). Suppose that at this timestep there are \\(n\\) nodes and \\(m\\) edges. Then, the total number of nodes of degree \\(d\\) is \\(n_d^{(t)} = np_d^{(t)}\\). Suppose that we do one step of the preferential attachment model. Let’s ask: what will be the new expected value of \\(n_d^{(t+1)}\\)?\nWell, in the previous timestep there were \\(n_d^{(t)}\\) nodes of degree \\(d\\). How could this quantity change? There are two processes that could make \\(n_d^{(t+1)}\\) different from \\(n_d^{(t)}\\). If we selected a node \\(u\\) with degree \\(d-1\\) in the model update, then this node will become a node of degree \\(d\\) (since it will have one new edge attached to it), and will newly count towards the total \\(n_d^{(t+1)}\\). On the other hand, if we select a node \\(u\\) of degree \\(d\\), then this node will become a node of degree \\(d+1\\), and therefore no longer count for \\(n_d^{(t+1)}\\).\nSo, we can write down our estimate for the expected value of \\(n_d^{(t+1)}\\).\n\\[\n\\begin{aligned}\n    \\mathbb{E}\\left[n_d^{(t+1)}\\right] - n_d^{(t)} = \\mathbb{P}[d_u = d-1] - \\mathbb{P}[d_u = d]\\;.\n\\end{aligned}\n\\]\nLet’s compute the probabilities appearing on the righthand side. With probability \\(\\alpha\\), we select a node from \\(G_t\\) proportional to its degree. This means that, if a specific node \\(u\\) has degree \\(d-1\\), the probability of picking \\(u\\) is\n\\[\n\\begin{aligned}\n    \\mathbb{P}[u \\text{ is picked}] = \\frac{d-1}{\\sum_{w \\in G} d_w} = \\frac{d-1}{2m^{(t)}}\\;.\n\\end{aligned}\n\\]\nOf all the nodew we could pick, \\(p_{d-1}^{(t)}n\\) of them have degree \\(d-1\\). So, the probability of picking a node with degree \\(d-1\\) is \\(n p_{d-1}^{(t)}\\frac{d-1}{2m}\\). On the other hand, if we flipped a tails (with probability \\(1-\\alpha\\)), then we pick a node uniformly at random; each one is equally probable and \\(p_{d-1}^{(t)}\\) of them have degree \\(d-1\\). So, in this case the probability is simply \\(p_{d-1}^{(t)}\\). Combining using the law of total probability, we have\n\\[\n\\begin{aligned}\n    \\mathbb{P}[d_u = d-1] &= \\alpha n p_{d-1}^{(t)}\\frac{d-1}{2m^{(t)}} + (1-\\alpha)p_{d-1}^{(t)} \\\\\n                          &= \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)}\\;.\n\\end{aligned}\n\\]\nA similar calculation shows that\n\\[\n\\begin{aligned}\n    \\mathbb{P}[d_u = d] &= \\alpha n p_{d}^{(t)}\\frac{d}{2m^{(t)}} + (1-\\alpha)p_{d}^{(t)} \\\\\n                          &= \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;,\n\\end{aligned}\n\\]\nso our expectation is\n\\[\n\\begin{aligned}\n    \\mathbb{E}\\left[n_d^{(t+1)}\\right] - n_d^{(t)} = \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nUp until now, everything has been exact: no approximations involved. Now we’re going to start making approximations and assumptions. These can all be justified by rigorous probabilistic arguments, but we won’t do this here.\n\nWe’ll assume that \\(n_d^{(t+1)}\\) is equal to its expectation.\nIn each timestep, we add one new node and one new edge. This means that, after enough timesteps, the number of nodes \\(n\\) and number of edges \\(m\\) should be approximately equal. We’ll therefore assume that \\(t\\) is sufficiently large that \\(\\frac{n}{m} \\approx 1\\).\nStationarity: we’ll assume that, for sufficiently large \\(t\\), \\(p_d^{(t)}\\) is a constant: \\(p_d^{(t)} = p_d^{(t+1)} \\triangleq p_d\\).\n\nTo track these assumptions, we’ll use the symbol \\(\\doteq\\) to mean “equal under these assumptions.”\nWith these assumptions, we can simplify. First, we’ll replace \\(\\mathbb{E}\\left[n_d^{(t+1)}\\right]\\) with \\(n_d^{(t+1)}\\), which we’ll write as \\((n+1)p_d^{(t+1)}\\)\n\\[\n\\begin{aligned}\n    (n+1)p_d^{(t+1)} - np_d^{(t)} \\doteq \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nNext, we’ll assume \\(\\frac{n}{m} \\approx 1\\):\n\\[\n\\begin{aligned}\n    (n+1)p_d^{(t+1)} - np_d^{(t)} \\doteq \\left[\\alpha  \\frac{d-1}{2} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha \\frac{d}{2} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nFinally, we’ll assume stationarity:\n\\[\n\\begin{aligned}\n    (n+1)p_d - np_d^{(t)} \\doteq \\left[\\alpha  \\frac{d-1}{2} + (1-\\alpha)\\right]p_{d-1} - \\left[\\alpha \\frac{d}{2} + (1-\\alpha)\\right]p_{d}\\;.\n\\end{aligned}\n\\]\nAfter a long setup, this looks much more manageable! Our next step is to solve for \\(p_d\\), from which we find\n\\[\n\\begin{aligned}\n    p_d &\\doteq \\frac{\\alpha  \\frac{d-1}{2} + (1-\\alpha)}{1 + \\alpha \\frac{d}{2} + (1-\\alpha)} p_{d-1} \\\\\n    &= \\frac{2(1-\\alpha) + (d-1)\\alpha}{2(1-\\alpha) + 1 + d\\alpha }p_{d-1} \\\\\n    &= \\left(1 - \\frac{1 + \\alpha }{2(1-\\alpha) + 1 + d\\alpha }\\right)p_{d-1}\\;.\n\\end{aligned}\n\\] When \\(d\\) grows large, this expression is approximately \\[\n\\begin{aligned}\n    p_d \\simeq \\left(1 - \\frac{1}{d}\\frac{1+\\alpha}{\\alpha}\\right) p_{d-1}\\;.\n\\end{aligned}\n\\]\nNow for a trick “out of thin air.” As \\(d\\) grows large,\n\\[\n\\begin{aligned}\n    1 - \\frac{1}{d}\\frac{1+\\alpha}{\\alpha} \\rightarrow \\left(\\frac{d-1}{d} \\right)^{-\\frac{1 + \\alpha}{\\alpha}}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nJustify the approximation above. To do this, Taylor-expand the function \\(f(x) = x^{-\\gamma}\\) to first order around the point \\(x_0 = 1\\) and use this expansion to estimate the value of \\(1 + \\frac{1}{d}\\)\n\n\nApplying this last approximation, we have shown that, for sufficiently large \\(d\\),\n\\[\n\\begin{aligned}\n    p_d \\simeq \\left(\\frac{d-1}{d} \\right)^{\\frac{1 + \\alpha}{\\alpha}} p_{d-1}\\;.\n\\end{aligned}\n\\]\nThis recurrence relation, if it were exact, would imply that \\(p_d = C d^{\\frac{1+\\alpha}{\\alpha}}\\), as shown by the following exercise:\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that \\(p_d\\) is a probability distribution with the property that, for some \\(d_*\\) and for all \\(d &gt; d_*\\), it holds that\n\\[\n\\begin{aligned}\n    p_d = \\left(\\frac{d-1}{d}\\right)^{\\gamma} p_{d-1}\\;.\n\\end{aligned}\n\\]\nProve using induction that \\(p_d = C d^{-\\gamma}\\) for some constant \\(C\\), and explain how to compute \\(C\\).\n\n\n\nSolution. Our proof is by induction.\nBase Case: Since the statement is claimed to hold for all \\(d &gt; d_*\\), our base case is \\(d = d_* + 1\\). In this case we have \\(p_d = \\left(\\frac{d_*}{d}\\right)^{\\gamma} p_{d_*} = \\left[p_{d_*}d_*^{\\gamma}\\right]d^{-\\gamma}\\). We’ll let \\(C = p_{d_*}d_*^\\gamma\\).\nInductive Step: Suppose that the statement holds for some arbitrary \\(d = \\hat{d} &gt; d_*\\). We’ll show that the statement also holds for \\(d = \\hat{d} + 1\\). We can calculate directly\n\\[\n\\begin{aligned}\n    p_{\\hat{d} +  1} &= \\left(\\frac{\\hat{d}}{\\hat{d} + 1}\\right)^{\\gamma} p_{\\hat{d}} \\\\\n    &= C \\left(\\frac{\\hat{d}}{\\hat{d} + 1}\\right)^{\\gamma} \\hat{d}^{-\\gamma} \\\\\n    &= C (\\hat{d} + 1)^{-\\gamma}\\;.\n\\end{aligned}\n\\]\nThis completes the inductive step and the proof.\n\nThis concludes our argument. Although this argument contains many approximations, it is also possible to reach the same conclusion using fully rigorous probabilistic arguments (Bollobás et al. 2001).",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html#sec-power-law-estimation",
    "href": "source/31-power-laws.html#sec-power-law-estimation",
    "title": "5  Power Law Degree Distributions",
    "section": "Estimating Power Laws From Data",
    "text": "Estimating Power Laws From Data\nAfter the publication of Barabási and Albert (1999), there was a proliferation of papers purporting to find power-law degree distributions in empirical networks. For a time, the standard method for estimating the exponent \\(\\gamma\\) was to use the key visual signature of power laws – power laws are linear on log-log axes. This suggests performing linear regression in log-log space; the slope of the regression line is the estimate of \\(\\gamma\\). This approach, however, is badly flawed: errors can be large, and uncertainty quantification is not reliably available. Clauset, Shalizi, and Newman (2009) discuss this problem in greater detail, and propose an alternative scheme based on maximum likelihood estimation and goodness-of-fit tests. Although the exact maximum-likelihood estimate of \\(\\gamma\\) is the output of a maximization problem and is not available in closed form, the authors supply a relatively accurate approximation:\n\\[\n\\begin{aligned}\n    \\hat{\\gamma} = 1 + n\\left(\\sum_{i=1}^n \\log \\frac{d_i}{d_*}\\right)^{-1}\\;.\n\\end{aligned}\n\\]\nAs they show, this estimate and related methods are much more reliable estimators of \\(\\gamma\\) than the linear regression method.\nAn important cautionary note: the estimate \\(\\hat{\\gamma}\\) can be formed regardless of whether or not the power law is a good descriptor of the data. Supplementary methods such as goodness-of-fit tests are necessary to determine whether a power law is appropriate at all. Clauset, Shalizi, and Newman (2009) give some guidance on such methods as well.\n\nPreferential Attachment in Growing Graphs\nWhat if we are able to observe more than the degree distribution of a network? What if we could also observe the growth of the network, and actually know which edges were added at which times? Under such circumstances, it is possible to estimate more directly the extent to which a graph might grow via the preferential attachment mechanism, possibly alongside additional mechanisms. Overgoor, Benson, and Ugander (2019) supply details on how to estimate the parameters of a general class of models, including preferential attachment, from observed network growth.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/31-power-laws.html#are-power-laws-good-descriptors-of-real-world-networks",
    "href": "source/31-power-laws.html#are-power-laws-good-descriptors-of-real-world-networks",
    "title": "5  Power Law Degree Distributions",
    "section": "Are Power Laws Good Descriptors of Real-World Networks?",
    "text": "Are Power Laws Good Descriptors of Real-World Networks?\nAre power laws really that common in empirical data? Broido and Clauset (2019) controversially claimed that scale free networks are rare. In a bit more detail, the authors compare power-law distributions to several competing distributions as models of real-world network degree sequences. The authors find that that the competing models—especially lognormal distributions, which also have heavy tails—are often better fits to observed data than power laws. This paper stirred considerable controversy, which is briefly documented by Holme (2019).\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12. https://doi.org/10.1126/science.286.5439.509.\n\n\nBollobás, Bela, Oliver Riordan, Joel Spencer, and Gábor Tusnády. 2001. “The Degree Sequence of a Scale-Free Random Graph Process.” Random Structures & Algorithms 18 (3): 279–90. https://doi.org/10.1002/rsa.1009.\n\n\nBroido, Anna D., and Aaron Clauset. 2019. “Scale-Free Networks Are Rare.” Nature Communications 10 (1): 1017. https://doi.org/10.1038/s41467-019-08746-5.\n\n\nClauset, Aaron, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. “Power-Law Distributions in Empirical Data.” SIAM Review 51 (4): 661–703. https://doi.org/10.1137/070710111.\n\n\nHolme, Petter. 2019. “Rare and Everywhere: Perspectives on Scale-Free Networks.” Nature Communications 10 (1): 1016. https://doi.org/10.1038/s41467-019-09038-8.\n\n\nMitzenmacher, Michael. 2004. “A Brief History of Generative Models for Power Law and Lognormal Distributions.” Internet Mathematics 1 (2): 226–51. https://doi.org/10.1080/15427951.2004.10129088.\n\n\nOvergoor, Jan, Austin Benson, and Johan Ugander. 2019. “Choosing to Grow a Graph: Modeling Network Formation as Discrete Choice.” In The World Wide Web Conference, 1409–20. San Francisco CA USA: ACM. https://doi.org/10.1145/3308558.3313662.\n\n\nPrice, Derek de Solla. 1976. “A General Theory of Bibliometric and Other Cumulative Advantage Processes.” Journal of the American Society for Information Science 27 (5): 292–306.\n\n\nRozemberczki, Benedek, Carl Allen, and Rik Sarkar. 2021. “Multi-Scale Attributed Node Embedding.” Journal of Complex Networks 9 (2).\n\n\nSimon, Herbert A. 1955. “On a Class of Skew Distribution Functions.” Biometrika 42 (3-4): 425–40. https://doi.org/10.1093/biomet/42.3-4.425.\n\n\nYule. 1925. “A Mathematical Theory of Evolution, Based on the Conclusions of Dr. J. C. Willis, F. R. S.” Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character 213 (402-410): 21–87. https://doi.org/10.1098/rstb.1925.0002.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#densitysparsity",
    "href": "source/02-degree-walks-paths.html#densitysparsity",
    "title": "3  Degree, Walks, and Paths",
    "section": "Density/sparsity",
    "text": "Density/sparsity\n\n\n\n\n\n\n\nDefinition 3.4 The density or connectance \\(\\rho\\) of a simple network is the fraction of possible edges that are actually present. That is,\n\\[\n    \\rho = \\frac{\\text{number of edges}}{\\text{possible edges}} = \\frac{m}{\\binom{n}{2}} \\,.\n\\]\n\n\n\n\nOne way to interpret density is to think of it as a probability that a pair of nodes picked uniformly at random is connected by an edge.\nWe can rewrite density in terms of expected degree using our earlier exercises :This simplification comes from the binomial coefficient formula \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\). Also, we can notice the cool fact that \\(n \\choose 2\\) is equivalent to the sum of the first \\(n-1\\) integers!\n\\[\\begin{align}\n    \\rho &= \\frac{m}{\\binom{n}{2}} \\\\\n    &= \\frac{m}{\\frac{1}{2}n(n-1)} \\\\\n    &= \\frac{2m}{n(n-1)} \\\\\n    &= \\frac{c}{n-1} \\,.\n\\end{align}\\]\nIf a network is sufficiently large, you can approximate the density as \\(\\rho \\approx \\frac{c}{n}.\\)",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#walks-and-paths",
    "href": "source/02-degree-walks-paths.html#walks-and-paths",
    "title": "3  Degree, Walks, and Paths",
    "section": "Walks and paths",
    "text": "Walks and paths\nWe may like to know if it it is possible to reach one node from another by traversing edges. For this task, we introduce the notion of a walk.\n\nCode showing a walk\n\n\n\n\n\n\n\n\nDefinition 3.5 A walk of length \\(k \\geq 2\\) is a set of edges \\(\\{ (i_1,j_1), (i_2, j_2), \\dots, (i_k, j_k)\\}\\) with the property that \\(i_l = j_{l-1}\\) for each \\(2 \\leq l \\leq k\\). We say this is a walk from node \\(i_1\\) to node \\(j_k.\\)\nThe length of a walk is the number of edges it contains.\nA single edge \\((i,j)\\) is always considered a walk of length 1 from \\(i\\) to \\(j\\).\n\n\n\n\nA walk between two nodes is not always well-defined. Consider the example below, where there is no walk from BLAH to BLAH.\n\n(CODE for network with two components)\n\nA question that pops up a lot in network analysis is “How many walks of length \\(r\\) exist between nodes \\(i\\) and \\(j\\)?”\nThe adjacency matrix gives a concise way to address this question. First, let’s consider \\(r=1\\). That’s just the number of edges from node \\(j\\) to \\(i\\), which is exactly \\(A_{ij}\\). Said another way,\n\nThe \\(ij\\)th entry of \\({\\bf A}^1\\) counts the number of walks of length 1 from node \\(j\\) to node \\(i\\).\n\nThis observation generalizes by induction.\n\n\n\n\n\n\n\nTheorem 3.1 (Counting Walks) The \\(ij\\)th entry of the matrix \\({\\bf A}^r\\) contains the number of walks of length \\(r\\) from \\(j\\) to \\(i\\).\n\n\n\n\n\nProof. We proceed by induction on walk length \\(r\\). We discussed the base case above: that is, the \\(ij\\)th entry of the adjacency matrix \\({\\bf A}_{ij} =  {\\bf A}_{ij}^1\\) gives us walks of length 1, by definition.\nNow, suppose that \\({\\bf A}^r\\) gives the number of walks of length \\(r\\); we will show that \\({\\bf A}^{r+1}\\) gives the number of walks of length \\({r+1}\\). By definition, \\({\\bf A}^{r+1} = {\\bf A}^{r}{\\bf A}.\\) Thinking about matrix multiplication as an inner product, we see that the \\(ij\\) entry can be written\n\\[\n    A_{ij}^{r+1} = \\sum_{l=1}^n A_{il}^rA_{lj} \\,,\n\\]\nthat is, entry \\(ij\\) comes from summing the componentwise product of the \\(i\\)th row of \\({\\bf A}^r\\) with the \\(j\\)th column of \\({\\bf A}.\\)\nThe number of walks from node \\(j\\) to \\(i\\) of length \\(r+1\\) is equivalent to the number of walks of length \\(r\\) from \\(j\\) to \\(l\\) multiplied by the number of length \\(1\\) walks from \\(l\\) to \\(i\\), which is exactly the quantity we have written above. This completes the proof.\n\n\n\n\n\n\n\n\nDefinition 3.6 A path is a walk that is not self-intersecting. That is, any edge \\((i,j)\\) shows up in a path at most once.\nA geodesic path or shortest path is from \\(i\\) to \\(j\\) is a walk fom \\(i\\) to \\(j\\) of minimum length; i.e. a walk such that no other walk has shorter length.\nThe length of a geodesic path is called the (geodesic) distance between \\(i\\) and \\(j\\) If two nodes are not path-connected, their geodesic distance is undefined.\n\n\n\n\nRemarks\n\nShortest paths are not necessarily unique.\nShortest paths are self-avoiding. This is because if a shortest path intersected itself, this would create a loop which could be removed to create a shorter path.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#cyclic-and-acyclic-graphs",
    "href": "source/02-degree-walks-paths.html#cyclic-and-acyclic-graphs",
    "title": "3  Degree, Walks, and Paths",
    "section": "Cyclic and acyclic graphs",
    "text": "Cyclic and acyclic graphs\n\n\n\n\n\n\n\nDefinition 3.7 A cycle is a path from a node \\(i\\) to itself.\nA network with no cycles is acyclic.\n\n\n\n\nBy our definition, self-edges are cycles of length 1.\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the number of cycles of length \\(r\\) in a network starting and ending at node \\(i\\)?\n\n\n\nSolution. Since a cycle can be represented as a walk from a node \\(i\\) to itself, this is the diagonal element \\(A_{ii}^r\\) from the theorem we proved earlier.\nBe careful: this quantity separately counts cycles where the same nodes are visited in a different order. For example, the cycle \\(1 \\to 2 \\to 3 \\to 1\\) is counted separately as the cycle \\(1 \\to 3 \\to 2 \\to 1.\\) To count distinct cycles, you need to divide by the number of combinations.\n\nWhile the formula above is very useful to look for cycles of a specific length, it could be quite inefficient to use to detect whether we have a cycle of any length (because we may have to check the diagonal entries of \\({\\bf A}^r\\) for all possible cycle lengths \\(r\\)). We can construct a simple algorithm to determine computationally whether a network is cyclic or acyclic.\nUse the sketch below to write this algorithm.\n\nFind a node with no out-edges.\nIf no such node exists, the network is cyclic. Otherwise, remove the node and repeat the process.\nIf all nodes can be removed using the strategy above, the network is acyclic.\n\n\nCode block here\n\nThis algorithm has a nice mathematical consequence. If we label the nodes in an acyclic network according to the order we removed them, we will end up with an adjacency matrix that is strictly upper triangular.  There exists at least one such labeling for any acyclic network.This is because each node that is removed could only have out-edges that were already removed previously, i.e., nonzero entries of the \\(i\\)th column could only occur between columns 1 and \\(i-1\\).\n\nTrees\n\n\n\n\n\n\n\nDefinition 3.8 A tree is a connected, acyclic, undirected network. \n\n\n\n\nBy “connected”, we mean every node is reachable from every other node by traversing a sequence of edges, i.e., there exists a walk between any two nodesTrees are often drawn as rooted trees with a root node at the top and leaf nodes below.\nA few remarks:\n\nTopologically, the root of a tree is not unique\nAll trees are necessarily simple graphs, because self- and multiedges would create cycles.\n\nTrees play important roles, especially in math and computer science. Trees have several useful properties that we can exploit for network analysis:\n\nBecause trees have no cycles, there is exactly one path between any pair of nodes (as long as we don’t allow ``backtracking’’’). Many calculations on networks with this property are simple(r).\nA tree of \\(n\\) nodes has exactly \\(n-1\\) edges. Furthermore, any connected network with \\(n-1\\) edges and \\(n\\) nodes is a tree.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#exercise-2",
    "href": "source/02-degree-walks-paths.html#exercise-2",
    "title": "3  Degree, Walks, and Paths",
    "section": "",
    "text": "Use degree to calculate the total number of edges in a directed network, and use this to calculate the mean (expected) in-degree and out-degree of a directed network.",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  },
  {
    "objectID": "source/02-degree-walks-paths.html#computing",
    "href": "source/02-degree-walks-paths.html#computing",
    "title": "3  Degree, Walks, and Paths",
    "section": "Computing",
    "text": "Computing",
    "crumbs": [
      "Measuring Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Degree, Walks, and Paths</span>"
    ]
  }
]