[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in network science. The target audience for these notes are undergraduates in mathematics and computer science who have completed courses in linear algebra, discrete mathematics, and programming.\nThese notes are a collaborative project between Dr. Heather Zinn Brooks (Department of Mathematics, Harvey Mudd College) and Dr. Phil Chodrow (Department of Computer Science, Middlebury College).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction with students who are interested in and fluent in both mathematics and computation. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as Jupyter Notebooks which can be opened in Google Colab. Certain code components have been removed. The purpose is to facilitate live-coding in lectures.\n\n\nUse and Reuse\nThese notes are free to reuse and adapt for educational purposes. Attribution is appreciated but not required.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, the most important of which is Newman (2018).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages. We thank Mason Porter for inspiration in earlier versions of this course.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Network Science: Models, Mathematics, and Computation",
    "section": "References",
    "text": "References\n\n\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/01-networkrepresentations.html",
    "href": "chapters/01-networkrepresentations.html",
    "title": "1  Networks and their representations",
    "section": "",
    "text": "Introductory Graph Terminology\nWe’ll begin by introducing some terminology for types of graphs we may encounter.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "chapters/01-networkrepresentations.html#introductory-graph-terminology",
    "href": "chapters/01-networkrepresentations.html#introductory-graph-terminology",
    "title": "1  Networks and their representations",
    "section": "",
    "text": "Simple graphs\n\n\n\n\n\n\n\nDefinition 1.2 Edges that connect nodes to themselves are called self-edges or self-loops.\nIf there is more than one edge between the same two nodes, this is called a multiedge. A network with multiedges is called a multigraph.\n\n\n\n\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is left as a coding exercise in the live notes.\n\n\nG = nx.Graph()\nedgelist = [(1, 4), (2, 5), (3, 5), (4, 5), (5, 6)]\nG.add_edges_from(edgelist)\nnx.draw(G)\n\n\n\n\n\n\n\n\nFigure 1.2: A simple graph with 6 nodes and 5 edges.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 A network that has neither self-edges nor multiedges is called a simple graph or simple network.\n\n\n\n\nMany of the networks we’ll study this semester will be simple graphs.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is left as a coding exercise in the live notes.\n\n\nG = nx.Graph()\nG.add_edges_from([(1, 4), (2, 4), (3, 3), (3, 4)])\nnx.draw(G)\n\n\n\n\n\n\n\n\nFigure 1.3: A graph with self-edges.\n\n\n\n\n\n\n\nPlanar graphs\n\n\n\n\n\n\n\nDefinition 1.4 A planar graph is a graph that can be embedded in the plane without having any edges cross.\n\n\n\n\nPlanar graphs are commonly studied objects in graph theory and there are many cool theorems about them (see, for example, the four-color theorem or Kuratowski’s theorem). This means that a network that can be represented with a planar graph can leverage this theory.\n\n\nDirected graphs\nIn this course, we will spend much of our time focused on the analysis of undirected graphs, where an edge between nodes \\(i\\) and \\(j\\) is an unordered pair \\(\\{i,j\\}\\). However, there is an extension that can be important in many modeling contexts where there may be an edge from \\(j\\) to \\(i\\), but no edge from \\(i\\) to \\(j\\). This is called a directed graph. Informally, each edge in a directed graph has a direction, pointing from one node to another. Directed edges are usually represented by lines with arrows.\n\n\n\n\n\n\n\nDefinition 1.5 A directed graph (also called a directed network or digraph) is a graph in which each edge is an ordered pair \\((j, i)\\), which indicates an edge from node \\(j\\) to node \\(i\\). Such edges are called directed edges (or arcs).\n\n\n\n\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# This is left as a coding exercise in the live notes.\n\n\nDG = nx.DiGraph()\nDG.add_edges_from([(1, 2), (1, 3), (3, 4), (4, 1)])\n\nnx.draw(DG, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')\n\n\n\n\n\n\n\n\nFigure 1.4: A directed graph. We’ve chosen some arbitrary node labels for mathematical encoding later on in the notes.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "chapters/01-networkrepresentations.html#matrix-representations-of-graphs",
    "href": "chapters/01-networkrepresentations.html#matrix-representations-of-graphs",
    "title": "1  Networks and their representations",
    "section": "Matrix Representations of Graphs",
    "text": "Matrix Representations of Graphs\nOne reason that graphs are particularly useful mathematical representations of networks is that they can be encoded with matrices. This is a huge advantage, because we’ll be able to leverage a lot of theory we know from linear algebra.\nFor our definitions below, we’ll suppose we have a graph \\(G\\) with \\(n\\) vertices.We take the convention that the directed edge \\((j, i)\\) is an edge from \\(j\\) to \\(i\\). Notice that \\((j, i)\\) is represented in the \\(i\\)th row and the \\(j\\)th column of the adjacency matrix. As with so many things in math, this notation is a choice made for mathematical convenience. This choice also allows us to align with both the Newman (2018) textbook and NetworkX syntax. Be aware that different authors and sources might make a different choice!\n\nAdjacency matrix\n\n\n\n\n\n\n\nDefinition 1.6 The adjacency matrix \\(A\\) of a graph \\(G = (V, E)\\) is an \\(n \\times n\\) matrix where \\(n = \\vert V \\vert\\). Its entries are \\[\\begin{align*}\n    A_{ij} = \\begin{cases}\n    1 & (j, i) \\in E \\,, \\\\\n    0 & \\text{otherwise.}\n    \\end{cases}\n\\end{align*}\\]\n\n\n\n\nWe can make a few observations about how the graph structure relates to the structure of the adjacency matrix.\n\nIf there are no self-edges, the diagonal elements are all zero.\nIf the network is undirected, then an edge between \\(i\\) and \\(j\\) implies the existence of an edge between \\(j\\) and \\(i\\). This means that \\(A_{ji} = A_{ij}\\) and thus the adjacency matrix is symmetric.\nSimilarly, if the adjacency matrix is not symmetric, the network cannot be undirected.\n\n\nExample. The adjacency matrix for the graph in Figure 1.4 is \\[\n\\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}\n\\]\n\nIf we have a graph with self-edges, then \\(\\{i,i\\} \\in E\\) for some \\(i.\\) If the graph is undirected, we represent this in the adjacency matrix by setting \\(A_{ii} = 2.\\) If the graph is directed, we set \\(A_{ii} = 1.\\)This is another convention that will make the mathematics easier. An intuitive way to understand this choice is that, in undirected graphs, every edge “shows up” twice in the adjacency matrix, whereas in directed graphs every edge “shows up” once.\nIf we have a graph with multiedges, then we can set the corresponding matrix element \\(A_{ij}\\) equal to the multiplicity of the edge. For example, a double edge between nodes 2 and 3 in an undirected graph is represented \\(A_{23} = A_{32} = 2.\\)\nBut why stop there? Instead of requiring an integer number of edges between two nodes, we could extend this idea to form weighted networks with real-valued edge weights. Sometimes it is useful to represent edges as having a strength, weight, or value. In this situation, we set the value of \\(A_{ij}\\) equal to the weight of the corresponding edge \\((j, i)\\). For example, weights in an airport network could be used represent a distance between two airports, or weights in a social media network could represent the number of messages sent between two individuals.\n\n\nMany more matrices …\nThere are LOTS of matrices that can be associated to networks. There’s no “right” one — some are more useful than others for certain jobs. Throughout this course, we’ll see examples of matrices that are well-suited to certain specific tasks, like ranking or clustering. If you’re interested in searching around a bit, some other fun matrices are:\n\nThe graph Laplacian matrix and its variants.\nThe nonbacktracking or Hashimoto matrix.\nThe modularity matrix.\nThe random-walk transition matrix.\nThe PageRank matrix.\nThe node-edge incidence matrix.\n\nAnd the list goes on!",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "chapters/01-networkrepresentations.html#graphs-in-networkx",
    "href": "chapters/01-networkrepresentations.html#graphs-in-networkx",
    "title": "1  Networks and their representations",
    "section": "Graphs in NetworkX",
    "text": "Graphs in NetworkX\nIn this class, we will also be studying networks from a computational perspective. We will be using the NetworkX package in Python.\n\nCreating networks\nWe can create an empty undirected graph \\(G\\) using G = nx.Graph(). You could instead create an empty directed graph using nx.DiGraph() or an empty multigraph using nx.MultiGraph().\nThere are multiple ways to grow your graph in NetworkX.\n\nAdding nodes or edges manually\nYou can add one node at a time. For example,\n\nG.add_node(1)\n\nwill add a node with the label 1. We use an integer here, but a node can be any hashable Python object. You can add one edge at a time using tuples of nodes\n\nG.add_edge(1, 2)\n\nIf you create an edge connecting to a node that’s not in your graph yet, the node gets created automatically.\nIn most cases it’s pretty inefficient to add one node or edge at a time. Fortunately, you can also add nodes and edges from a list or any other iterable container:\n\nG.add_nodes_from(nodelist)\n\n\nG.add_edges_from(edgelist)\n\nThere are corresponding methods to remove nodes and edges: G.remove_node(), G.remove_edge(), G.remove_nodes_from(), G.remove_edges_from().\n\n\nCreating a graph dictionary\nYou can also build your graph using a dictionary that maps nodes to neighbors. The code below creates a graph with 3 nodes, where nodes 1 and 3 are both connected to node 2, but not to each other.\n\n\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\ngraph_dict = {1: [2], 2: [1, 3], 3:[2]}\nG = nx.Graph(graph_dict) \nnx.draw(G)\n\n\n\n\n\n\n\n\nFigure 1.5: A network built using a dictionary that maps nodes to neighbors.\n\n\n\n\n\n\n\nUsing an adjacency matrix\nYou can directly create a graph using a numpy array that encodes your adjacency matrix representation for your graph. The three-node example above has the adjacency matrix \\[\n\\begin{pmatrix}\n    0 & 1 & 0 \\\\\n    1 & 0 & 1 \\\\\n    0 & 1 & 0\n\\end{pmatrix}\n\\]\nwhich can be encoded after importing the numpy package as np as follows:\n\nA = np.array([[0, 1, 0], [1, 0, 0], [0, 1, 0]])\n\nYou can then quickly build an undirected graph \\(G\\) with\n\nG = nx.from_numpy_array(A)\n\nFor a directed graph, you need to make two additional changes to the code above. You need specify that you want a directed graph with create_using. Additionally, you need to be very careful about the adjacency matrix convention for in and out edges. The default convention for from_numpy_array points edges in the opposite direction to what we defined above (it assumes \\(A_{ij} = 1\\) if \\((i,j) \\in E\\)), so if you want to match that convention, you’ll need to take the transpose with np.transpose.\n\nG = nx.from_numpy_array(np.transpose(A), create_using = nx.DiGraph)\n\n\n\n\nVisualizing your networks\nYou can use the matplotlib plot interface to draw your network. If you have created a graph \\(G\\), you can quickly visualize it using\n\nnx.draw(G)\n\nThere are lots of fun ways to customize your figure, including changing colors, sizes, adding labels, and more. See the documentation for details.\nPractice with the following exercises:\n\nUsing NetworkX to reproduce Figure 1.2, Figure 1.3, Figure 1.4. Use at least two of the different methods described above.\nDraw the network represented by the adjacency matrix \\[\n\\begin{pmatrix}\n  0 & 1 & 0 & 0 & 1 \\\\\n  1 & 0 & 0 & 1 & 1 \\\\\n  0 & 0 & 0 & 1 & 0 \\\\\n  1 & 0 & 0 & 0 & 0 \\\\\n  1 & 0 & 0 & 0 & 0\n\\end{pmatrix} \\,.\n\\]\nCreate your own network.\n\n\n\nShow code\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n# Use NetworkX to reproduce the figures from the notes.\n# Then, create and visualize the network represented by the given adjacency matrix.\n# Finally, create your own network.\n# This is left as a coding exercise in the live notes.\n\n\nA = np.array([[0, 1, 0, 0, 1], [1, 0, 0, 1, 1], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0]])\nG = nx.from_numpy_array(np.transpose(A), create_using = nx.DiGraph)\nnx.draw(G, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')\n\n\n\n\n\n\n\n\nFigure 1.6: The network corresponding to the adjacency matrix given in the exercise.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "chapters/01-networkrepresentations.html#references",
    "href": "chapters/01-networkrepresentations.html#references",
    "title": "1  Networks and their representations",
    "section": "References",
    "text": "References\n\n\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.",
    "crumbs": [
      "Network Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Networks and their representations</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html",
    "href": "chapters/30-real-world.html",
    "title": "2  Structure of Empirical Networks",
    "section": "",
    "text": "Introduction\nIt’s all well and good to study the theoretical properties of hypothetical networks. In this set of notes, we’ll start addressing an important empirical question:\nOf course, there’s no simple answer to this question: we observe network data sets across a wide variety of domains, and many of them have different properties. For our purposes today, we’ll look at a sample of four networks:\nShow code\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\nimport numpy as np\nfrom scipy.special import factorial\nimport pandas as pd\nimport random\n\n# acquire twitch data\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ZHTW_edges.csv\"\nedges = pd.read_csv(url)\nG_twitch = nx.from_pandas_edgelist(edges, \"from\", \"to\", create_using=nx.Graph)\n\n# acquire chameleon data\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/chameleon_edges.csv\"\nedges = pd.read_csv(url)\nG_chameleon = nx.from_pandas_edgelist(edges, \"id1\", \"id2\", create_using=nx.Graph)\n\n# two built-in networkx data sets. \nG_karate = nx.karate_club_graph()\nG_les_mis = nx.les_miserables_graph()\nA fundamental principle in measuring networks is to compare. If we say that a network has a high value of some measurement \\(X\\), then the correct reply is:\nThe following code constructs a random synthetic counterpart graph for each of our empirical graphs. It then adds all of these to a dictionary so that we can easily access both the real and synthetic graphs later.\ndef random_counterpart(G):\n    degrees = [deg for (node, deg) in G.degree()]\n    # G_random = nx.configuration_model(degrees, create_using=nx.Graph)\n    G_random = nx.expected_degree_graph(degrees, selfloops=False)\n    G_random.remove_edges_from(nx.selfloop_edges(G_random))\n    return G_random\n\ndef add_to_dataset_dict(dataset_dict, G, name):\n\n    dataset_dict[name] = {\n        \"graph\" : G,\n        \"random\" : random_counterpart(G)\n    }\n    \ndataset_dict = {}\nadd_to_dataset_dict(dataset_dict, G_twitch, \"twitch\")\nadd_to_dataset_dict(dataset_dict, G_chameleon, \"chameleon\")\nadd_to_dataset_dict(dataset_dict, G_karate, \"karate\")\nadd_to_dataset_dict(dataset_dict, G_les_mis, \"les_mis\")\nTo help us compute and compare measurements on these graphs, we’ll define the following function which will manage these computations and organize the result as a table. This function takes as an argument a function fun which accepts a graph as an input and returns a scalar value.\ndef compute_metric(fun = lambda x: 0, compare = True):\n    print(\"Data Set\" + \" \" * 10 + \"Real\", end = \"\")\n    if compare: \n            print(\" \" * 10 + \"Random\")\n    else: \n        print()\n    print(\"-\" * 22, end = \"\")\n    if compare: \n        print(\"-\"*18)\n    else: \n        print()\n    for data_set in dataset_dict:\n        print(data_set + \" \" * (14 - len(data_set)) + f\"{fun(dataset_dict[data_set]['graph']):&gt;8.2f}\", end = \"\")\n        if compare:\n            print(\" \" * (8) + f\"{fun(dataset_dict[data_set]['random']):&gt;8.2f}\")\n        else: \n            print()",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#introduction",
    "href": "chapters/30-real-world.html#introduction",
    "title": "2  Structure of Empirical Networks",
    "section": "",
    "text": "What are real networks like?\n\n\n\ntwitch: A network of mutual friends on the Twitch streaming platform. The data set was collected by Rozemberczki, Allen, and Sarkar (2021).\nchameleon: A network of Wikipedia pages on topics related to chameleons (yes, the animal). An edge exists between two nodes if the corresponding Wikipedia pages link to each other. The data set was collected by Rozemberczki, Allen, and Sarkar (2021).\nkarate: The Zachary Karate Club social network (Zachary 1977), which is packaged with NetworkX.\nles_mis: A network of character interactions in the novel Les Miserables by Victor Hugo, also packaged with NetworkX.\n\n\n\n\nHigh compared to what?\n\nThere are many reasonable answers to this question, and we’ll explore several of them when we come to the study of random graphs. For now, we are going to compare each of our real networks to a synthetic random graph with a similar degree sequence. Technically, we are using a model that reproduces the degree sequence approximately and in expectation. This model is due to Chung and Lu (2002).",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#node-and-edge-counts",
    "href": "chapters/30-real-world.html#node-and-edge-counts",
    "title": "2  Structure of Empirical Networks",
    "section": "Node and Edge Counts",
    "text": "Node and Edge Counts\nLet’s start with something simple: how many nodes and edges are in each graph?\n\nprint(\"Number of nodes\")\ncompute_metric(lambda x: x.number_of_nodes())\nprint(\"\\nNumber of edges\")\ncompute_metric(lambda x: x.number_of_edges())\n\nNumber of nodes\nData Set          Real          Random\n----------------------------------------\ntwitch         2772.00         2772.00\nchameleon      2277.00         2277.00\nkarate           34.00           34.00\nles_mis          77.00           77.00\n\nNumber of edges\nData Set          Real          Random\n----------------------------------------\ntwitch        63462.00        61673.00\nchameleon     31421.00        31088.00\nkarate           78.00           75.00\nles_mis         254.00          253.00\n\n\nAlthough the number of nodes agree exactly in the real and random networks, there are some small discrepancies in the edge counts. This is due to the fact that our procedure for constructing random graphs (a) only preserves the degrees in expectation rather than exactly and (b) can create some self-loops, which get discarded.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#clustering-coefficient",
    "href": "chapters/30-real-world.html#clustering-coefficient",
    "title": "2  Structure of Empirical Networks",
    "section": "Clustering Coefficient",
    "text": "Clustering Coefficient\nLet’s move on to something more complex. Take a moment and think of two of your friends, whom we’ll call \\(A\\) and \\(B\\). Are \\(A\\) and \\(B\\) themselves friends with each other? If they do, then we say that there is a triad or triangle in the network.\n\n\n\n\n\n\nA triangle in a social network\n\n\n\nA stylized fact about many networks—especially social networks—is that triangles like these are common. In order to validate this stylized fact, we need to (a) determine how to measure the prevalence of triangles and (b) compare the value of this measure on our real networks to that of their random counterparts.\nThere are many possible measures of the prevalence of triangles, but here we will use the transitivity: the fraction of all possible triangles that are present in the network. The formula for transitivity is\n\\[\n\\begin{aligned}\n    T(G) = \\frac{\\mathrm{trace}(\\mathbf{A}^3)}{\\sum_{i} k_i(k_i - 1)}\n\\end{aligned}\n\\]\nHere, \\(\\mathbf{A}\\) is the adjacency matrix of \\(G\\) and \\(k_i\\) is the degree of node \\(i\\).\nThe numerator of this expression is proportional to the number of triangles in the network (technically, it is off by a factor of 6) and the denominator is proportional to the number of paths of length two in the network. You can think of a triplet as a possible triangle: just add one more edge and a triangle forms. bn\n\n\n\n\n\n\nA triplet in a social network centered on you\n\n\n\nLet’s write a function to compute the transitivity of a graph.\n\ndef my_transitivity(G):\n    A = nx.adjacency_matrix(G).A\n    A = 1*(A &gt;= 1) # convert to unweighted form\n\n    # numerator\n    num_triangles = np.trace(A @ A @ A)\n\n    # denominator\n    degrees = A.sum(axis = 0)\n    num_triplets = np.sum(degrees * (degrees - 1))\n\n    return num_triangles / num_triplets\n\nLet’s compare our function to the built-in function supplied by NetworkX.\n\nprint(my_transitivity(G_karate))\nprint(nx.transitivity(G_karate))\n\n0.2556818181818182\n0.2556818181818182\n\n\n/Users/philchodrow/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/scipy/sparse/_base.py:752: VisibleDeprecationWarning:\n\nPlease use `.todense()` instead\n\n\n\nLooks good! We’ll move forward with the NetworkX version, as it is substantially faster on larger graphs.\n\ncompute_metric(nx.transitivity)\n\nData Set          Real          Random\n----------------------------------------\ntwitch            0.12            0.15\nchameleon         0.31            0.12\nkarate            0.26            0.28\nles_mis           0.50            0.23\n\n\nWe observe that the chameleon and les_mis graphs appear to have substantially greater transitivity than their random counterparts, while both karate and twitch have similar transitivity to their random counterparts. Under this comparison, some networks indeed display very high transitivity.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#connected-components",
    "href": "chapters/30-real-world.html#connected-components",
    "title": "2  Structure of Empirical Networks",
    "section": "Connected Components",
    "text": "Connected Components\nWhat about the number of connected components in the network?\n\ncompute_metric(lambda x: len(list(nx.connected_components(x))))\n\nData Set          Real          Random\n----------------------------------------\ntwitch            1.00           57.00\nchameleon         1.00           70.00\nkarate            1.00            2.00\nles_mis           1.00            6.00\n\n\nRecall that we’ve engineered all of our real networks to have only one connected component, filtering if necessary. On the other hand, the random networks tend to have multiple connected components.\nWould it be fair to say that real networks are more connected than would be expected at random? Some caution is required here. Many researchers collect network data using methods that are especially likely to produce connected networks. For example, snowball sampling in study design refers to the method of recruiting participants for a survey or other instrument by asking people to recommend their friends. Since they can’t recommend people they don’t know, the snowball sample collected from an individual is always connected. Similarly, data sets like the chameleon data set are constructed by following links from one Wikipedia page to another. This method always produces a connected network as well. So, while it is true that many network data sets contain a single connected component, this is often an artifact of data collection rather than a fundamental property of the network.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#degree-degree-correlations",
    "href": "chapters/30-real-world.html#degree-degree-correlations",
    "title": "2  Structure of Empirical Networks",
    "section": "Degree-Degree Correlations",
    "text": "Degree-Degree Correlations\nWe have constructed random counterpart networks that have similar degree sequences to the real networks we are studying. Networks, however, can have interesting degree structures beyond just the degree sequence. One such structure is the degree-assortativity. The degree assortativity measures the extent to which nodes of similar degree are connected to each other. There are several ways to measure degree assortativity, but the most common one (due to Newman (2018)) has formula\n\\[\n\\begin{aligned}\n    C = \\frac{\\sum_{(u,v) \\in G} k_u k_v - \\frac{1}{m}\\left(\\sum_{(u,v) \\in G}  k_u\\right)^2}{\\sum_{(u,v) \\in G} k_u^2 - \\frac{1}{m}\\left(\\sum_{(u,v) \\in G}  k_u\\right)^2}\\;.\n\\end{aligned}\n\\]\nIf you are familiar with probability and statistics, this formula is equivalent to \\(C = \\frac{\\mathrm{cov}(K_1,K_2)}{\\sqrt{\\mathrm{var}(K_1)\\mathrm{var}(K_2)}}\\), where \\(K_1\\) and \\(K_2\\) are the degrees of the nodes at the ends of an edge selected uniformly at random from \\(G\\). This is also the Pearson correlation coefficient between \\(K_1\\) and \\(K_2\\).\nAn assortative network (with high assortativity) is one in which nodes of high degree tend to connect to each other frequently. A disassortative network (with negative assortativity) is one in which nodes of high degree tend to connect to nodes of low degree. Let’s take a look at the assortativity values in our networks:\n\ncompute_metric(nx.degree_assortativity_coefficient)\n\nData Set          Real          Random\n----------------------------------------\ntwitch           -0.23           -0.10\nchameleon        -0.20           -0.06\nkarate           -0.48           -0.26\nles_mis          -0.17           -0.08\n\n\nIt looks like all of our networks are disassortative, and somewhat moreso than their random counterparts. Disassortativity is a common feature of many networks, and it is often attributed to the presence of hubs in the network. Hubs are nodes with very high degree, and they tend to connect to many other nodes. Since there are only a few hubs, they are more likely to connect to nodes of low degree than to other hubs.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#shortest-path-lengths",
    "href": "chapters/30-real-world.html#shortest-path-lengths",
    "title": "2  Structure of Empirical Networks",
    "section": "Shortest Path Lengths",
    "text": "Shortest Path Lengths\nIn a famous study, Stanley Milgram (1967) 1 asked participants to ensure that a letter reached a target person, whom they did not know, in a US city. However, the participants were only allowed to send the letter to someone they knew on a first-name basis. That person could then send the letter to another person they knew on a first-name basis, and so on, until the letter was delivered (or lost). Perhaps surprisingly, many participants were able to reach the target person in only a few steps, on average. This experiment is the origin of the famous phrase six degrees of separation: in many social networks, most individuals are separated by relatively few links, even when the network is very large.\n1 Yes, that Milgram (Milgram 1963).To test this in our networks, we’ll compute the length of the shortest path between a pair of nodes, averaged across all possible pairs. This quantity isn’t defined for the random counterpart networks that have multiple disconnected components (why?), and so we’ll stick to calculating it on the real-world networks.\n\ncompute_metric(nx.average_shortest_path_length, compare = False)\n\nData Set          Real\n----------------------\ntwitch            2.43\nchameleon         3.56\nkarate            2.41\nles_mis           2.64\n\n\nIndeed, despite some of these networks having thousands of nodes and edges, the average shortest path length does not exceed 4 links. We’ll consider some theoretical models that aim to explain this phenomenon later in the course.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/30-real-world.html#references",
    "href": "chapters/30-real-world.html#references",
    "title": "2  Structure of Empirical Networks",
    "section": "References",
    "text": "References\n\n\n\n\nChung, Fan, and Linyuan Lu. 2002. “Connected Components in Random Graphs with Given Expected Degree Sequences.” Annals of Combinatorics 6 (2): 125–45.\n\n\nMilgram, Stanley. 1963. “Behavioral Study of Obedience.” The Journal of Abnormal and Social Psychology 67 (4): 371.\n\n\n———. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67.\n\n\nNewman, Mark. 2018. Networks. Oxford University Press.\n\n\nRozemberczki, Benedek, Carl Allen, and Rik Sarkar. 2021. “Multi-Scale Attributed Node Embedding.” Journal of Complex Networks 9 (2).\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure of Empirical Networks</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html",
    "href": "chapters/31-power-laws.html",
    "title": "3  Power Law Degree Distributions",
    "section": "",
    "text": "Introduction\nLast time, we studied several properties of real-world networks and compared them to randomized models of those networks in which the degrees were held constant. That is, we were looking at aspects of the structure of real-world networks that are not captured by the degree distribution alone. But what about the degree distribution itself? Do real world networks have degree distributions that are especially interesting? What models can account for the degree distributions that we observe?\nfrom matplotlib import pyplot as plt\nimport networkx as nx\nplt.style.use('seaborn-v0_8-whitegrid')\nimport numpy as np\nfrom scipy.special import factorial\nimport pandas as pd\nimport random\nTo observe a degree distribution, let’s take a look at a data set collected from the streaming platform Twitch by Rozemberczki, Allen, and Sarkar (2021). Nodes are users on Twitch. An edge exists between them if they are mutual friends on the platform. The authors collected data sets for users speaking several different languages; we’ll use the network of English-speaking users. Let’s now download the data (as a Pandas data frame) and convert it into a graph using Networkx.\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ENGB_edges.csv\"\nedges = pd.read_csv(url)\nG = nx.from_pandas_edgelist(edges, \"from\", \"to\", create_using=nx.Graph)\n\nnum_nodes = G.number_of_nodes()\nnum_edges = G.number_of_edges()\n\nprint(f\"This graph has {num_nodes} nodes and {num_edges} edges. The mean degree is {2*num_edges/num_nodes:.1f}.\")\n\nThis graph has 7126 nodes and 35324 edges. The mean degree is 9.9.\ndef degree_sequence(G):\n    degrees = nx.degree(G)\n    degree_sequence = np.array([deg[1] for deg in degrees])\n    return degree_sequence\n\ndef log_binned_histogram(degree_sequence, interval = 5, num_bins = 20):\n    hist, bins = np.histogram(degree_sequence, bins = min(int(len(degree_sequence)/interval), num_bins))\n    bins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n    hist, bins = np.histogram(degree_sequence, bins = bins)\n    binwidths = bins[1:] - bins[:-1]\n    hist = hist / binwidths\n    p = hist/hist.sum()\n\n    return bins[:-1], p\n\ndef plot_degree_distribution(G, **kwargs):\n\n    deg_seq = degree_sequence(G)\n    x, p = log_binned_histogram(deg_seq, **kwargs)\n    plt.scatter(x, p,  facecolors='none', edgecolors =  'cornflowerblue', linewidth = 2, label = \"Data\")\n    plt.gca().set(xlabel = \"Degree\", xlim = (0.5, x.max()*2))\n    plt.gca().set(ylabel = \"Density\")\n    plt.gca().loglog()\n    plt.legend()\n    return plt.gca()\nLet’s use this function to inspect the degree distrubtion of the data:\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nThere are a few things to notice about this degree distribution. First, most nodes have relatively small degrees, fewer tham the mean of 9.9. However, there are a small number of nodes that have degrees which are much larger: almost two orders of magnitude larger! You can think of these nodes as “super stars” or “hubs” of the network; they may correspond to especially popular or influential accounts.\nRecall that, from our discussion of the Erdős–Rényi model \\(G(n,p)\\), the degree distribution of a \\(G(n,p)\\) model with mean degree \\(\\bar{d}\\) is approximately Poisson with mean \\(\\bar{d}\\). Let’s compare this Poisson distribution to the data.\ndeg_seq = degree_sequence(G)\nmean_degree  = deg_seq.mean()\nd_      = np.arange(1, 30, 1)\npoisson = np.exp(-mean_degree)*(mean_degree**d_)/factorial(d_)\n\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nax.plot(d_, poisson,  linewidth = 1, label = \"Poisson fit\", color = \"grey\", linestyle = \"--\")\nax.legend()\nComparing the Poisson fit predicted by the Erdős–Rényi model to the actual data, we see that the the Poisson places much higher probability mass on degrees that are close to the mean of 9.9. The Poisson would predict that almost no nodes would have degree higher than \\(10^2\\), while in the data there are several.\nWe often say that the Poisson has a “light right tail” – the probability mass allocated by the Poisson dramatically drops off as we move to the right of the mean. In contrast, the data itself appears to have a “heavy tail”: there is substantial probability mass even far to the right from the mean.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html#introduction",
    "href": "chapters/31-power-laws.html#introduction",
    "title": "3  Power Law Degree Distributions",
    "section": "",
    "text": "Let’s now define some helper functions to extract and visualize the degree distribution of a graph. Our first function extracts the degree distribution for easy computation, while the second creates a variable-width histogram in which each bin has the same width when plotted on a logarithmic horizontal axis. This is called logarithmic binning.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html#power-laws-as-models-of-heavy-tailed-distributions",
    "href": "chapters/31-power-laws.html#power-laws-as-models-of-heavy-tailed-distributions",
    "title": "3  Power Law Degree Distributions",
    "section": "Power Laws As Models of Heavy-Tailed Distributions",
    "text": "Power Laws As Models of Heavy-Tailed Distributions\nThere are many probability distributions that have heavy tails. By far the most important (and controversial) in the history of network science is the power law degree distribution.\n\n\n\n\n\n\n\nDefinition 3.1 (Power Law Distribution) A random variable \\(D\\) has a discrete power law distribution with cutoff \\(d_*\\) and exponent \\(\\gamma &gt; 1\\) if its probability mass function is has the form\n\\[\n\\begin{aligned}\n    p_d \\triangleq \\mathbb{P}[D = d] = C d^{-\\gamma}\\;,\n\\end{aligned}\n\\tag{3.1}\\]\nfor all \\(d &gt; d_*\\). Here, \\(C\\) is a normalizing constant that ensures that the distribution sums to \\(1\\). The entries of the distribution for \\(d \\leq d_*\\) are are arbitrary.\n\n\n\n\nAn important intuitive insight about power law distributions is that they are linear on log-log axes. To see this, we can take the logarithm of both sides in Equation 3.1:\n\\[\n\\begin{aligned}\n    \\log p_d = \\log C - \\gamma \\log d\\;.\n\\end{aligned}\n\\]\nSo, \\(\\log p_d\\) is a linear function of \\(\\log d\\) with slope \\(-\\gamma\\).\nLet’s try plotting such a distribution against the data. Since the power law distribution is defined for all \\(d &gt; d_*\\), we’ll need to choose a cutoff \\(d_*\\) and an exponent \\(\\gamma\\). For now, we’ll do this by eye. If we inspect the plot of the data above, it looks like linear behavior takes over somewhere around \\(d_* = 10^\\frac{3}{2} \\approx 30\\).\n\ndeg_seq = degree_sequence(G)\ncutoff  = 30\nd_      = np.arange(cutoff, deg_seq.max(), 1)\ngamma   = 2.7\npower_law = 18*d_**(-gamma)\n\nax = plot_degree_distribution(G, interval = 10, num_bins = 30)\nax.plot(d_, power_law,  linewidth = 1, label = fr\"Power law: $\\gamma = {gamma}$\" , color = \"grey\", linestyle = \"--\")\nax.legend()\n\n\n\n\n\n\n\n\nThe power law appears to be a much better fit than the Poisson to the tail of the distribution, making apparently reasonable predictions about the numbers of nodes with very high degrees. Where do the parameters for the power law come from? Here we performed a fit “by eye”, but we discuss some systematic approaches in Section 3.5\nThe claim that a given network “follows a power law” is a bit murky: like other models, power laws are idealizations that no real data set matches exactly. The idea of a power law is also fundamentally asymptotic in nature: the power law that we fit to the data also predicts that we should see nodes of degree \\(10^3\\), \\(10^4\\), or \\(10^5\\) if we were to allow the network to keep growing. Since the network can’t keep growing (it’s data, we have a finite amount of it), we have to view the power law’s predictions about very high-degree nodes as extrapolations toward an idealized, infinite-size data set to which we obviously do not have access.\n\nStatistical Properties of Power Laws\nPower law distributions are much more variable than, say, Poisson distributions. Indeed, when \\(\\gamma \\leq 3\\), the variance of a power law distribution is infinite. To show this, we calculate the second moment of the distribution:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[D^2] &= \\sum_{d = 1}^\\infty p_d d^2 \\\\\n                    &= \\sum_{d = 1}^{d_*} p_d d^2 + \\sum_{d = d_* + 1}^\\infty d^2 C d^{-\\gamma} \\\\\n                    &= K + C \\sum_{d = d_*}^\\infty d^{2-\\gamma}\\;,\n\\end{aligned}\n\\]\nwhere \\(K\\) is a (finite) constant. Now we draw upon a fact from calculus: the sequence \\(\\sum_{x = 1}^\\infty x^{-p}\\) converges if and only if \\(p &gt; 1\\). This means that the sum \\(\\sum_{d = d_*}^\\infty d^{2-\\gamma}\\) converges if and only if \\(2 - \\gamma &lt; -1\\), which requires \\(\\gamma &gt; 3\\). If \\(\\gamma \\leq 3\\), the second moment (and therefore the variance) is undefined. Heuristically, this means that the distribution can generate samples which are are arbitrarily far from the mean, much as the data above includes samples which are orders of magnitude higher than the mean.\n\n\nPurported Universality of Power Laws\nBarabási and Albert (1999) made the first published that power-law degree distributions are very common in real-world networks. This initial paper has since been cited over 46,000 times (as of August 2024). Many subsequent studies have fit power-law tails to degree distributions in empirical networks across social, technological, and biological domains.  The idea that power laws are common in real-world networks is sometimes called the “scale-free hypothesis.” Part of the appeal of this hypothesis comes from statistical physics and complexity science, where power law distributions are common signatures of self-organizing systems.Clauset, Shalizi, and Newman (2009), Broido and Clauset (2019) and Holme (2019) provide some review and references for these claims.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html#the-preferential-attachment-model-a-generative-model-for-power-law-degrees",
    "href": "chapters/31-power-laws.html#the-preferential-attachment-model-a-generative-model-for-power-law-degrees",
    "title": "3  Power Law Degree Distributions",
    "section": "The Preferential Attachment Model: A Generative Model for Power Law Degrees",
    "text": "The Preferential Attachment Model: A Generative Model for Power Law Degrees\nWhy would power-law degree distributions be common in empirical networks? A common way to answer questions like this is to propose a generative model. A generative model is a random network model that is intended to produce some kind of realistic structure.  If the mechanism proposed by the generative model is plausible as a real, common mechanism, then we might expect that the large-scale structure generated by that model would be commonly observed.Generative models contrast with null models like the \\(G(n,p)\\) model, which are usually used to contrast with real networks. Generative models are models of what the data is like; null models are models of what the data is not like.\nBarabási and Albert (1999) are responsible for popularizing the claim that many empirical networks are scale-free. Alongside this claim, they offered a generative model called preferential attachment. The preferential attachment model offers a simple mechanism of network growth which leads to power-law degree distributions. Their model is closely related to the regrettably much less famous models of Yule (1925), Simon (1955), and Price (1976).\nHere’s how the Yule-Simon-Price-Barabási-Albert model works. First, we start off with some initial graph \\(G_0\\). Then, in each timestep \\(t=1,2,\\ldots\\), we: The model of Barabási and Albert (1999) did not include a uniform selection mechanism, which corresponds to the case \\(\\alpha = 1\\).\n\nFlip a coin with probability of heads equal to \\(\\alpha\\). If this coin lands heads, then:\n\nChoose a node \\(u\\) from \\(G_{t-1}\\) with probability proportional to its degree.\n\nOtherwise, if the coin lands tails, choose a node \\(u\\) from \\(G_{t-1}\\) uniformly at random.\nAdd a node \\(v\\) to \\(G_{t-1}\\).\nAdd edge \\((u,v)\\) to \\(G_{t-1}\\).\n\nWe repeat this process as many times as desired. Intuitively, the preferential attachment model expresses the idea that “the rich get richer”: nodes that already have many connections are more likely to receive new connections.\nHere’s a quick implementation. Note: this implementation of preferential attachment is useful for illustrating the mathematics and operations with Networkx. It is, however, not efficient.\n\n# initial condition\nG = nx.Graph() \nG.add_edge(0, 1) \n\nalpha = 3/5 # proportion of degree-based selection steps\n\n# main loop\nfor _ in range(10000):\n    degrees = nx.degree(G)\n\n    # determine u using one of two mechanisms\n    if np.random.rand() &lt; alpha: \n        deg_seq = np.array([deg[1] for deg in degrees])\n        degree_weights = deg_seq / deg_seq.sum()\n        u = np.random.choice(np.arange(len(degrees)), p = degree_weights)\n    else: \n        u = np.random.choice(np.arange(len(degrees)))\n\n    # integer index of new node v\n    v = len(degrees)\n\n    # add new edge to graph    \n    G.add_edge(u, v)\n\nLet’s go ahead and plot the result. We’ll add a visualization of the exponent \\(\\gamma\\) as well. How do we know the right value of \\(\\gamma\\)? It turns out that there is a theoretical estimate based on \\(\\alpha\\) which we’ll derive in the next section.\n\ndeg_seq = degree_sequence(G)\ncutoff  = 10\nd_      = np.arange(cutoff, deg_seq.max(), 1)\ngamma   = (1 + alpha) / alpha\npower_law = .15*d_**(-gamma)\n\nax = plot_degree_distribution(G, interval = 2, num_bins = 30)\nax.plot(d_, power_law,  linewidth = 1, label = fr\"Power law: $\\gamma = {gamma:.2f}$\" , color = \"grey\", linestyle = \"--\")\nax.legend()\n\n\n\n\n\n\n\n\nThis fit is somewhat noisy, reflecting the fact that we simulated a relatively small number of preferential attachment steps.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html#analyzing-preferential-attachment",
    "href": "chapters/31-power-laws.html#analyzing-preferential-attachment",
    "title": "3  Power Law Degree Distributions",
    "section": "Analyzing Preferential Attachment",
    "text": "Analyzing Preferential Attachment\nLet’s now see if we can understand mathematically why the preferential attachment model leads to networks with power-law degree distributions. There are many ways to demonstrate this fact, including both “casual” and highly rigorous techniques. Here, we’ll use a “casual” argument from Mitzenmacher (2004).\nLet \\(p_d^{(t)}\\) be the proportion of nodes of degree \\(d \\geq 2\\) after algorithmic timestep \\(t\\). Suppose that at this timestep there are \\(n\\) nodes and \\(m\\) edges. Then, the total number of nodes of degree \\(d\\) is \\(n_d^{(t)} = np_d^{(t)}\\). Suppose that we do one step of the preferential attachment model. Let’s ask: what will be the new expected value of \\(n_d^{(t+1)}\\)?\nWell, in the previous timestep there were \\(n_d^{(t)}\\) nodes of degree \\(d\\). How could this quantity change? There are two processes that could make \\(n_d^{(t+1)}\\) different from \\(n_d^{(t)}\\). If we selected a node \\(u\\) with degree \\(d-1\\) in the model update, then this node will become a node of degree \\(d\\) (since it will have one new edge attached to it), and will newly count towards the total \\(n_d^{(t+1)}\\). On the other hand, if we select a node \\(u\\) of degree \\(d\\), then this node will become a node of degree \\(d+1\\), and therefore no longer count for \\(n_d^{(t+1)}\\).\nSo, we can write down our estimate for the expected value of \\(n_d^{(t+1)}\\).\n\\[\n\\begin{aligned}\n    \\mathbb{E}\\left[n_d^{(t+1)}\\right] - n_d^{(t)} = \\mathbb{P}[d_u = d-1] - \\mathbb{P}[d_u = d]\\;.\n\\end{aligned}\n\\]\nLet’s compute the probabilities appearing on the righthand side. With probability \\(\\alpha\\), we select a node from \\(G_t\\) proportional to its degree. This means that, if a specific node \\(u\\) has degree \\(d-1\\), the probability of picking \\(u\\) is\n\\[\n\\begin{aligned}\n    \\mathbb{P}[u \\text{ is picked}] = \\frac{d-1}{\\sum_{w \\in G} d_w} = \\frac{d-1}{2m^{(t)}}\\;.\n\\end{aligned}\n\\]\nOf all the nodew we could pick, \\(p_{d-1}^{(t)}n\\) of them have degree \\(d-1\\). So, the probability of picking a node with degree \\(d-1\\) is \\(n p_{d-1}^{(t)}\\frac{d-1}{2m}\\). On the other hand, if we flipped a tails (with probability \\(1-\\alpha\\)), then we pick a node uniformly at random; each one is equally probable and \\(p_{d-1}^{(t)}\\) of them have degree \\(d-1\\). So, in this case the probability is simply \\(p_{d-1}^{(t)}\\). Combining using the law of total probability, we have\n\\[\n\\begin{aligned}\n    \\mathbb{P}[d_u = d-1] &= \\alpha n p_{d-1}^{(t)}\\frac{d-1}{2m^{(t)}} + (1-\\alpha)p_{d-1}^{(t)} \\\\\n                          &= \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)}\\;.\n\\end{aligned}\n\\]\nA similar calculation shows that\n\\[\n\\begin{aligned}\n    \\mathbb{P}[d_u = d] &= \\alpha n p_{d}^{(t)}\\frac{d}{2m^{(t)}} + (1-\\alpha)p_{d}^{(t)} \\\\\n                          &= \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;,\n\\end{aligned}\n\\]\nso our expectation is\n\\[\n\\begin{aligned}\n    \\mathbb{E}\\left[n_d^{(t+1)}\\right] - n_d^{(t)} = \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nUp until now, everything has been exact: no approximations involved. Now we’re going to start making approximations and assumptions. These can all be justified by rigorous probabilistic arguments, but we won’t do this here.\n\nWe’ll assume that \\(n_d^{(t+1)}\\) is equal to its expectation.\nIn each timestep, we add one new node and one new edge. This means that, after enough timesteps, the number of nodes \\(n\\) and number of edges \\(m\\) should be approximately equal. We’ll therefore assume that \\(t\\) is sufficiently large that \\(\\frac{n}{m} \\approx 1\\).\nStationarity: we’ll assume that, for sufficiently large \\(t\\), \\(p_d^{(t)}\\) is a constant: \\(p_d^{(t)} = p_d^{(t+1)} \\triangleq p_d\\).\n\nTo track these assumptions, we’ll use the symbol \\(\\doteq\\) to mean “equal under these assumptions.”\nWith these assumptions, we can simplify. First, we’ll replace \\(\\mathbb{E}\\left[n_d^{(t+1)}\\right]\\) with \\(n_d^{(t+1)}\\), which we’ll write as \\((n+1)p_d^{(t+1)}\\)\n\\[\n\\begin{aligned}\n    (n+1)p_d^{(t+1)} - np_d^{(t)} \\doteq \\left[\\alpha n \\frac{d-1}{2m} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha n \\frac{d}{2m} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nNext, we’ll assume \\(\\frac{n}{m} \\approx 1\\):\n\\[\n\\begin{aligned}\n    (n+1)p_d^{(t+1)} - np_d^{(t)} \\doteq \\left[\\alpha  \\frac{d-1}{2} + (1-\\alpha)\\right]p_{d-1}^{(t)} - \\left[\\alpha \\frac{d}{2} + (1-\\alpha)\\right]p_{d}^{(t)}\\;.\n\\end{aligned}\n\\]\nFinally, we’ll assume stationarity:\n\\[\n\\begin{aligned}\n    (n+1)p_d - np_d^{(t)} \\doteq \\left[\\alpha  \\frac{d-1}{2} + (1-\\alpha)\\right]p_{d-1} - \\left[\\alpha \\frac{d}{2} + (1-\\alpha)\\right]p_{d}\\;.\n\\end{aligned}\n\\]\nAfter a long setup, this looks much more manageable! Our next step is to solve for \\(p_d\\), from which we find\n\\[\n\\begin{aligned}\n    p_d &\\doteq \\frac{\\alpha  \\frac{d-1}{2} + (1-\\alpha)}{1 + \\alpha \\frac{d}{2} + (1-\\alpha)} p_{d-1} \\\\\n    &= \\frac{2(1-\\alpha) + (d-1)\\alpha}{2(1-\\alpha) + 1 + d\\alpha }p_{d-1} \\\\\n    &= \\left(1 - \\frac{1 + \\alpha }{2(1-\\alpha) + 1 + d\\alpha }\\right)p_{d-1}\\;.\n\\end{aligned}\n\\] When \\(d\\) grows large, this expression is approximately \\[\n\\begin{aligned}\n    p_d \\simeq \\left(1 - \\frac{1}{d}\\frac{1+\\alpha}{\\alpha}\\right) p_{d-1}\\;.\n\\end{aligned}\n\\]\nNow for a trick “out of thin air.” As \\(d\\) grows large,\n\\[\n\\begin{aligned}\n    1 - \\frac{1}{d}\\frac{1+\\alpha}{\\alpha} \\rightarrow \\left(\\frac{d-1}{d} \\right)^{-\\frac{1 + \\alpha}{\\alpha}}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nJustify the approximation above. To do this, Taylor-expand the function \\(f(x) = x^{-\\gamma}\\) to first order around the point \\(x_0 = 1\\) and use this expansion to estimate the value of \\(1 + \\frac{1}{d}\\)\n\n\nApplying this last approximation, we have shown that, for sufficiently large \\(d\\),\n\\[\n\\begin{aligned}\n    p_d \\simeq \\left(\\frac{d-1}{d} \\right)^{\\frac{1 + \\alpha}{\\alpha}} p_{d-1}\\;.\n\\end{aligned}\n\\]\nThis recurrence relation, if it were exact, would imply that \\(p_d = C d^{\\frac{1+\\alpha}{\\alpha}}\\), as shown by the following exercise:\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that \\(p_d\\) is a probability distribution with the property that, for some \\(d_*\\) and for all \\(d &gt; d_*\\), it holds that\n\\[\n\\begin{aligned}\n    p_d = \\left(\\frac{d-1}{d}\\right)^{\\gamma} p_{d-1}\\;.\n\\end{aligned}\n\\]\nProve using induction that \\(p_d = C d^{-\\gamma}\\) for some constant \\(C\\), and explain how to compute \\(C\\).\n\n\n\nSolution. Our proof is by induction.\nBase Case: Since the statement is claimed to hold for all \\(d &gt; d_*\\), our base case is \\(d = d_* + 1\\). In this case we have \\(p_d = \\left(\\frac{d_*}{d}\\right)^{\\gamma} p_{d_*} = \\left[p_{d_*}d_*^{\\gamma}\\right]d^{-\\gamma}\\). We’ll let \\(C = p_{d_*}d_*^\\gamma\\).\nInductive Step: Suppose that the statement holds for some arbitrary \\(d = \\hat{d} &gt; d_*\\). We’ll show that the statement also holds for \\(d = \\hat{d} + 1\\). We can calculate directly\n\\[\n\\begin{aligned}\n    p_{\\hat{d} +  1} &= \\left(\\frac{\\hat{d}}{\\hat{d} + 1}\\right)^{\\gamma} p_{\\hat{d}} \\\\\n    &= C \\left(\\frac{\\hat{d}}{\\hat{d} + 1}\\right)^{\\gamma} \\hat{d}^{-\\gamma} \\\\\n    &= C (\\hat{d} + 1)^{-\\gamma}\\;.\n\\end{aligned}\n\\]\nThis completes the inductive step and the proof.\n\nThis concludes our argument. Although this argument contains many approximations, it is also possible to reach the same conclusion using fully rigorous probabilistic arguments (Bollobás et al. 2001).",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html#sec-power-law-estimation",
    "href": "chapters/31-power-laws.html#sec-power-law-estimation",
    "title": "3  Power Law Degree Distributions",
    "section": "Estimating Power Laws From Data",
    "text": "Estimating Power Laws From Data\nAfter the publication of Barabási and Albert (1999), there was a proliferation of papers purporting to find power-law degree distributions in empirical networks. For a time, the standard method for estimating the exponent \\(\\gamma\\) was to use the key visual signature of power laws – power laws are linear on log-log axes. This suggests performing linear regression in log-log space; the slope of the regression line is the estimate of \\(\\gamma\\). This approach, however, is badly flawed: errors can be large, and uncertainty quantification is not reliably available. Clauset, Shalizi, and Newman (2009) discuss this problem in greater detail, and propose an alternative scheme based on maximum likelihood estimation and goodness-of-fit tests. Although the exact maximum-likelihood estimate of \\(\\gamma\\) is the output of a maximization problem and is not available in closed form, the authors supply a relatively accurate approximation:\n\\[\n\\begin{aligned}\n    \\hat{\\gamma} = 1 + n\\left(\\sum_{i=1}^n \\log \\frac{d_i}{d_*}\\right)^{-1}\\;.\n\\end{aligned}\n\\]\nAs they show, this estimate and related methods are much more reliable estimators of \\(\\gamma\\) than the linear regression method.\nAn important cautionary note: the estimate \\(\\hat{\\gamma}\\) can be formed regardless of whether or not the power law is a good descriptor of the data. Supplementary methods such as goodness-of-fit tests are necessary to determine whether a power law is appropriate at all. Clauset, Shalizi, and Newman (2009) give some guidance on such methods as well.\n\nPreferential Attachment in Growing Graphs\nWhat if we are able to observe more than the degree distribution of a network? What if we could also observe the growth of the network, and actually know which edges were added at which times? Under such circumstances, it is possible to estimate more directly the extent to which a graph might grow via the preferential attachment mechanism, possibly alongside additional mechanisms. Overgoor, Benson, and Ugander (2019) supply details on how to estimate the parameters of a general class of models, including preferential attachment, from observed network growth.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/31-power-laws.html#are-power-laws-good-descriptors-of-real-world-networks",
    "href": "chapters/31-power-laws.html#are-power-laws-good-descriptors-of-real-world-networks",
    "title": "3  Power Law Degree Distributions",
    "section": "Are Power Laws Good Descriptors of Real-World Networks?",
    "text": "Are Power Laws Good Descriptors of Real-World Networks?\nAre power laws really that common in empirical data? Broido and Clauset (2019) controversially claimed that scale free networks are rare. In a bit more detail, the authors compare power-law distributions to several competing distributions as models of real-world network degree sequences. The authors find that that the competing models—especially lognormal distributions, which also have heavy tails—are often better fits to observed data than power laws. This paper stirred considerable controversy, which is briefly documented by Holme (2019).\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12. https://doi.org/10.1126/science.286.5439.509.\n\n\nBollobás, Bela, Oliver Riordan, Joel Spencer, and Gábor Tusnády. 2001. “The Degree Sequence of a Scale-Free Random Graph Process.” Random Structures & Algorithms 18 (3): 279–90. https://doi.org/10.1002/rsa.1009.\n\n\nBroido, Anna D., and Aaron Clauset. 2019. “Scale-Free Networks Are Rare.” Nature Communications 10 (1): 1017. https://doi.org/10.1038/s41467-019-08746-5.\n\n\nClauset, Aaron, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. “Power-Law Distributions in Empirical Data.” SIAM Review 51 (4): 661–703. https://doi.org/10.1137/070710111.\n\n\nHolme, Petter. 2019. “Rare and Everywhere: Perspectives on Scale-Free Networks.” Nature Communications 10 (1): 1016. https://doi.org/10.1038/s41467-019-09038-8.\n\n\nMitzenmacher, Michael. 2004. “A Brief History of Generative Models for Power Law and Lognormal Distributions.” Internet Mathematics 1 (2): 226–51. https://doi.org/10.1080/15427951.2004.10129088.\n\n\nOvergoor, Jan, Austin Benson, and Johan Ugander. 2019. “Choosing to Grow a Graph: Modeling Network Formation as Discrete Choice.” In The World Wide Web Conference, 1409–20. San Francisco CA USA: ACM. https://doi.org/10.1145/3308558.3313662.\n\n\nPrice, Derek de Solla. 1976. “A General Theory of Bibliometric and Other Cumulative Advantage Processes.” Journal of the American Society for Information Science 27 (5): 292–306.\n\n\nRozemberczki, Benedek, Carl Allen, and Rik Sarkar. 2021. “Multi-Scale Attributed Node Embedding.” Journal of Complex Networks 9 (2).\n\n\nSimon, Herbert A. 1955. “On a Class of Skew Distribution Functions.” Biometrika 42 (3-4): 425–40. https://doi.org/10.1093/biomet/42.3-4.425.\n\n\nYule. 1925. “A Mathematical Theory of Evolution, Based on the Conclusions of Dr. J. C. Willis, F. R. S.” Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character 213 (402-410): 21–87. https://doi.org/10.1098/rstb.1925.0002.",
    "crumbs": [
      "Real-World Networks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Power Law Degree Distributions</span>"
    ]
  }
]