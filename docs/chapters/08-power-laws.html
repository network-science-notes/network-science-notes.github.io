<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.10">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Power Law Degree Distributions – Network Science: Models, Mathematics, and Computation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/09-random-graphs.html" rel="next">
<link href="../chapters/07-real-world.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2d9718c933debafcce942f9b212640bc.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c1bdd270c1c0708cd2ff05417efafcc5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/07-real-world.html">Real-World Networks</a></li><li class="breadcrumb-item"><a href="../chapters/08-power-laws.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power Law Degree Distributions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Network Science: Models, Mathematics, and Computation</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Network Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-networkrepresentations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Networks and Their Representations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-degree-walks-paths.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Degree, Walks, and Paths</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-components-laplacian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Components and the Graph Laplacian</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Measuring Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-centrality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Centrality and Importance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Networks and Why You Shouldn’t</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-modularity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homophily, assortativity, and modularity</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Real-World Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-real-world.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Structure of Empirical Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-power-laws.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power Law Degree Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Models of Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-random-graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-configuration-model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Configuration models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-generating-functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probability Generating Functions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Network Algorithms</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-modularity-maximization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Community Detection and Modularity Maximization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-spectral-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Spectral Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/41-link-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Link Prediction and Feedback Loops</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Applications and Extensions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/50-random-walks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Random Walks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/51-agent-based-modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Agent-Based Modeling on Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/52-epidemiology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Epidemic Models on Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Appendices</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#power-laws-as-models-of-heavy-tailed-distributions" id="toc-power-laws-as-models-of-heavy-tailed-distributions" class="nav-link" data-scroll-target="#power-laws-as-models-of-heavy-tailed-distributions">Power Laws As Models of Heavy-Tailed Distributions</a>
  <ul class="collapse">
  <li><a href="#statistical-properties-of-power-laws" id="toc-statistical-properties-of-power-laws" class="nav-link" data-scroll-target="#statistical-properties-of-power-laws">Statistical Properties of Power Laws</a></li>
  <li><a href="#purported-universality-of-power-laws" id="toc-purported-universality-of-power-laws" class="nav-link" data-scroll-target="#purported-universality-of-power-laws">Purported Universality of Power Laws</a></li>
  </ul></li>
  <li><a href="#the-preferential-attachment-model-a-generative-model-for-power-law-degrees" id="toc-the-preferential-attachment-model-a-generative-model-for-power-law-degrees" class="nav-link" data-scroll-target="#the-preferential-attachment-model-a-generative-model-for-power-law-degrees">The Preferential Attachment Model: A Generative Model for Power Law Degrees</a></li>
  <li><a href="#analyzing-preferential-attachment" id="toc-analyzing-preferential-attachment" class="nav-link" data-scroll-target="#analyzing-preferential-attachment">Analyzing Preferential Attachment</a></li>
  <li><a href="#sec-power-law-estimation" id="toc-sec-power-law-estimation" class="nav-link" data-scroll-target="#sec-power-law-estimation">Estimating Power Laws From Data</a>
  <ul class="collapse">
  <li><a href="#preferential-attachment-in-growing-graphs" id="toc-preferential-attachment-in-growing-graphs" class="nav-link" data-scroll-target="#preferential-attachment-in-growing-graphs">Preferential Attachment in Growing Graphs</a></li>
  </ul></li>
  <li><a href="#are-power-laws-good-descriptors-of-real-world-networks" id="toc-are-power-laws-good-descriptors-of-real-world-networks" class="nav-link" data-scroll-target="#are-power-laws-good-descriptors-of-real-world-networks">Are Power Laws Good Descriptors of Real-World Networks?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/07-real-world.html">Real-World Networks</a></li><li class="breadcrumb-item"><a href="../chapters/08-power-laws.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power Law Degree Distributions</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power Law Degree Distributions</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>Open the live notebook in Google Colab <a href="https://colab.research.google.com/github/network-science-notes/network-science-notes.github.io/blob/main/docs/live-notebooks/08-power-laws.ipynb">here</a>.</em></p>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Last time, we studied several properties of real-world networks and compared them to randomized models of those networks in which the degrees were held constant. That is, we were looking at aspects of the structure of real-world networks that <strong>are not captured by the degree distribution alone</strong>. But what about the degree distribution itself? Do real world networks have degree distributions that are especially interesting? What models can account for the degree distributions that we observe?</p>
<div id="e09442ac" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> factorial</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To observe a degree distribution, let’s take a look at a data set collected from the streaming platform Twitch by <span class="citation" data-cites="musae">Rozemberczki, Allen, and Sarkar (<a href="#ref-musae" role="doc-biblioref">2021</a>)</span>. Nodes are users on Twitch. An edge exists between them if they are mutual friends on the platform. The authors collected data sets for users speaking several different languages; we’ll use the network of English-speaking users. Let’s now download the data (as a Pandas data frame) and convert it into a graph using Networkx.</p>
<div id="dc6f88bc" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ENGB_edges.csv"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>edges <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.from_pandas_edgelist(edges, <span class="st">"from"</span>, <span class="st">"to"</span>, create_using<span class="op">=</span>nx.Graph)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>num_nodes <span class="op">=</span> G.number_of_nodes()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>num_edges <span class="op">=</span> G.number_of_edges()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This graph has </span><span class="sc">{</span>num_nodes<span class="sc">}</span><span class="ss"> nodes and </span><span class="sc">{</span>num_edges<span class="sc">}</span><span class="ss"> edges. The mean degree is </span><span class="sc">{</span><span class="dv">2</span><span class="op">*</span>num_edges<span class="op">/</span>num_nodes<span class="sc">:.1f}</span><span class="ss">."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>This graph has 7126 nodes and 35324 edges. The mean degree is 9.9.</code></pre>
</div>
</div>
<div class="page-columns page-full"><p>Let’s now define some helper functions to extract and visualize the degree distribution of a graph. Our first function extracts the degree distribution for easy computation, while the second creates a variable-width histogram in which each bin has the <em>same width when plotted on a logarithmic horizontal axis</em>. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">This is called <em>logarithmic binning</em>.</span></div></div>
<div id="bb7f0340" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> degree_sequence(G):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    degrees <span class="op">=</span> nx.degree(G)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    degree_sequence <span class="op">=</span> np.array([deg[<span class="dv">1</span>] <span class="cf">for</span> deg <span class="kw">in</span> degrees])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> degree_sequence</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_binned_histogram(degree_sequence, interval <span class="op">=</span> <span class="dv">5</span>, num_bins <span class="op">=</span> <span class="dv">20</span>):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    hist, bins <span class="op">=</span> np.histogram(degree_sequence, bins <span class="op">=</span> <span class="bu">min</span>(<span class="bu">int</span>(<span class="bu">len</span>(degree_sequence)<span class="op">/</span>interval), num_bins))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    bins <span class="op">=</span> np.logspace(np.log10(bins[<span class="dv">0</span>]),np.log10(bins[<span class="op">-</span><span class="dv">1</span>]),<span class="bu">len</span>(bins))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    hist, bins <span class="op">=</span> np.histogram(degree_sequence, bins <span class="op">=</span> bins)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    binwidths <span class="op">=</span> bins[<span class="dv">1</span>:] <span class="op">-</span> bins[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    hist <span class="op">=</span> hist <span class="op">/</span> binwidths</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> hist<span class="op">/</span>hist.<span class="bu">sum</span>()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bins[:<span class="op">-</span><span class="dv">1</span>], p</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_degree_distribution(G, <span class="op">**</span>kwargs):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    x, p <span class="op">=</span> log_binned_histogram(deg_seq, <span class="op">**</span>kwargs)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x, p,  facecolors<span class="op">=</span><span class="st">'none'</span>, edgecolors <span class="op">=</span>  <span class="st">'cornflowerblue'</span>, linewidth <span class="op">=</span> <span class="dv">2</span>, label <span class="op">=</span> <span class="st">"Data"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Degree"</span>, xlim <span class="op">=</span> (<span class="fl">0.5</span>, x.<span class="bu">max</span>()<span class="op">*</span><span class="dv">2</span>))</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    plt.gca().<span class="bu">set</span>(ylabel <span class="op">=</span> <span class="st">"Density"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    plt.gca().loglog()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plt.gca()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use this function to inspect the degree distrubtion of the data:</p>
<div id="102dce6b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">10</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-power-laws_files/figure-html/cell-5-output-1.png" class="figure-img" width="589" height="429"></p>
</figure>
</div>
</div>
</div>
<p>There are a few things to notice about this degree distribution. First, most nodes have relatively small degrees, fewer tham the mean of 9.9. However, there are a small number of nodes that have degrees which are <em>much</em> larger: almost two orders of magnitude larger! You can think of these nodes as “super stars” or “hubs” of the network; they may correspond to especially popular or influential accounts.</p>
<p>Recall that, from our discussion of the Erdős–Rényi model <span class="math inline">\(G(n,p)\)</span>, the degree distribution of a <span class="math inline">\(G(n,p)\)</span> model with mean degree <span class="math inline">\(\bar{d}\)</span> is approximately Poisson with mean <span class="math inline">\(\bar{d}\)</span>. Let’s compare this Poisson distribution to the data.</p>
<div id="701f9954" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>mean_degree  <span class="op">=</span> deg_seq.mean()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>d_      <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">30</span>, <span class="dv">1</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>poisson <span class="op">=</span> np.exp(<span class="op">-</span>mean_degree)<span class="op">*</span>(mean_degree<span class="op">**</span>d_)<span class="op">/</span>factorial(d_)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">10</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>ax.plot(d_, poisson,  linewidth <span class="op">=</span> <span class="dv">1</span>, label <span class="op">=</span> <span class="st">"Poisson fit"</span>, color <span class="op">=</span> <span class="st">"grey"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-power-laws_files/figure-html/cell-6-output-1.png" class="figure-img" width="589" height="429"></p>
</figure>
</div>
</div>
</div>
<p>Comparing the Poisson fit predicted by the Erdős–Rényi model to the actual data, we see that the the Poisson places much higher probability mass on degrees that are <em>close to the mean</em> of 9.9. The Poisson would predict that almost no nodes would have degree higher than <span class="math inline">\(10^2\)</span>, while in the data there are several.</p>
<p>We often say that the Poisson has a “light right tail” – the probability mass allocated by the Poisson dramatically drops off as we move to the right of the mean. In contrast, the data itself appears to have a “heavy tail”: there is substantial probability mass even far to the right from the mean.</p>
</section>
<section id="power-laws-as-models-of-heavy-tailed-distributions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="power-laws-as-models-of-heavy-tailed-distributions">Power Laws As Models of Heavy-Tailed Distributions</h2>
<p>There are many probability distributions that have heavy tails. By far the most important (and controversial) in the history of network science is the <em>power law degree distribution</em>.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-power-law" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8.1 (Power Law Distribution)</strong></span> A random variable <span class="math inline">\(D\)</span> has a <em>discrete power law distribution</em> with cutoff <span class="math inline">\(d_*\)</span> and exponent <span class="math inline">\(\gamma &gt; 1\)</span> if its probability mass function is has the form</p>
<p><span id="eq-power-law"><span class="math display">\[
\begin{aligned}
    p_d \triangleq \mathbb{P}[D = d] = C d^{-\gamma}\;,
\end{aligned}
\tag{8.1}\]</span></span></p>
<p>for all <span class="math inline">\(d &gt; d_*\)</span>. Here, <span class="math inline">\(C\)</span> is a normalizing constant that ensures that the distribution sums to <span class="math inline">\(1\)</span>. The entries of the distribution for <span class="math inline">\(d \leq d_*\)</span> are are arbitrary.</p>
</div>
</div>
</div>
</div>
<p>An important intuitive insight about power law distributions is that they are <em>linear on log-log axes</em>. To see this, we can take the logarithm of both sides in <a href="#eq-power-law" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>:</p>
<p><span class="math display">\[
\begin{aligned}
    \log p_d = \log C - \gamma \log d\;.
\end{aligned}
\]</span></p>
<p>So, <span class="math inline">\(\log p_d\)</span> is a linear function of <span class="math inline">\(\log d\)</span> with slope <span class="math inline">\(-\gamma\)</span>.</p>
<p>Let’s try plotting such a distribution against the data. Since the power law distribution is defined for all <span class="math inline">\(d &gt; d_*\)</span>, we’ll need to choose a cutoff <span class="math inline">\(d_*\)</span> and an exponent <span class="math inline">\(\gamma\)</span>. For now, we’ll do this by eye. If we inspect the plot of the data above, it looks like linear behavior takes over somewhere around <span class="math inline">\(d_* = 10^\frac{3}{2} \approx 30\)</span>.</p>
<div id="2767a4b6" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cutoff  <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>d_      <span class="op">=</span> np.arange(cutoff, deg_seq.<span class="bu">max</span>(), <span class="dv">1</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>gamma   <span class="op">=</span> <span class="fl">2.7</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>power_law <span class="op">=</span> <span class="dv">18</span><span class="op">*</span>d_<span class="op">**</span>(<span class="op">-</span>gamma)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">10</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>ax.plot(d_, power_law,  linewidth <span class="op">=</span> <span class="dv">1</span>, label <span class="op">=</span> <span class="vs">fr"Power law: $\gamma = </span><span class="sc">{</span>gamma<span class="sc">}</span><span class="vs">$"</span> , color <span class="op">=</span> <span class="st">"grey"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-power-laws_files/figure-html/cell-7-output-1.png" class="figure-img" width="589" height="429"></p>
</figure>
</div>
</div>
</div>
<div class="page-columns page-full"><p>The power law appears to be a much better fit than the Poisson to the tail of the distribution, making apparently reasonable predictions about the numbers of nodes with very high degrees. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Where do the parameters for the power law come from? Here we performed a fit “by eye”, but we discuss some systematic approaches in <a href="#sec-power-law-estimation" class="quarto-xref"><span>Section 8.5</span></a></span></div></div>
<p>The claim that a given network “follows a power law” is a bit murky: like other models, power laws are idealizations that no real data set matches exactly. The idea of a power law is also fundamentally asymptotic in nature: the power law that we fit to the data also predicts that we should see nodes of degree <span class="math inline">\(10^3\)</span>, <span class="math inline">\(10^4\)</span>, or <span class="math inline">\(10^5\)</span> if we were to allow the network to keep growing. Since the network can’t keep growing (it’s data, we have a finite amount of it), we have to view the power law’s predictions about very high-degree nodes as extrapolations toward an idealized, infinite-size data set to which we obviously do not have access.</p>
<section id="statistical-properties-of-power-laws" class="level3">
<h3 class="anchored" data-anchor-id="statistical-properties-of-power-laws">Statistical Properties of Power Laws</h3>
<p>Power law distributions are much more <em>variable</em> than, say, Poisson distributions. Indeed, when <span class="math inline">\(\gamma \leq 3\)</span>, the variance of a power law distribution is infinite. To show this, we calculate the second moment of the distribution:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}[D^2] &amp;= \sum_{d = 1}^\infty p_d d^2 \\
                    &amp;= \sum_{d = 1}^{d_*} p_d d^2 + \sum_{d = d_* + 1}^\infty d^2 C d^{-\gamma} \\
                    &amp;= K + C \sum_{d = d_*}^\infty d^{2-\gamma}\;,
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is a (finite) constant. Now we draw upon a fact from calculus: the sequence <span class="math inline">\(\sum_{x = 1}^\infty x^{-p}\)</span> converges if and only if <span class="math inline">\(p &gt; 1\)</span>. This means that the sum <span class="math inline">\(\sum_{d = d_*}^\infty d^{2-\gamma}\)</span> converges if and only if <span class="math inline">\(2 - \gamma &lt; -1\)</span>, which requires <span class="math inline">\(\gamma &gt; 3\)</span>. If <span class="math inline">\(\gamma \leq 3\)</span>, the second moment (and therefore the variance) is undefined. Heuristically, this means that the distribution can generate samples which are are arbitrarily far from the mean, much as the data above includes samples which are orders of magnitude higher than the mean.</p>
</section>
<section id="purported-universality-of-power-laws" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="purported-universality-of-power-laws">Purported Universality of Power Laws</h3>
<div class="page-columns page-full"><p><span class="citation" data-cites="barabasiEmergenceScalingRandom1999">Barabási and Albert (<a href="#ref-barabasiEmergenceScalingRandom1999" role="doc-biblioref">1999</a>)</span> made the first published that power-law degree distributions are very common in real-world networks. This initial paper has since been cited over 46,000 times (as of August 2024). <em>Many</em> subsequent studies have fit power-law tails to degree distributions in empirical networks across social, technological, and biological domains.  The idea that power laws are common in real-world networks is sometimes called the “scale-free hypothesis.” Part of the appeal of this hypothesis comes from statistical physics and complexity science, where power law distributions are common signatures of self-organizing systems.</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><span class="citation" data-cites="clausetPowerLawDistributionsEmpirical2009">Clauset, Shalizi, and Newman (<a href="#ref-clausetPowerLawDistributionsEmpirical2009" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="broidoScalefreeNetworksAre2019">Broido and Clauset (<a href="#ref-broidoScalefreeNetworksAre2019" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="holmeRareEverywherePerspectives2019">Holme (<a href="#ref-holmeRareEverywherePerspectives2019" role="doc-biblioref">2019</a>)</span> provide some review and references for these claims.</span></div></div>
</section>
</section>
<section id="the-preferential-attachment-model-a-generative-model-for-power-law-degrees" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-preferential-attachment-model-a-generative-model-for-power-law-degrees">The Preferential Attachment Model: A Generative Model for Power Law Degrees</h2>
<div class="page-columns page-full"><p>Why would power-law degree distributions be common in empirical networks? A common way to answer questions like this is to propose a <em>generative model</em>. A <em>generative model</em> is a random network model that is intended to produce some kind of realistic structure.  If the mechanism proposed by the generative model is plausible as a real, common mechanism, then we might expect that the large-scale structure generated by that model would be commonly observed.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Generative models contrast with <em>null models</em> like the <span class="math inline">\(G(n,p)\)</span> model, which are usually used to <em>contrast</em> with real networks. Generative models are models of what the data is like; null models are models of what the data is <em>not</em> like.</span></div></div>
<p><span class="citation" data-cites="barabasiEmergenceScalingRandom1999">Barabási and Albert (<a href="#ref-barabasiEmergenceScalingRandom1999" role="doc-biblioref">1999</a>)</span> are responsible for popularizing the claim that many empirical networks are scale-free. Alongside this claim, they offered a generative model called <em>preferential attachment</em>. The preferential attachment model offers a simple mechanism of network growth which leads to power-law degree distributions. Their model is closely related to the regrettably much less famous models of <span class="citation" data-cites="yuleMathematicalTheoryEvolution1925">Yule (<a href="#ref-yuleMathematicalTheoryEvolution1925" role="doc-biblioref">1925</a>)</span>, <span class="citation" data-cites="simonClassSkewDistribution1955">Simon (<a href="#ref-simonClassSkewDistribution1955" role="doc-biblioref">1955</a>)</span>, and <span class="citation" data-cites="price1976general">Price (<a href="#ref-price1976general" role="doc-biblioref">1976</a>)</span>.</p>
<div class="page-columns page-full"><p>Here’s how the Yule-Simon-Price-Barabási-Albert model works. First, we start off with some initial graph <span class="math inline">\(G_0\)</span>. Then, in each timestep <span class="math inline">\(t=1,2,\ldots\)</span>, we: </p><div class="no-row-height column-margin column-container"><span class="margin-aside">The model of <span class="citation" data-cites="barabasiEmergenceScalingRandom1999">Barabási and Albert (<a href="#ref-barabasiEmergenceScalingRandom1999" role="doc-biblioref">1999</a>)</span> did not include a uniform selection mechanism, which corresponds to the case <span class="math inline">\(\alpha = 1\)</span>.</span></div></div>
<ol type="1">
<li>Flip a coin with probability of heads equal to <span class="math inline">\(\alpha\)</span>. If this coin lands heads, then:
<ol type="1">
<li>Choose a node <span class="math inline">\(u\)</span> from <span class="math inline">\(G_{t-1}\)</span> with probability <em>proportional to its degree</em>.</li>
</ol></li>
<li>Otherwise, if the coin lands tails, choose a node <span class="math inline">\(u\)</span> from <span class="math inline">\(G_{t-1}\)</span> uniformly at random.</li>
<li>Add a node <span class="math inline">\(v\)</span> to <span class="math inline">\(G_{t-1}\)</span>.</li>
<li>Add edge <span class="math inline">\((u,v)\)</span> to <span class="math inline">\(G_{t-1}\)</span>.</li>
</ol>
<p>We repeat this process as many times as desired. Intuitively, the preferential attachment model expresses the idea that “the rich get richer”: nodes that already have many connections are more likely to receive new connections.</p>
<div class="page-columns page-full"><p>Here’s a quick implementation. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Note: this implementation of preferential attachment is useful for illustrating the mathematics and operations with Networkx. It is, however, not efficient.</span></div></div>
<div id="d1e24aa8" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initial condition</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph() </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>G.add_edge(<span class="dv">0</span>, <span class="dv">1</span>) </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">3</span><span class="op">/</span><span class="dv">5</span> <span class="co"># proportion of degree-based selection steps</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># main loop</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    degrees <span class="op">=</span> nx.degree(G)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine u using one of two mechanisms</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> alpha: </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        deg_seq <span class="op">=</span> np.array([deg[<span class="dv">1</span>] <span class="cf">for</span> deg <span class="kw">in</span> degrees])</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        degree_weights <span class="op">=</span> deg_seq <span class="op">/</span> deg_seq.<span class="bu">sum</span>()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.choice(np.arange(<span class="bu">len</span>(degrees)), p <span class="op">=</span> degree_weights)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.choice(np.arange(<span class="bu">len</span>(degrees)))</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># integer index of new node v</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> <span class="bu">len</span>(degrees)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add new edge to graph    </span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    G.add_edge(u, v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s go ahead and plot the result. We’ll add a visualization of the exponent <span class="math inline">\(\gamma\)</span> as well. How do we know the right value of <span class="math inline">\(\gamma\)</span>? It turns out that there is a theoretical estimate based on <span class="math inline">\(\alpha\)</span> which we’ll derive in the next section.</p>
<div id="c906eeb6" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>cutoff  <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>d_      <span class="op">=</span> np.arange(cutoff, deg_seq.<span class="bu">max</span>(), <span class="dv">1</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>gamma   <span class="op">=</span> (<span class="dv">2</span> <span class="op">+</span> alpha) <span class="op">/</span> alpha</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>power_law <span class="op">=</span> <span class="dv">10</span><span class="op">*</span>d_<span class="op">**</span>(<span class="op">-</span>gamma)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">2</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>ax.plot(d_, power_law,  linewidth <span class="op">=</span> <span class="dv">1</span>, label <span class="op">=</span> <span class="vs">fr"Power law: $\gamma = </span><span class="sc">{</span>gamma<span class="sc">:.2f}</span><span class="vs">$"</span> , color <span class="op">=</span> <span class="st">"grey"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08-power-laws_files/figure-html/cell-9-output-1.png" class="figure-img" width="588" height="429"></p>
</figure>
</div>
</div>
</div>
<p>This fit is somewhat noisy, reflecting the fact that we simulated a relatively small number of preferential attachment steps.</p>
</section>
<section id="analyzing-preferential-attachment" class="level2">
<h2 class="anchored" data-anchor-id="analyzing-preferential-attachment">Analyzing Preferential Attachment</h2>
<p>Let’s now see if we can understand mathematically why the preferential attachment model leads to networks with power-law degree distributions. There are many ways to demonstrate this fact, including both “casual” and highly rigorous techniques. Here, we’ll use a “casual” argument from <span class="citation" data-cites="mitzenmacherBriefHistoryGenerative2004">Mitzenmacher (<a href="#ref-mitzenmacherBriefHistoryGenerative2004" role="doc-biblioref">2004</a>)</span>.</p>
<p>Let <span class="math inline">\(p_d^{(t)}\)</span> be the <em>proportion</em> of nodes of degree <span class="math inline">\(d \geq 2\)</span> after algorithmic timestep <span class="math inline">\(t\)</span>. Suppose that at this timestep there are <span class="math inline">\(n\)</span> nodes and <span class="math inline">\(m\)</span> edges. Then, the total <em>number</em> of nodes of degree <span class="math inline">\(d\)</span> is <span class="math inline">\(n_d^{(t)} = np_d^{(t)}\)</span>. Suppose that we do one step of the preferential attachment model. Let’s ask: what will be the new expected value of <span class="math inline">\(n_d^{(t+1)}\)</span>?</p>
<p>Well, in the previous timestep there were <span class="math inline">\(n_d^{(t)}\)</span> nodes of degree <span class="math inline">\(d\)</span>. How could this quantity change? There are two processes that could make <span class="math inline">\(n_d^{(t+1)}\)</span> different from <span class="math inline">\(n_d^{(t)}\)</span>. If we selected a node <span class="math inline">\(u\)</span> with degree <span class="math inline">\(d-1\)</span> in the model update, then this node will become a node of degree <span class="math inline">\(d\)</span> (since it will have one new edge attached to it), and will newly count towards the total <span class="math inline">\(n_d^{(t+1)}\)</span>. On the other hand, if we select a node <span class="math inline">\(u\)</span> of degree <span class="math inline">\(d\)</span>, then this node will become a node of degree <span class="math inline">\(d+1\)</span>, and therefore no longer count for <span class="math inline">\(n_d^{(t+1)}\)</span>.</p>
<p>So, we can write down our estimate for the expected value of <span class="math inline">\(n_d^{(t+1)}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}\left[n_d^{(t+1)}\right] - n_d^{(t)} = \mathbb{P}[d_u = d-1] - \mathbb{P}[d_u = d]\;.
\end{aligned}
\]</span></p>
<p>Let’s compute the probabilities appearing on the righthand side. With probability <span class="math inline">\(\alpha\)</span>, we select a node from <span class="math inline">\(G_t\)</span> proportional to its degree. This means that, if a <em>specific</em> node <span class="math inline">\(u\)</span> has degree <span class="math inline">\(d-1\)</span>, the probability of picking <span class="math inline">\(u\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}[u \text{ is picked}] = \frac{d-1}{\sum_{w \in G} d_w} = \frac{d-1}{2m^{(t)}}\;.
\end{aligned}
\]</span></p>
<p>Of all the nodew we could pick, <span class="math inline">\(p_{d-1}^{(t)}n\)</span> of them have degree <span class="math inline">\(d-1\)</span>. So, the probability of picking a node with degree <span class="math inline">\(d-1\)</span> is <span class="math inline">\(n p_{d-1}^{(t)}\frac{d-1}{2m}\)</span>. On the other hand, if we flipped a tails (with probability <span class="math inline">\(1-\alpha\)</span>), then we pick a node uniformly at random; each one is equally probable and <span class="math inline">\(p_{d-1}^{(t)}\)</span> of them have degree <span class="math inline">\(d-1\)</span>. So, in this case the probability is simply <span class="math inline">\(p_{d-1}^{(t)}\)</span>. Combining using the law of total probability, we have</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}[d_u = d-1] &amp;= \alpha n p_{d-1}^{(t)}\frac{d-1}{2m^{(t)}} + (1-\alpha)p_{d-1}^{(t)} \\
                          &amp;= \left[\alpha n \frac{d-1}{2m} + (1-\alpha)\right]p_{d-1}^{(t)}\;.
\end{aligned}
\]</span></p>
<p>A similar calculation shows that</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}[d_u = d] &amp;= \alpha n p_{d}^{(t)}\frac{d}{2m^{(t)}} + (1-\alpha)p_{d}^{(t)} \\
                          &amp;= \left[\alpha n \frac{d}{2m} + (1-\alpha)\right]p_{d}^{(t)}\;,
\end{aligned}
\]</span></p>
<p>so our expectation is</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}\left[n_d^{(t+1)}\right] - n_d^{(t)} = \left[\alpha n \frac{d-1}{2m} + (1-\alpha)\right]p_{d-1}^{(t)} - \left[\alpha n \frac{d}{2m} + (1-\alpha)\right]p_{d}^{(t)}\;.
\end{aligned}
\]</span></p>
<p>Up until now, everything has been exact: no approximations involved. Now we’re going to start making approximations and assumptions. These can all be justified by rigorous probabilistic arguments, but we won’t do this here.</p>
<ol type="1">
<li>We’ll assume that <span class="math inline">\(n_d^{(t+1)}\)</span> is equal to its expectation.</li>
<li>In each timestep, we add one new node and one new edge. This means that, after enough timesteps, the number of nodes <span class="math inline">\(n\)</span> and number of edges <span class="math inline">\(m\)</span> should be approximately equal. We’ll therefore assume that <span class="math inline">\(t\)</span> is sufficiently large that <span class="math inline">\(\frac{n}{m} \approx 1\)</span>.</li>
<li><strong>Stationarity</strong>: we’ll assume that, for sufficiently large <span class="math inline">\(t\)</span>, <span class="math inline">\(p_d^{(t)}\)</span> is a <em>constant</em>: <span class="math inline">\(p_d^{(t)} = p_d^{(t+1)} \triangleq p_d\)</span>.</li>
</ol>
<p>To track these assumptions, we’ll use the symbol <span class="math inline">\(\doteq\)</span> to mean “equal under these assumptions.”</p>
<p>With these assumptions, we can simplify. First, we’ll replace <span class="math inline">\(\mathbb{E}\left[n_d^{(t+1)}\right]\)</span> with <span class="math inline">\(n_d^{(t+1)}\)</span>, which we’ll write as <span class="math inline">\((n+1)p_d^{(t+1)}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
    (n+1)p_d^{(t+1)} - np_d^{(t)} \doteq \left[\alpha n \frac{d-1}{2m} + (1-\alpha)\right]p_{d-1}^{(t)} - \left[\alpha n \frac{d}{2m} + (1-\alpha)\right]p_{d}^{(t)}\;.
\end{aligned}
\]</span></p>
<p>Next, we’ll assume <span class="math inline">\(\frac{n}{m} \approx 1\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    (n+1)p_d^{(t+1)} - np_d^{(t)} \doteq \left[\alpha  \frac{d-1}{2} + (1-\alpha)\right]p_{d-1}^{(t)} - \left[\alpha \frac{d}{2} + (1-\alpha)\right]p_{d}^{(t)}\;.
\end{aligned}
\]</span></p>
<p>Finally, we’ll assume stationarity:</p>
<p><span class="math display">\[
\begin{aligned}
    (n+1)p_d - np_d \doteq \left[\alpha  \frac{d-1}{2} + (1-\alpha)\right]p_{d-1} - \left[\alpha \frac{d}{2} + (1-\alpha)\right]p_{d}\;.
\end{aligned}
\]</span></p>
<p>After a long setup, this looks much more manageable! Our next step is to solve for <span class="math inline">\(p_d\)</span>, from which we find</p>
<p><span class="math display">\[
\begin{aligned}
    p_d &amp;\doteq \frac{\alpha  \frac{d-1}{2} + (1-\alpha)}{1 + \alpha \frac{d}{2} + (1-\alpha)} p_{d-1} \\
    &amp;= \frac{2(1-\alpha) + (d-1)\alpha}{2(1-\alpha) + 2 + d\alpha }p_{d-1} \\
    &amp;= \left(1 - \frac{2 + \alpha }{2(1-\alpha) + 2 + d\alpha }\right)p_{d-1}\;.
\end{aligned}
\]</span> When <span class="math inline">\(d\)</span> grows large, this expression is approximately <span class="math display">\[
\begin{aligned}
    p_d \simeq \left(1 - \frac{1}{d}\frac{2+\alpha}{\alpha}\right) p_{d-1}\;.
\end{aligned}
\]</span></p>
<p>Now for a trick “out of thin air.” As <span class="math inline">\(d\)</span> grows large,</p>
<p><span class="math display">\[
\begin{aligned}
    1 - \frac{1}{d}\frac{2+\alpha}{\alpha} \rightarrow \left(\frac{d-1}{d} \right)^{\frac{2 + \alpha}{\alpha}}
\end{aligned}
\]</span></p>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Justify the approximation above. To do this, Taylor-expand the function <span class="math inline">\(f(x) = x^{\gamma}\)</span> to first order around the point <span class="math inline">\(x_0 = 1\)</span> and use this expansion to estimate the value of <span class="math inline">\(1 - \frac{1}{d}\)</span>.</p>
</div>
</div>

<p>Applying this last approximation, we have shown that, for sufficiently large <span class="math inline">\(d\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
    p_d \simeq \left(\frac{d-1}{d} \right)^{\frac{2 + \alpha}{\alpha}} p_{d-1}\;.
\end{aligned}
\]</span></p>
<p>This recurrence relation, if it were exact, would imply that <span class="math inline">\(p_d = C d^{-\frac{2+\alpha}{\alpha}}\)</span>, as shown by the following exercise:</p>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose that <span class="math inline">\(p_d\)</span> is a probability distribution with the property that, for some <span class="math inline">\(d_*\)</span> and for all <span class="math inline">\(d &gt; d_*\)</span>, it holds that</p>
<p><span class="math display">\[
\begin{aligned}
    p_d = \left(\frac{d-1}{d}\right)^{\gamma} p_{d-1}\;.
\end{aligned}
\]</span></p>
<p>Prove using induction that <span class="math inline">\(p_d = C d^{-\gamma}\)</span> for some constant <span class="math inline">\(C\)</span>, and explain how to compute <span class="math inline">\(C\)</span>.</p>
</div>
</div>

<p>This concludes our argument. Although this argument contains many approximations, it is also possible to reach the same conclusion using fully rigorous probabilistic arguments <span class="citation" data-cites="bollobasDegreeSequenceScale2001">(<a href="#ref-bollobasDegreeSequenceScale2001" role="doc-biblioref">Bollobás et al. 2001</a>)</span>.</p>
</section>
<section id="sec-power-law-estimation" class="level2">
<h2 class="anchored" data-anchor-id="sec-power-law-estimation">Estimating Power Laws From Data</h2>
<p>After the publication of <span class="citation" data-cites="barabasiEmergenceScalingRandom1999">Barabási and Albert (<a href="#ref-barabasiEmergenceScalingRandom1999" role="doc-biblioref">1999</a>)</span>, there was a proliferation of papers purporting to find power-law degree distributions in empirical networks. For a time, the standard method for estimating the exponent <span class="math inline">\(\gamma\)</span> was to use the key visual signature of power laws – power laws are linear on log-log axes. This suggests performing linear regression in log-log space; the slope of the regression line is the estimate of <span class="math inline">\(\gamma\)</span>. This approach, however, is badly flawed: errors can be large, and uncertainty quantification is not reliably available. <span class="citation" data-cites="clausetPowerLawDistributionsEmpirical2009">Clauset, Shalizi, and Newman (<a href="#ref-clausetPowerLawDistributionsEmpirical2009" role="doc-biblioref">2009</a>)</span> discuss this problem in greater detail, and propose an alternative scheme based on maximum likelihood estimation and goodness-of-fit tests. Although the exact maximum-likelihood estimate of <span class="math inline">\(\gamma\)</span> is the output of a maximization problem and is not available in closed form, the authors supply a relatively accurate approximation:</p>
<p><span class="math display">\[
\begin{aligned}
    \hat{\gamma} = 1 + n\left(\sum_{i=1}^n \log \frac{d_i}{d_*}\right)^{-1}\;.
\end{aligned}
\]</span></p>
<p>As they show, this estimate and related methods are much more reliable estimators of <span class="math inline">\(\gamma\)</span> than the linear regression method.</p>
<p>An important cautionary note: the estimate <span class="math inline">\(\hat{\gamma}\)</span> can be formed regardless of whether or not the power law is a good descriptor of the data. Supplementary methods such as goodness-of-fit tests are necessary to determine whether a power law is appropriate at all. <span class="citation" data-cites="clausetPowerLawDistributionsEmpirical2009">Clauset, Shalizi, and Newman (<a href="#ref-clausetPowerLawDistributionsEmpirical2009" role="doc-biblioref">2009</a>)</span> give some guidance on such methods as well.</p>
<section id="preferential-attachment-in-growing-graphs" class="level3">
<h3 class="anchored" data-anchor-id="preferential-attachment-in-growing-graphs">Preferential Attachment in Growing Graphs</h3>
<p>What if we are able to observe more than the degree distribution of a network? What if we could also observe the growth of the network, and actually know which edges were added at which times? Under such circumstances, it is possible to estimate more directly the extent to which a graph might grow via the preferential attachment mechanism, possibly alongside additional mechanisms. <span class="citation" data-cites="overgoorChoosingGrowGraph2019">Overgoor, Benson, and Ugander (<a href="#ref-overgoorChoosingGrowGraph2019" role="doc-biblioref">2019</a>)</span> supply details on how to estimate the parameters of a general class of models, including preferential attachment, from observed network growth.</p>
</section>
</section>
<section id="are-power-laws-good-descriptors-of-real-world-networks" class="level2">
<h2 class="anchored" data-anchor-id="are-power-laws-good-descriptors-of-real-world-networks">Are Power Laws Good Descriptors of Real-World Networks?</h2>
<p>Are power laws <em>really</em> that common in empirical data? <span class="citation" data-cites="broidoScalefreeNetworksAre2019">Broido and Clauset (<a href="#ref-broidoScalefreeNetworksAre2019" role="doc-biblioref">2019</a>)</span> controversially claimed that <em>scale free networks are rare</em>. In a bit more detail, the authors compare power-law distributions to several competing distributions as models of real-world network degree sequences. The authors find that that the competing models—especially <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">lognormal distributions</a>, which also have heavy tails—are often better fits to observed data than power laws. This paper stirred considerable controversy, which is briefly documented by <span class="citation" data-cites="holmeRareEverywherePerspectives2019">Holme (<a href="#ref-holmeRareEverywherePerspectives2019" role="doc-biblioref">2019</a>)</span>.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-barabasiEmergenceScalingRandom1999" class="csl-entry" role="listitem">
Barabási, Albert-László, and Réka Albert. 1999. <span>“Emergence of <span>Scaling</span> in <span>Random Networks</span>.”</span> <em>Science</em> 286 (5439): 509–12. <a href="https://doi.org/10.1126/science.286.5439.509">https://doi.org/10.1126/science.286.5439.509</a>.
</div>
<div id="ref-bollobasDegreeSequenceScale2001" class="csl-entry" role="listitem">
Bollobás, Bela, Oliver Riordan, Joel Spencer, and Gábor Tusnády. 2001. <span>“The Degree Sequence of a Scale-Free Random Graph Process.”</span> <em>Random Structures &amp; Algorithms</em> 18 (3): 279–90. <a href="https://doi.org/10.1002/rsa.1009">https://doi.org/10.1002/rsa.1009</a>.
</div>
<div id="ref-broidoScalefreeNetworksAre2019" class="csl-entry" role="listitem">
Broido, Anna D., and Aaron Clauset. 2019. <span>“Scale-Free Networks Are Rare.”</span> <em>Nature Communications</em> 10 (1): 1017. <a href="https://doi.org/10.1038/s41467-019-08746-5">https://doi.org/10.1038/s41467-019-08746-5</a>.
</div>
<div id="ref-clausetPowerLawDistributionsEmpirical2009" class="csl-entry" role="listitem">
Clauset, Aaron, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. <span>“Power-<span>Law Distributions</span> in <span>Empirical Data</span>.”</span> <em>SIAM Review</em> 51 (4): 661–703. <a href="https://doi.org/10.1137/070710111">https://doi.org/10.1137/070710111</a>.
</div>
<div id="ref-holmeRareEverywherePerspectives2019" class="csl-entry" role="listitem">
Holme, Petter. 2019. <span>“Rare and Everywhere: <span>Perspectives</span> on Scale-Free Networks.”</span> <em>Nature Communications</em> 10 (1): 1016. <a href="https://doi.org/10.1038/s41467-019-09038-8">https://doi.org/10.1038/s41467-019-09038-8</a>.
</div>
<div id="ref-mitzenmacherBriefHistoryGenerative2004" class="csl-entry" role="listitem">
Mitzenmacher, Michael. 2004. <span>“A <span>Brief History</span> of <span>Generative Models</span> for <span>Power Law</span> and <span>Lognormal Distributions</span>.”</span> <em>Internet Mathematics</em> 1 (2): 226–51. <a href="https://doi.org/10.1080/15427951.2004.10129088">https://doi.org/10.1080/15427951.2004.10129088</a>.
</div>
<div id="ref-overgoorChoosingGrowGraph2019" class="csl-entry" role="listitem">
Overgoor, Jan, Austin Benson, and Johan Ugander. 2019. <span>“Choosing to <span>Grow</span> a <span>Graph</span>: <span>Modeling Network Formation</span> as <span>Discrete Choice</span>.”</span> In <em>The <span>World Wide Web Conference</span></em>, 1409–20. San Francisco CA USA: ACM. <a href="https://doi.org/10.1145/3308558.3313662">https://doi.org/10.1145/3308558.3313662</a>.
</div>
<div id="ref-price1976general" class="csl-entry" role="listitem">
Price, Derek de Solla. 1976. <span>“A General Theory of Bibliometric and Other Cumulative Advantage Processes.”</span> <em>Journal of the American Society for Information Science</em> 27 (5): 292–306.
</div>
<div id="ref-musae" class="csl-entry" role="listitem">
Rozemberczki, Benedek, Carl Allen, and Rik Sarkar. 2021. <span>“<span>Multi-Scale Attributed Node Embedding</span>.”</span> <em>Journal of Complex Networks</em> 9 (2).
</div>
<div id="ref-simonClassSkewDistribution1955" class="csl-entry" role="listitem">
Simon, Herbert A. 1955. <span>“On a <span>Class</span> of <span>Skew Distribution Functions</span>.”</span> <em>Biometrika</em> 42 (3-4): 425–40. <a href="https://doi.org/10.1093/biomet/42.3-4.425">https://doi.org/10.1093/biomet/42.3-4.425</a>.
</div>
<div id="ref-yuleMathematicalTheoryEvolution1925" class="csl-entry" role="listitem">
Yule. 1925. <span>“A Mathematical Theory of Evolution, Based on the Conclusions of <span>Dr</span>. <span>J</span>. <span>C</span>. <span>Willis</span>, <span>F</span>. <span>R</span>. <span>S</span>.”</span> <em>Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character</em> 213 (402-410): 21–87. <a href="https://doi.org/10.1098/rstb.1925.0002">https://doi.org/10.1098/rstb.1925.0002</a>.
</div>
</div>
</section>

<p><br> <br> <span style="color:grey;">© Heather Zinn Brooks and Phil Chodrow, 2025</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/07-real-world.html" class="pagination-link" aria-label="Structure of Empirical Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Structure of Empirical Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/09-random-graphs.html" class="pagination-link" aria-label="Random Graphs: Erdős–Rényi">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> false</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="an">code-summary:</span><span class="co"> "Show code"</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> env</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="an">cache:</span><span class="co"> true</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fu"># Power Law Degree Distributions</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>*Open the live notebook in Google Colab [here](https://colab.research.google.com/github/network-science-notes/network-science-notes.github.io/blob/main/docs/live-notebooks/08-power-laws.ipynb).* </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>Last time, we studied several properties of real-world networks and compared them to randomized models of those networks in which the degrees were held constant. That is, we were looking at aspects of the structure of real-world networks that **are not captured by the degree distribution alone**. But what about the degree distribution itself? Do real world networks have degree distributions that are especially interesting? What models can account for the degree distributions that we observe? </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> factorial</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>To observe a degree distribution, let's take a look at a data set collected from the streaming platform Twitch by @musae. Nodes are users on Twitch. An edge exists between them if they are mutual friends on the platform. The authors collected data sets for users speaking several different languages; we'll use the network of English-speaking users. Let's now download the data (as a Pandas data frame) and convert it into a graph using Networkx. </span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/benedekrozemberczki/MUSAE/master/input/edges/ENGB_edges.csv"</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>edges <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.from_pandas_edgelist(edges, <span class="st">"from"</span>, <span class="st">"to"</span>, create_using<span class="op">=</span>nx.Graph)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>num_nodes <span class="op">=</span> G.number_of_nodes()</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>num_edges <span class="op">=</span> G.number_of_edges()</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This graph has </span><span class="sc">{</span>num_nodes<span class="sc">}</span><span class="ss"> nodes and </span><span class="sc">{</span>num_edges<span class="sc">}</span><span class="ss"> edges. The mean degree is </span><span class="sc">{</span><span class="dv">2</span><span class="op">*</span>num_edges<span class="op">/</span>num_nodes<span class="sc">:.1f}</span><span class="ss">."</span>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>Let's now define some helper functions to extract and visualize the degree distribution of a graph. Our first function extracts the degree distribution for easy computation, while the second creates a variable-width histogram in which each bin has the *same width when plotted on a logarithmic horizontal axis*. [This is called *logarithmic binning*.]{.aside} </span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> degree_sequence(G):</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    degrees <span class="op">=</span> nx.degree(G)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    degree_sequence <span class="op">=</span> np.array([deg[<span class="dv">1</span>] <span class="cf">for</span> deg <span class="kw">in</span> degrees])</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> degree_sequence</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_binned_histogram(degree_sequence, interval <span class="op">=</span> <span class="dv">5</span>, num_bins <span class="op">=</span> <span class="dv">20</span>):</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    hist, bins <span class="op">=</span> np.histogram(degree_sequence, bins <span class="op">=</span> <span class="bu">min</span>(<span class="bu">int</span>(<span class="bu">len</span>(degree_sequence)<span class="op">/</span>interval), num_bins))</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    bins <span class="op">=</span> np.logspace(np.log10(bins[<span class="dv">0</span>]),np.log10(bins[<span class="op">-</span><span class="dv">1</span>]),<span class="bu">len</span>(bins))</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    hist, bins <span class="op">=</span> np.histogram(degree_sequence, bins <span class="op">=</span> bins)</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    binwidths <span class="op">=</span> bins[<span class="dv">1</span>:] <span class="op">-</span> bins[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    hist <span class="op">=</span> hist <span class="op">/</span> binwidths</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> hist<span class="op">/</span>hist.<span class="bu">sum</span>()</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bins[:<span class="op">-</span><span class="dv">1</span>], p</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_degree_distribution(G, <span class="op">**</span>kwargs):</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>    deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    x, p <span class="op">=</span> log_binned_histogram(deg_seq, <span class="op">**</span>kwargs)</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x, p,  facecolors<span class="op">=</span><span class="st">'none'</span>, edgecolors <span class="op">=</span>  <span class="st">'cornflowerblue'</span>, linewidth <span class="op">=</span> <span class="dv">2</span>, label <span class="op">=</span> <span class="st">"Data"</span>)</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Degree"</span>, xlim <span class="op">=</span> (<span class="fl">0.5</span>, x.<span class="bu">max</span>()<span class="op">*</span><span class="dv">2</span>))</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    plt.gca().<span class="bu">set</span>(ylabel <span class="op">=</span> <span class="st">"Density"</span>)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    plt.gca().loglog()</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plt.gca()</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>Let's use this function to inspect the degree distrubtion of the data: </span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">10</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>There are a few things to notice about this degree distribution. First, most nodes have relatively small degrees, fewer tham the mean of 9.9. However, there are a small number of nodes that have degrees which are *much* larger: almost two orders of magnitude larger! You can think of these nodes as "super stars" or "hubs" of the network; they may correspond to especially popular or influential accounts. </span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>Recall that, from our discussion of the Erdős–Rényi model $G(n,p)$, the degree distribution of a $G(n,p)$ model with mean degree $\bar{d}$ is approximately Poisson with mean $\bar{d}$. Let's compare this Poisson distribution to the data. </span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>mean_degree  <span class="op">=</span> deg_seq.mean()</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>d_      <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">30</span>, <span class="dv">1</span>)</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>poisson <span class="op">=</span> np.exp(<span class="op">-</span>mean_degree)<span class="op">*</span>(mean_degree<span class="op">**</span>d_)<span class="op">/</span>factorial(d_)</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">10</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>ax.plot(d_, poisson,  linewidth <span class="op">=</span> <span class="dv">1</span>, label <span class="op">=</span> <span class="st">"Poisson fit"</span>, color <span class="op">=</span> <span class="st">"grey"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>Comparing the Poisson fit predicted by the Erdős–Rényi model to the actual data, we see that the the Poisson places much higher probability mass on degrees that are *close to the mean* of 9.9. The Poisson would predict that almost no nodes would have degree higher than $10^2$, while in the data there are several. </span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>We often say that the Poisson has a "light right tail" -- the probability mass allocated by the Poisson dramatically drops off as we move to the right of the mean. In contrast, the data itself appears to have a "heavy tail": there is substantial probability mass even far to the right from the mean. </span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="fu">## Power Laws As Models of Heavy-Tailed Distributions</span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>There are many probability distributions that have heavy tails. By far the most important (and controversial) in the history of network science is the *power law degree distribution*. </span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>::: {#def-power-law}</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="fu">## Power Law Distribution</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a>A random variable $D$ has a *discrete power law distribution* with cutoff $d_*$ and exponent $\gamma &gt; 1$ if its probability mass function is has the form </span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>    p_d \triangleq \mathbb{P}<span class="co">[</span><span class="ot">D = d</span><span class="co">]</span> = C d^{-\gamma}\;,</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>$$ {#eq-power-law}</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>for all $d &gt; d_*$. Here, $C$ is a normalizing constant that ensures that the distribution sums to $1$. The entries of the distribution for $d \leq d_*$ are are arbitrary. </span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>An important intuitive insight about power law distributions is that they are *linear on log-log axes*. To see this, we can take the logarithm of both sides in @eq-power-law: </span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>    \log p_d = \log C - \gamma \log d\;.</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>So, $\log p_d$ is a linear function of $\log d$ with slope $-\gamma$. </span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>Let's try plotting such a distribution against the data. Since the power law distribution is defined for all $d &gt; d_*$, we'll need to choose a cutoff $d_*$ and an exponent $\gamma$. For now, we'll do this by eye. If we inspect the plot of the data above, it looks like  linear behavior takes over somewhere around $d_* = 10^\frac{3}{2} \approx 30$. </span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a>deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a>cutoff  <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>d_      <span class="op">=</span> np.arange(cutoff, deg_seq.<span class="bu">max</span>(), <span class="dv">1</span>)</span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>gamma   <span class="op">=</span> <span class="fl">2.7</span></span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>power_law <span class="op">=</span> <span class="dv">18</span><span class="op">*</span>d_<span class="op">**</span>(<span class="op">-</span>gamma)</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">10</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a>ax.plot(d_, power_law,  linewidth <span class="op">=</span> <span class="dv">1</span>, label <span class="op">=</span> <span class="vs">fr"Power law: $\gamma = </span><span class="sc">{</span>gamma<span class="sc">}</span><span class="vs">$"</span> , color <span class="op">=</span> <span class="st">"grey"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>The power law appears to be a much better fit than the Poisson to the tail of the distribution, making apparently reasonable predictions about the numbers of nodes with very high degrees. <span class="co">[</span><span class="ot">Where do the parameters for the power law come from? Here we performed a fit "by eye", but we discuss some systematic approaches in @sec-power-law-estimation</span><span class="co">]</span>{.aside}</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>The claim that a given network "follows a power law" is a bit murky: like other models, power laws are idealizations that no real data set matches exactly. The idea of a power law is also fundamentally asymptotic in nature: the power law that we fit to the data also predicts that we should see nodes of degree $10^3$, $10^4$, or $10^5$ if we were to allow the network to keep growing. Since the network can't keep growing (it's data, we have a finite amount of it), we have to view the power law's predictions about very high-degree nodes as extrapolations toward an idealized, infinite-size data set to which we obviously do not have access. </span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistical Properties of Power Laws</span></span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a>Power law distributions are much more *variable* than, say, Poisson distributions. Indeed, when $\gamma \leq 3$, the variance of a power law distribution is infinite. To show this, we calculate the second moment of the distribution: </span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>    \mathbb{E}<span class="co">[</span><span class="ot">D^2</span><span class="co">]</span> &amp;= \sum_{d = 1}^\infty p_d d^2 <span class="sc">\\</span> </span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>                    &amp;= \sum_{d = 1}^{d_*} p_d d^2 + \sum_{d = d_* + 1}^\infty d^2 C d^{-\gamma} <span class="sc">\\</span></span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>                    &amp;= K + C \sum_{d = d_*}^\infty d^{2-\gamma}\;,</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a>$$      </span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a>where $K$ is a (finite) constant. Now we draw upon a fact from calculus: the sequence $\sum_{x = 1}^\infty x^{-p}$ converges if and only if $p &gt; 1$. This means that the sum $\sum_{d = d_*}^\infty d^{2-\gamma}$ converges if and only if $2 - \gamma &lt; -1$, which requires $\gamma &gt; 3$. If $\gamma \leq 3$, the second moment (and therefore the variance) is undefined. Heuristically, this means that the distribution can generate samples which are are arbitrarily far from the mean, much as the data above includes samples which are orders of magnitude higher than the mean. </span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a><span class="fu">### Purported Universality of Power Laws</span></span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>@barabasiEmergenceScalingRandom1999 made the first published that power-law degree distributions are very common in real-world networks. This initial paper has since been cited over 46,000 times (as of August 2024). *Many* subsequent studies have fit power-law tails to degree distributions in empirical networks across social, technological, and biological domains. <span class="co">[</span><span class="ot">@clausetPowerLawDistributionsEmpirical2009,  @broidoScalefreeNetworksAre2019 and @holmeRareEverywherePerspectives2019 provide some review and references for these claims.</span><span class="co">]</span>{.aside}</span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a>The idea that power laws are common in real-world networks is sometimes called the "scale-free hypothesis." Part of the appeal of this hypothesis comes from statistical physics and complexity science, where power law distributions are common signatures of self-organizing systems. </span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Preferential Attachment Model: A Generative Model for Power Law Degrees</span></span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a>Why would power-law degree distributions be common in empirical networks? A common way to answer questions like this is to propose a *generative model*. A *generative model* is a random network model that is intended to produce some kind of realistic structure. [Generative models contrast with *null models* like the $G(n,p)$ model, which are usually used to *contrast* with real networks. Generative models are models of what the data is like; null models are models of what the data is *not* like.]{.aside} If the mechanism proposed by the generative model is plausible as a real, common mechanism, then we might expect that the large-scale structure generated by that model would be commonly observed. </span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a>@barabasiEmergenceScalingRandom1999 are responsible for popularizing the claim that many empirical networks are scale-free. Alongside this claim, they offered a generative model called *preferential attachment*. The preferential attachment model offers a simple mechanism of network growth which leads to power-law degree distributions. Their model is closely related to the regrettably much less famous models of @yuleMathematicalTheoryEvolution1925, @simonClassSkewDistribution1955, and @price1976general. </span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>Here's how the Yule-Simon-Price-Barabási-Albert model works. First, we start off with some initial graph $G_0$. Then, in each timestep $t=1,2,\ldots$, we: <span class="co">[</span><span class="ot">The model of @barabasiEmergenceScalingRandom1999 did not include a uniform selection mechanism, which corresponds to the case $\alpha = 1$.</span><span class="co">]</span>{.aside}</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Flip a coin with probability of heads equal to $\alpha$. If this coin lands heads, then: </span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a><span class="ss">    1. </span>Choose a node $u$ from $G_{t-1}$ with probability *proportional to its degree*. </span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Otherwise, if the coin lands tails, choose a node $u$ from $G_{t-1}$ uniformly at random. </span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Add a node $v$ to $G_{t-1}$. </span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Add edge $(u,v)$ to $G_{t-1}$. </span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a>We repeat this process as many times as desired. Intuitively, the preferential attachment model expresses the idea that "the rich get richer": nodes that already have many connections are more likely to receive new connections.</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a>Here's a quick implementation. <span class="co">[</span><span class="ot">Note: this implementation of preferential attachment is useful for illustrating the mathematics and operations with Networkx. It is, however, not efficient.</span><span class="co">]</span>{.aside}</span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a><span class="co"># initial condition</span></span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.Graph() </span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>G.add_edge(<span class="dv">0</span>, <span class="dv">1</span>) </span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">3</span><span class="op">/</span><span class="dv">5</span> <span class="co"># proportion of degree-based selection steps</span></span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a><span class="co"># main loop</span></span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a>    degrees <span class="op">=</span> nx.degree(G)</span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine u using one of two mechanisms</span></span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> alpha: </span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a>        deg_seq <span class="op">=</span> np.array([deg[<span class="dv">1</span>] <span class="cf">for</span> deg <span class="kw">in</span> degrees])</span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a>        degree_weights <span class="op">=</span> deg_seq <span class="op">/</span> deg_seq.<span class="bu">sum</span>()</span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.choice(np.arange(<span class="bu">len</span>(degrees)), p <span class="op">=</span> degree_weights)</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.choice(np.arange(<span class="bu">len</span>(degrees)))</span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>    <span class="co"># integer index of new node v</span></span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> <span class="bu">len</span>(degrees)</span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add new edge to graph    </span></span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a>    G.add_edge(u, v)</span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>Let's go ahead and plot the result. We'll add a visualization of the exponent $\gamma$ as well. How do we know the right value of $\gamma$? It turns out that there is a theoretical estimate based on $\alpha$ which we'll derive in the next section. </span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>deg_seq <span class="op">=</span> degree_sequence(G)</span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a>cutoff  <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>d_      <span class="op">=</span> np.arange(cutoff, deg_seq.<span class="bu">max</span>(), <span class="dv">1</span>)</span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a>gamma   <span class="op">=</span> (<span class="dv">2</span> <span class="op">+</span> alpha) <span class="op">/</span> alpha</span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a>power_law <span class="op">=</span> <span class="dv">10</span><span class="op">*</span>d_<span class="op">**</span>(<span class="op">-</span>gamma)</span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plot_degree_distribution(G, interval <span class="op">=</span> <span class="dv">2</span>, num_bins <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a>ax.plot(d_, power_law,  linewidth <span class="op">=</span> <span class="dv">1</span>, label <span class="op">=</span> <span class="vs">fr"Power law: $\gamma = </span><span class="sc">{</span>gamma<span class="sc">:.2f}</span><span class="vs">$"</span> , color <span class="op">=</span> <span class="st">"grey"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>)</span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a>This fit is somewhat noisy, reflecting the fact that we simulated a relatively small number of preferential attachment steps. </span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a><span class="fu">## Analyzing Preferential Attachment</span></span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a>Let's now see if we can understand mathematically why the preferential attachment model leads to networks with power-law degree distributions. There are many ways to demonstrate this fact, including both "casual" and highly rigorous techniques. Here, we'll use a "casual" argument from @mitzenmacherBriefHistoryGenerative2004. </span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a>Let $p_d^{(t)}$ be the *proportion* of nodes of degree $d \geq 2$ after algorithmic timestep $t$. Suppose that at this timestep there are $n$ nodes and $m$ edges. Then, the total *number* of nodes of degree $d$ is $n_d^{(t)} = np_d^{(t)}$. Suppose that we do one step of the preferential attachment model. Let's ask: what will be the new expected value of $n_d^{(t+1)}$? </span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a>Well, in the previous timestep there were $n_d^{(t)}$ nodes of degree $d$. How could this quantity change? There are two processes that could make $n_d^{(t+1)}$  different from $n_d^{(t)}$. If we selected a node $u$ with degree $d-1$ in the model update, then this node will become a node of degree $d$ (since it will have one new edge attached to it), and will newly count towards the total $n_d^{(t+1)}$.  On the other hand, if we select a node  $u$ of degree $d$, then this node will become a node of degree $d+1$, and therefore no longer count for $n_d^{(t+1)}$. </span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a>So, we can write down our estimate for the expected value of $n_d^{(t+1)}$. </span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a>    \mathbb{E}\left<span class="co">[</span><span class="ot">n_d^{(t+1)}\right</span><span class="co">]</span> - n_d^{(t)} = \mathbb{P}<span class="co">[</span><span class="ot">d_u = d-1</span><span class="co">]</span> - \mathbb{P}<span class="co">[</span><span class="ot">d_u = d</span><span class="co">]</span>\;.</span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a>Let's compute the probabilities appearing on the righthand side. With probability $\alpha$, we select a node from $G_t$ proportional to its degree. This means that, if a *specific* node $u$ has degree $d-1$, the probability of picking $u$ is </span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a>    \mathbb{P}<span class="co">[</span><span class="ot">u \text{ is picked}</span><span class="co">]</span> = \frac{d-1}{\sum_{w \in G} d_w} = \frac{d-1}{2m^{(t)}}\;.</span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a>Of all the nodew we could pick, $p_{d-1}^{(t)}n$ of them have degree $d-1$. So, the probability of picking a node with degree $d-1$ is $n p_{d-1}^{(t)}\frac{d-1}{2m}$. On the other hand, if we flipped a tails (with probability $1-\alpha$), then we pick a node uniformly at random; each one is equally probable and $p_{d-1}^{(t)}$ of them have degree $d-1$. So, in this case the probability is simply $p_{d-1}^{(t)}$. Combining using the law of total probability, we have </span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a>    \mathbb{P}<span class="co">[</span><span class="ot">d_u = d-1</span><span class="co">]</span> &amp;= \alpha n p_{d-1}^{(t)}\frac{d-1}{2m^{(t)}} + (1-\alpha)p_{d-1}^{(t)} <span class="sc">\\</span> </span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a>                          &amp;= \left<span class="co">[</span><span class="ot">\alpha n \frac{d-1}{2m} + (1-\alpha)\right</span><span class="co">]</span>p_{d-1}^{(t)}\;. </span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a>A similar calculation shows that </span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a>    \mathbb{P}<span class="co">[</span><span class="ot">d_u = d</span><span class="co">]</span> &amp;= \alpha n p_{d}^{(t)}\frac{d}{2m^{(t)}} + (1-\alpha)p_{d}^{(t)} <span class="sc">\\</span> </span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a>                          &amp;= \left<span class="co">[</span><span class="ot">\alpha n \frac{d}{2m} + (1-\alpha)\right</span><span class="co">]</span>p_{d}^{(t)}\;, </span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a>so our expectation is </span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a>    \mathbb{E}\left<span class="co">[</span><span class="ot">n_d^{(t+1)}\right</span><span class="co">]</span> - n_d^{(t)} = \left<span class="co">[</span><span class="ot">\alpha n \frac{d-1}{2m} + (1-\alpha)\right</span><span class="co">]</span>p_{d-1}^{(t)} - \left<span class="co">[</span><span class="ot">\alpha n \frac{d}{2m} + (1-\alpha)\right</span><span class="co">]</span>p_{d}^{(t)}\;.</span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a>Up until now, everything has been exact: no approximations involved. Now we're going to start making approximations and assumptions. These can all be justified by rigorous probabilistic arguments, but we won't do this here. </span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We'll assume that $n_d^{(t+1)}$ is equal to its expectation. </span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>In each timestep, we add one new node and one new edge. This means that, after enough timesteps, the number of nodes $n$ and number of edges $m$ should be approximately equal. We'll therefore assume that $t$ is sufficiently large that $\frac{n}{m} \approx 1$. </span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Stationarity**: we'll assume that, for sufficiently large $t$, $p_d^{(t)}$ is a *constant*: $p_d^{(t)} = p_d^{(t+1)} \triangleq p_d$. </span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a>To track these assumptions, we'll use the symbol $\doteq$ to mean "equal under these assumptions."</span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a>With these assumptions, we can simplify. First, we'll replace $\mathbb{E}\left<span class="co">[</span><span class="ot">n_d^{(t+1)}\right</span><span class="co">]</span>$ with $n_d^{(t+1)}$, which we'll write as $(n+1)p_d^{(t+1)}$ </span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a>    (n+1)p_d^{(t+1)} - np_d^{(t)} \doteq \left<span class="co">[</span><span class="ot">\alpha n \frac{d-1}{2m} + (1-\alpha)\right</span><span class="co">]</span>p_{d-1}^{(t)} - \left<span class="co">[</span><span class="ot">\alpha n \frac{d}{2m} + (1-\alpha)\right</span><span class="co">]</span>p_{d}^{(t)}\;.</span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a>Next, we'll assume $\frac{n}{m} \approx 1$: </span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a>    (n+1)p_d^{(t+1)} - np_d^{(t)} \doteq \left<span class="co">[</span><span class="ot">\alpha  \frac{d-1}{2} + (1-\alpha)\right</span><span class="co">]</span>p_{d-1}^{(t)} - \left<span class="co">[</span><span class="ot">\alpha \frac{d}{2} + (1-\alpha)\right</span><span class="co">]</span>p_{d}^{(t)}\;.</span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a>Finally, we'll assume stationarity: </span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a>    (n+1)p_d - np_d \doteq \left<span class="co">[</span><span class="ot">\alpha  \frac{d-1}{2} + (1-\alpha)\right</span><span class="co">]</span>p_{d-1} - \left<span class="co">[</span><span class="ot">\alpha \frac{d}{2} + (1-\alpha)\right</span><span class="co">]</span>p_{d}\;.</span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-331"><a href="#cb10-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-332"><a href="#cb10-332" aria-hidden="true" tabindex="-1"></a>After a long setup, this looks much more manageable! Our next step is to solve for $p_d$, from which we find </span>
<span id="cb10-333"><a href="#cb10-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-334"><a href="#cb10-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-335"><a href="#cb10-335" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-336"><a href="#cb10-336" aria-hidden="true" tabindex="-1"></a>    p_d &amp;\doteq \frac{\alpha  \frac{d-1}{2} + (1-\alpha)}{1 + \alpha \frac{d}{2} + (1-\alpha)} p_{d-1} <span class="sc">\\</span> </span>
<span id="cb10-337"><a href="#cb10-337" aria-hidden="true" tabindex="-1"></a>    &amp;= \frac{2(1-\alpha) + (d-1)\alpha}{2(1-\alpha) + 2 + d\alpha }p_{d-1} <span class="sc">\\</span></span>
<span id="cb10-338"><a href="#cb10-338" aria-hidden="true" tabindex="-1"></a>    &amp;= \left(1 - \frac{2 + \alpha }{2(1-\alpha) + 2 + d\alpha }\right)p_{d-1}\;.</span>
<span id="cb10-339"><a href="#cb10-339" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-340"><a href="#cb10-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-341"><a href="#cb10-341" aria-hidden="true" tabindex="-1"></a>When $d$ grows large, this expression is approximately </span>
<span id="cb10-342"><a href="#cb10-342" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-343"><a href="#cb10-343" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-344"><a href="#cb10-344" aria-hidden="true" tabindex="-1"></a>    p_d \simeq \left(1 - \frac{1}{d}\frac{2+\alpha}{\alpha}\right) p_{d-1}\;.</span>
<span id="cb10-345"><a href="#cb10-345" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-346"><a href="#cb10-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-347"><a href="#cb10-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-348"><a href="#cb10-348" aria-hidden="true" tabindex="-1"></a>Now for a trick "out of thin air." As $d$ grows large, </span>
<span id="cb10-349"><a href="#cb10-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-350"><a href="#cb10-350" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-351"><a href="#cb10-351" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-352"><a href="#cb10-352" aria-hidden="true" tabindex="-1"></a>    1 - \frac{1}{d}\frac{2+\alpha}{\alpha} \rightarrow \left(\frac{d-1}{d} \right)^{\frac{2 + \alpha}{\alpha}}</span>
<span id="cb10-353"><a href="#cb10-353" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-354"><a href="#cb10-354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-355"><a href="#cb10-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-356"><a href="#cb10-356" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb10-357"><a href="#cb10-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-358"><a href="#cb10-358" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercise</span></span>
<span id="cb10-359"><a href="#cb10-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-360"><a href="#cb10-360" aria-hidden="true" tabindex="-1"></a>Justify the approximation above. To do this, Taylor-expand the function $f(x) = x^{\gamma}$ to first order around the point $x_0 = 1$ and use this expansion to estimate the value of $1 - \frac{1}{d}$.</span>
<span id="cb10-361"><a href="#cb10-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-362"><a href="#cb10-362" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-363"><a href="#cb10-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-364"><a href="#cb10-364" aria-hidden="true" tabindex="-1"></a>::: {.hide .solution}</span>
<span id="cb10-365"><a href="#cb10-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-366"><a href="#cb10-366" aria-hidden="true" tabindex="-1"></a>We will use a Taylor expansion to find a first-order approximation about $x_0 = 1$:</span>
<span id="cb10-367"><a href="#cb10-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-368"><a href="#cb10-368" aria-hidden="true" tabindex="-1"></a>The Taylor expansion to first order is</span>
<span id="cb10-369"><a href="#cb10-369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-370"><a href="#cb10-370" aria-hidden="true" tabindex="-1"></a>f(x) \approx f(x_0) + f'(x_0)(x-x_0).</span>
<span id="cb10-371"><a href="#cb10-371" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-372"><a href="#cb10-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-373"><a href="#cb10-373" aria-hidden="true" tabindex="-1"></a>Let $f(x) = x^{\gamma}$. Then $f'(x) = \gamma x ^{\gamma - 1}.$ This means that</span>
<span id="cb10-374"><a href="#cb10-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-375"><a href="#cb10-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-376"><a href="#cb10-376" aria-hidden="true" tabindex="-1"></a>f(x) \approx 1 +\gamma (x-1).</span>
<span id="cb10-377"><a href="#cb10-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-378"><a href="#cb10-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-379"><a href="#cb10-379" aria-hidden="true" tabindex="-1"></a>Furthermore, note that $\frac{1}{d} &lt;&lt;1$ for $d$ large. Substituting in $x = 1-\frac{1}{d}$ and $\gamma = \frac{2+\alpha}{\alpha}$ gives</span>
<span id="cb10-380"><a href="#cb10-380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-381"><a href="#cb10-381" aria-hidden="true" tabindex="-1"></a>f(1-\frac{1}{d}) \approx 1-\frac{1}{d} \frac{2+\alpha}{\alpha}.</span>
<span id="cb10-382"><a href="#cb10-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-383"><a href="#cb10-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-384"><a href="#cb10-384" aria-hidden="true" tabindex="-1"></a>This justifies the approximation: the expression we derived could be thought of as a first-order approximation of the function $\left(\frac{d - 1}{d}\right)^{\frac{2+\alpha}{\alpha}}.$</span>
<span id="cb10-385"><a href="#cb10-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-386"><a href="#cb10-386" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-387"><a href="#cb10-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-388"><a href="#cb10-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-389"><a href="#cb10-389" aria-hidden="true" tabindex="-1"></a>Applying this last approximation, we have shown that, for sufficiently large $d$, </span>
<span id="cb10-390"><a href="#cb10-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-391"><a href="#cb10-391" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-392"><a href="#cb10-392" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-393"><a href="#cb10-393" aria-hidden="true" tabindex="-1"></a>    p_d \simeq \left(\frac{d-1}{d} \right)^{\frac{2 + \alpha}{\alpha}} p_{d-1}\;.</span>
<span id="cb10-394"><a href="#cb10-394" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-395"><a href="#cb10-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-396"><a href="#cb10-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-397"><a href="#cb10-397" aria-hidden="true" tabindex="-1"></a>This recurrence relation, if it were exact, would imply that $p_d = C d^{-\frac{2+\alpha}{\alpha}}$, as shown by the following exercise:  </span>
<span id="cb10-398"><a href="#cb10-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-399"><a href="#cb10-399" aria-hidden="true" tabindex="-1"></a>::: {.callout-important} </span>
<span id="cb10-400"><a href="#cb10-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-401"><a href="#cb10-401" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercise</span></span>
<span id="cb10-402"><a href="#cb10-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-403"><a href="#cb10-403" aria-hidden="true" tabindex="-1"></a>Suppose that $p_d$ is a probability distribution with the property that, for some $d_*$ and for all $d &gt; d_*$, it holds that </span>
<span id="cb10-404"><a href="#cb10-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-405"><a href="#cb10-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-406"><a href="#cb10-406" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-407"><a href="#cb10-407" aria-hidden="true" tabindex="-1"></a>    p_d = \left(\frac{d-1}{d}\right)^{\gamma} p_{d-1}\;.</span>
<span id="cb10-408"><a href="#cb10-408" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-409"><a href="#cb10-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-410"><a href="#cb10-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-411"><a href="#cb10-411" aria-hidden="true" tabindex="-1"></a>Prove using induction that $p_d = C d^{-\gamma}$ for some constant $C$, and explain how to compute $C$. </span>
<span id="cb10-412"><a href="#cb10-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-413"><a href="#cb10-413" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-414"><a href="#cb10-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-415"><a href="#cb10-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-416"><a href="#cb10-416" aria-hidden="true" tabindex="-1"></a>::: {.hide .solution}</span>
<span id="cb10-417"><a href="#cb10-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-418"><a href="#cb10-418" aria-hidden="true" tabindex="-1"></a>Our proof is by induction. </span>
<span id="cb10-419"><a href="#cb10-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-420"><a href="#cb10-420" aria-hidden="true" tabindex="-1"></a>**Base Case:** Since the statement is claimed to hold for all $d &gt; d_*$, our base case is $d = d_* + 1$. In this case we have $p_d = \left(\frac{d_*}{d}\right)^{\gamma} p_{d_*} = \left[p_{d_*}d_*^{\gamma}\right]d^{-\gamma}$. We'll let $C = p_{d_*}d_*^\gamma$. </span>
<span id="cb10-421"><a href="#cb10-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-422"><a href="#cb10-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-423"><a href="#cb10-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-424"><a href="#cb10-424" aria-hidden="true" tabindex="-1"></a>**Inductive Step:**  Suppose that the statement holds for some arbitrary $d = \hat{d} &gt; d_*$. We'll show that the statement also holds for $d = \hat{d} + 1$. We can calculate directly </span>
<span id="cb10-425"><a href="#cb10-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-426"><a href="#cb10-426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-427"><a href="#cb10-427" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-428"><a href="#cb10-428" aria-hidden="true" tabindex="-1"></a>    p_{\hat{d} +  1} &amp;= \left(\frac{\hat{d}}{\hat{d} + 1}\right)^{\gamma} p_{\hat{d}} <span class="sc">\\</span> </span>
<span id="cb10-429"><a href="#cb10-429" aria-hidden="true" tabindex="-1"></a>    &amp;= C \left(\frac{\hat{d}}{\hat{d} + 1}\right)^{\gamma} \hat{d}^{-\gamma} <span class="sc">\\</span></span>
<span id="cb10-430"><a href="#cb10-430" aria-hidden="true" tabindex="-1"></a>    &amp;= C (\hat{d} + 1)^{-\gamma}\;.</span>
<span id="cb10-431"><a href="#cb10-431" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-432"><a href="#cb10-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-433"><a href="#cb10-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-434"><a href="#cb10-434" aria-hidden="true" tabindex="-1"></a>This completes the inductive step and the proof. </span>
<span id="cb10-435"><a href="#cb10-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-436"><a href="#cb10-436" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-437"><a href="#cb10-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-438"><a href="#cb10-438" aria-hidden="true" tabindex="-1"></a>This concludes our argument. Although this argument contains many approximations, it is also possible to reach the same conclusion using fully rigorous probabilistic arguments <span class="co">[</span><span class="ot">@bollobasDegreeSequenceScale2001</span><span class="co">]</span>. </span>
<span id="cb10-439"><a href="#cb10-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-440"><a href="#cb10-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-441"><a href="#cb10-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-442"><a href="#cb10-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-443"><a href="#cb10-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-444"><a href="#cb10-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-445"><a href="#cb10-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-446"><a href="#cb10-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-447"><a href="#cb10-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-448"><a href="#cb10-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-449"><a href="#cb10-449" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimating Power Laws From Data {#sec-power-law-estimation}</span></span>
<span id="cb10-450"><a href="#cb10-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-451"><a href="#cb10-451" aria-hidden="true" tabindex="-1"></a>After the publication of @barabasiEmergenceScalingRandom1999, there was a proliferation of papers purporting to find power-law degree distributions in empirical networks. For a time, the standard method for estimating the exponent $\gamma$ was to use the key visual signature of power laws -- power laws are linear on log-log axes. This suggests performing linear regression in log-log space; the slope of the regression line is the estimate of $\gamma$. This approach, however, is badly flawed: errors can be large, and uncertainty quantification is not reliably available. @clausetPowerLawDistributionsEmpirical2009 discuss this problem in greater detail, and propose an alternative scheme based on maximum likelihood estimation and goodness-of-fit tests. Although the exact maximum-likelihood estimate of $\gamma$ is the output of a maximization problem and is not available in closed form, the authors supply a relatively accurate approximation: </span>
<span id="cb10-452"><a href="#cb10-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-453"><a href="#cb10-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-454"><a href="#cb10-454" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb10-455"><a href="#cb10-455" aria-hidden="true" tabindex="-1"></a>    \hat{\gamma} = 1 + n\left(\sum_{i=1}^n \log \frac{d_i}{d_*}\right)^{-1}\;.</span>
<span id="cb10-456"><a href="#cb10-456" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb10-457"><a href="#cb10-457" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-458"><a href="#cb10-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-459"><a href="#cb10-459" aria-hidden="true" tabindex="-1"></a>As they show, this estimate and related methods are much more reliable estimators of $\gamma$ than the linear regression method. </span>
<span id="cb10-460"><a href="#cb10-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-461"><a href="#cb10-461" aria-hidden="true" tabindex="-1"></a>An important cautionary note: the estimate $\hat{\gamma}$ can be formed regardless of whether or not the power law is a good descriptor of the data. Supplementary methods such as goodness-of-fit tests are necessary to determine whether a power law is appropriate at all. @clausetPowerLawDistributionsEmpirical2009 give some guidance on such methods as well. </span>
<span id="cb10-462"><a href="#cb10-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-463"><a href="#cb10-463" aria-hidden="true" tabindex="-1"></a><span class="fu">### Preferential Attachment in Growing Graphs</span></span>
<span id="cb10-464"><a href="#cb10-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-465"><a href="#cb10-465" aria-hidden="true" tabindex="-1"></a>What if we are able to observe more than the degree distribution of a network? What if we could also observe the growth of the network, and actually know which edges were added at which times? Under such circumstances, it is possible to estimate more directly the extent to which a graph might grow via the preferential attachment mechanism, possibly alongside additional mechanisms. </span>
<span id="cb10-466"><a href="#cb10-466" aria-hidden="true" tabindex="-1"></a>@overgoorChoosingGrowGraph2019 supply details on how to estimate the parameters of a general class of models, including preferential attachment, from observed network growth.</span>
<span id="cb10-467"><a href="#cb10-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-468"><a href="#cb10-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-469"><a href="#cb10-469" aria-hidden="true" tabindex="-1"></a><span class="fu">## Are Power Laws Good Descriptors of Real-World Networks?</span></span>
<span id="cb10-470"><a href="#cb10-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-471"><a href="#cb10-471" aria-hidden="true" tabindex="-1"></a>Are power laws *really* that common in empirical data? @broidoScalefreeNetworksAre2019 controversially claimed that *scale free networks are rare*. In a bit more detail, the authors compare power-law  distributions to several competing distributions as models of real-world network degree sequences. The authors find that that the competing models---especially <span class="co">[</span><span class="ot">lognormal distributions</span><span class="co">](https://en.wikipedia.org/wiki/Log-normal_distribution)</span>, which also have heavy tails---are often better fits to observed data than power laws. This paper stirred considerable controversy, which is briefly documented by @holmeRareEverywherePerspectives2019. </span>
<span id="cb10-472"><a href="#cb10-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-473"><a href="#cb10-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-474"><a href="#cb10-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-475"><a href="#cb10-475" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>