<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.10">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Random Graphs: Erdős–Rényi – Network Science: Models, Mathematics, and Computation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/10-configuration-model.html" rel="next">
<link href="../chapters/08-power-laws.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2d9718c933debafcce942f9b212640bc.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c1bdd270c1c0708cd2ff05417efafcc5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/09-random-graphs.html">Models of Networks</a></li><li class="breadcrumb-item"><a href="../chapters/09-random-graphs.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Network Science: Models, Mathematics, and Computation</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Network Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-networkrepresentations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Networks and Their Representations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-degree-walks-paths.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Degree, Walks, and Paths</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-components-laplacian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Components and the Graph Laplacian</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Measuring Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-centrality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Centrality and Importance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Networks, and Why You Shouldn’t</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-modularity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Homophily, assortativity, and modularity</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Real-World Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-real-world.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Structure of Empirical Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-power-laws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power Law Degree Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Models of Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-random-graphs.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-configuration-model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Configuration models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-generating-functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probability Generating Functions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Network Algorithms</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-modularity-maximization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Community Detection and Modularity Maximization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-spectral-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Spectral Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/41-link-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Link Prediction and Feedback Loops</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Applications and Extensions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/50-random-walks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Random Walks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/51-agent-based-modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Agent-Based Modeling on Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/52-epidemiology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Epidemic Models on Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Appendices</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#why-random-graphs" id="toc-why-random-graphs" class="nav-link active" data-scroll-target="#why-random-graphs">Why Random Graphs?</a>
  <ul class="collapse">
  <li><a href="#random-graphs-as-insightful-models" id="toc-random-graphs-as-insightful-models" class="nav-link" data-scroll-target="#random-graphs-as-insightful-models">Random Graphs as Insightful Models</a></li>
  <li><a href="#random-graphs-as-mathematical-puzzles" id="toc-random-graphs-as-mathematical-puzzles" class="nav-link" data-scroll-target="#random-graphs-as-mathematical-puzzles">Random Graphs as Mathematical Puzzles</a></li>
  <li><a href="#random-graphs-as-null-hypotheses" id="toc-random-graphs-as-null-hypotheses" class="nav-link" data-scroll-target="#random-graphs-as-null-hypotheses">Random Graphs as Null Hypotheses</a></li>
  <li><a href="#random-graphs-and-statistical-inference" id="toc-random-graphs-and-statistical-inference" class="nav-link" data-scroll-target="#random-graphs-and-statistical-inference">Random Graphs and Statistical Inference</a></li>
  </ul></li>
  <li><a href="#the-gnp-model-erdősrényi" id="toc-the-gnp-model-erdősrényi" class="nav-link" data-scroll-target="#the-gnp-model-erdősrényi">The <span class="math inline">\(G(n,p)\)</span> Model (Erdős–Rényi)</a>
  <ul class="collapse">
  <li><a href="#clustering-coefficient" id="toc-clustering-coefficient" class="nav-link" data-scroll-target="#clustering-coefficient">Clustering Coefficient</a></li>
  </ul></li>
  <li><a href="#sparsity" id="toc-sparsity" class="nav-link" data-scroll-target="#sparsity">Sparsity</a>
  <ul class="collapse">
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering">Clustering</a></li>
  <li><a href="#cycles-and-local-tree-likeness" id="toc-cycles-and-local-tree-likeness" class="nav-link" data-scroll-target="#cycles-and-local-tree-likeness">Cycles and Local Tree-Likeness</a></li>
  <li><a href="#path-lengths" id="toc-path-lengths" class="nav-link" data-scroll-target="#path-lengths">Path Lengths</a></li>
  <li><a href="#a-caveat" id="toc-a-caveat" class="nav-link" data-scroll-target="#a-caveat">A Caveat</a></li>
  </ul></li>
  <li><a href="#component-sizes-and-the-branching-process-approximation" id="toc-component-sizes-and-the-branching-process-approximation" class="nav-link" data-scroll-target="#component-sizes-and-the-branching-process-approximation">Component Sizes and the Branching Process Approximation</a>
  <ul class="collapse">
  <li><a href="#application-to-erdősrényi" id="toc-application-to-erdősrényi" class="nav-link" data-scroll-target="#application-to-erdősrényi">Application to Erdős–Rényi</a></li>
  <li><a href="#the-subcritical-case" id="toc-the-subcritical-case" class="nav-link" data-scroll-target="#the-subcritical-case">The Subcritical Case</a></li>
  <li><a href="#the-giant-component" id="toc-the-giant-component" class="nav-link" data-scroll-target="#the-giant-component">The Giant Component</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/09-random-graphs.html">Models of Networks</a></li><li class="breadcrumb-item"><a href="../chapters/09-random-graphs.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Random Graphs: Erdős–Rényi</span></a></li></ol></nav>
<div class="quarto-title">
</div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>Open the live notebook in Google Colab <a href="https://colab.research.google.com/github/network-science-notes/network-science-notes.github.io/blob/main/docs/live-notebooks/09-random-graphs.ipynb">here</a>.</em></p>
<p>A random graph is a probability distribution over graphs. In this set of lecture notes, we’ll begin our exploration of several random graphs.</p>
<section id="why-random-graphs" class="level2">
<h2 class="anchored" data-anchor-id="why-random-graphs">Why Random Graphs?</h2>
<p>Why should we even study random graphs? There are a few reasons!</p>
<section id="random-graphs-as-insightful-models" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-as-insightful-models">Random Graphs as Insightful Models</h3>
<p>Random graphs allow us to build our intuition and skills in the study of networks. In many simple random graphs, quantities of interest (clustering coefficients, diameters, etc) can be calculated with pencil and paper. This allows us to build mathematical insight into the structure of many real-world models, without the need for detailed simulations.</p>
</section>
<section id="random-graphs-as-mathematical-puzzles" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-as-mathematical-puzzles">Random Graphs as Mathematical Puzzles</h3>
<p>Many properties of even simple random graphs are still under investigation by research mathematicians. While some properties can be calculated simply, others require extremely sophisticated machinery in order to understand. There are many mathematicians who spend their careers studying random graphs, and their cousins, random matrices.</p>
</section>
<section id="random-graphs-as-null-hypotheses" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-as-null-hypotheses">Random Graphs as Null Hypotheses</h3>
<p>Suppose that you compute the global clustering coefficient of a graph and find it to be <span class="math inline">\(0.31\)</span>. How do we interpret that? Is that high? Low? In this case, we should ask: <em>compared to what?</em> Random graphs allow us one way to make a comparison: that clustering coefficient is “high” if it’s larger than the clustering coefficient in a suitably chosen, comparable random graph. How to choose a comparable random graph is an important question, and the answer is not always clear!</p>
<p>In this way, random graphs often serve as the <em>null hypothesis</em>, in exactly the same way you might have heard of the null hypothesis for other kinds of statistical tests.</p>
</section>
<section id="random-graphs-and-statistical-inference" class="level3">
<h3 class="anchored" data-anchor-id="random-graphs-and-statistical-inference">Random Graphs and Statistical Inference</h3>
<p>Many statistical algorithms for tasks like graph clustering, ranking, and prediction come from random graph models. The idea, generally speaking, is to imagine that we observe a graph, and then try to make the best educated guess possible about the model that generated that graph. This is classical statistical inference.</p>
</section>
</section>
<section id="the-gnp-model-erdősrényi" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-gnp-model-erdősrényi">The <span class="math inline">\(G(n,p)\)</span> Model (Erdős–Rényi)</h2>
<div class="page-columns page-full"><p>The model of <span class="citation" data-cites="erdHos1960evolution">Erdős and Rényi (<a href="#ref-erdHos1960evolution" role="doc-biblioref">1960</a>)</span> is the simplest and most fundamental model of a random graph. Our primary interest in the ER model is for mathematical insight and null modeling. The ER model is <em>mostly</em> understood in its major mathematical properties, and it’s almost never used in statistical inference. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">The ER model is, however, a building block of models that <em>are</em> used in statistical inference. The <a href="https://en.wikipedia.org/wiki/Stochastic_block_model">stochastic blockmodel</a>, for example, is often used for graph clustering and community detection. In its simplest form, it’s a bunch of ER graphs glued together.</span></div></div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-ER" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.1 (Erdős–Rényi Random Graph)</strong></span> An <em>Erdős–Rényi random graph</em> with <span class="math inline">\(n\)</span> nodes and connection probability <span class="math inline">\(p\)</span>, written <span class="math inline">\(G(n,p)\)</span>, is a random graph constructed by placing an edge with probability <span class="math inline">\(p\)</span> between each pair of distinct nodes.</p>
</div>
</div>
</div>
</div>
<p>We can imagine visiting each possible pair of edges <span class="math inline">\((i,j)\)</span> and flipping a coin with probability of heads <span class="math inline">\(p\)</span>. If heads, we add <span class="math inline">\((i,j) \in E\)</span>; otherwise, we don’t.</p>
<div class="callout callout-style-simple callout-caution no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Exercise</strong>: Let <span class="math inline">\(i\)</span> be a fixed node in a <span class="math inline">\(G(n,p)\)</span> graph, and let <span class="math inline">\(K_i\)</span> be its (random) degree. Show that <span class="math inline">\(K_i\)</span> has binomial distribution with success probability <span class="math inline">\(p\)</span> and <span class="math inline">\(n-1\)</span> trials.</p>
</div>
</div>
</div>

<div class="callout callout-style-simple callout-caution no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Exercise</strong>: Show that <span class="math inline">\(\mathbb{E}[K_i] = p(n-1)\)</span>.</p>
</div>
</div>
</div>

<section id="clustering-coefficient" class="level3">
<h3 class="anchored" data-anchor-id="clustering-coefficient">Clustering Coefficient</h3>
<p>Both local and global clustering coefficients are defined in terms of a ratio of realized triangles to possible triangles. Analyzing ratios using probability theory can get tricky, but we can get a pretty reliable picture of things by computing the expectations of the numerator and denominator separately.</p>
<p>Let’s take the global clustering coefficient. For this, we need to compute the total number of triangles, and the total number of wedges.</p>
<p>How many triangles are there? Well, there are <span class="math inline">\(\binom{n}{3}\)</span> ways to choose 3 nodes from all the possibilities, and the probability that all three edges exist to form the triangle is <span class="math inline">\(p^3\)</span>. So, in expectation, there are <span class="math inline">\(\binom{n}{3}p^3\)</span> triangles in the graph.</p>
<p>How many wedges are there? Well, each triple of nodes contains three possible wedges, and the probability of any given wedge existing is <span class="math inline">\(p^2\)</span>. So, the expected number of wedges is <span class="math inline">\(\binom{n}{3}p^2\)</span>. Our estimate for the expected clustering coefficient is that it should be <em>about</em> <span class="math inline">\(p\)</span>, although we have been fast and loose with several mathematical details to arrive at this conclusion.</p>
</section>
</section>
<section id="sparsity" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sparsity">Sparsity</h2>
<p>Recall that it is possible to define sparsity more explicitly when we have a theoretical model (where it makes sense to take limits). Indeed, the <em>sparse</em> Erdős–Rényi model is very useful. Intuitively, the idea of sparsity is that there are not very many edges in comparison to the number of nodes.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-sparse-ER" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.2</strong></span> We say that a <span class="math inline">\(G(n,p)\)</span> graph is <strong>sparse</strong> when <span class="math inline">\(p = c/(n-1)\)</span> for some constant <span class="math inline">\(c\)</span>.</p>
</div>
</div>
</div>
</div>
<p>A consequence of sparsity is that <span class="math inline">\(\mathbb{E}[K_i] = c\)</span>; i.e.&nbsp;the expected degree of a node in sparse <span class="math inline">\(G(n,p)\)</span> is constant. When studying sparse <span class="math inline">\(G(n,p)\)</span>, we are almost always interested in the case <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
<section id="clustering" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="clustering">Clustering</h3>
<p>What does this imply for our estimation of the global clustering coefficient from before? Well, we expect the global clustering coefficient to be <em>about</em> <span class="math inline">\(p\)</span>, and if <span class="math inline">\(p = c/(n-1)\)</span>, then <span class="math inline">\(p \rightarrow 0\)</span> as <span class="math inline">\(n \to \infty\)</span> for sparse <span class="math inline">\(G(n,p)\)</span>.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside">The need to move beyond the ER model to develop sparse graphs with clustering coefficients was part of the motivation of <span class="citation" data-cites="watts1998collective">Watts and Strogatz (<a href="#ref-watts1998collective" role="doc-biblioref">1998</a>)</span>, a famous paper that introduced the “small world model.”</span></div></div>
<blockquote class="blockquote">
<p>Sparse Erdős–Rényi graphs have vanishing clustering coefficients.</p>
</blockquote>
<p><a href="#fig-clustering-decay" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> shows how the global clustering coefficient of a sparse Erdős–Rényi random graph decays as we increase <span class="math inline">\(n\)</span>. Although the estimate that the global clustering coefficient should be equal to <span class="math inline">\(p\)</span> was somewhat informal, experimentally it works quite well.</p>
<div id="cell-fig-clustering-decay" class="cell page-columns page-full" data-fig.cap="Each point gives the global clustering coefficient of an ER graph with mean degree $c = 5$ and specified number of nodes. The black line gives the estimate $T = p = c/(n-1)$ for the clustering coefficient that we computed earlier. The function `nx.fast_gnp_random_graph()` is a special function from NetworkX for generating ER random graphs quickly; you'll learn about how it works in an upcoming homework assignment." data-out.width="80%" data-execution_count="1" data-cap-location="undefined">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb1" data-cap-location="undefined"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> np.repeat(<span class="dv">2</span><span class="op">**</span>np.arange(<span class="dv">5</span>, <span class="dv">15</span>), <span class="dv">10</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> global_clustering(n, c):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, c<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nx.transitivity(G)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> [global_clustering(n, c) <span class="cf">for</span> n <span class="kw">in</span> N]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>theory <span class="op">=</span> ax.plot(N, c<span class="op">/</span>(N<span class="op">-</span><span class="dv">1</span>), color <span class="op">=</span> <span class="st">"black"</span>, label <span class="op">=</span> <span class="st">"Estimate"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>exp <span class="op">=</span> ax.scatter(N, T, label <span class="op">=</span> <span class="st">"Experiments"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>semil <span class="op">=</span> ax.semilogx()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Number of nodes"</span>, </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>       ylabel <span class="op">=</span> <span class="st">"Global clustering coefficient"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-clustering-decay" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" width="597" height="431">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-clustering-decay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-random-graphs_files/figure-html/fig-clustering-decay-output-1.png" id="fig-clustering-decay" width="597" height="431" class="figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig quarto-uncaptioned margin-caption" id="fig-clustering-decay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="cycles-and-local-tree-likeness" class="level3">
<h3 class="anchored" data-anchor-id="cycles-and-local-tree-likeness">Cycles and Local Tree-Likeness</h3>
<p>Recall the following definition:</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-cycle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.3</strong></span> A <strong>cycle</strong> is a walk that does not repeat edges and ends at the same node that it begins.</p>
</div>
</div>
</div>
</div>
<p>A triangle is an example of a cycle of length <span class="math inline">\(3\)</span>.</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-rare-cycles" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9.1</strong></span> In the sparse <span class="math inline">\(G(n,p)\)</span> model, for any length <span class="math inline">\(k\)</span>, the probability that there exists a cycle of length <span class="math inline">\(k\)</span> attached to node <span class="math inline">\(i\)</span> shrinks to 0 as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</div>
</div>
</div>
</div>
<p>You’ll prove a generalization of <a href="#thm-rare-cycles" class="quarto-xref">Theorem&nbsp;<span>9.1</span></a> in an upcoming homework problem.</p>
<p>Graphs in which cycles are very rare are often called <em>locally tree-like</em>. A tree is a graph without cycles; if cycles are very rare, then we can often use techniques that are normally guaranteed to only work on trees without running into (too much) trouble.</p>
</section>
<section id="path-lengths" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="path-lengths">Path Lengths</h3>
<div class="page-columns page-full"><p>How far apart are two nodes in <span class="math inline">\(G(n,p)\)</span>? Again, exactly computing the length of geodesic paths involves some challenging mathematical detail.  However, we can get a big-picture view of the situation by asking a slightly different question:</p><div class="no-row-height column-margin column-container"><span class="margin-aside">See <span class="citation" data-cites="riordan2010diameter">Riordan and Wormald (<a href="#ref-riordan2010diameter" role="doc-biblioref">2010</a>)</span> and references therein.</span></div></div>
<blockquote class="blockquote">
<p>Given two nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, what is the expected number of paths of length <span class="math inline">\(k\)</span> between them?</p>
</blockquote>
<p>Let <span class="math inline">\(R(k)\)</span> denote the number of <span class="math inline">\(k\)</span>-paths between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. Let <span class="math inline">\(r(k) = \mathbb{E}[R(k)]\)</span>. Let’s estimate <span class="math inline">\(r(k)\)</span>.</p>
<p>First, we know that <span class="math inline">\(r(1) = p\)</span>. For higher values of <span class="math inline">\(k\)</span>, we’ll use the following idea: in order for there to be a path of length <span class="math inline">\(k\)</span> from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>, there must be a node <span class="math inline">\(\ell\)</span> such that:</p>
<ul>
<li>There exists a path from <span class="math inline">\(i\)</span> to <span class="math inline">\(\ell\)</span> of length <span class="math inline">\(k-1\)</span>. In expectation, there are <span class="math inline">\(r(k-1)\)</span> of these.</li>
<li>There exists a path from <span class="math inline">\(\ell\)</span> to <span class="math inline">\(j\)</span> of length <span class="math inline">\(1\)</span>. This happens with probability <span class="math inline">\(p\)</span>.</li>
</ul>
<p>There are <span class="math inline">\(n-2\)</span> possibilities for <span class="math inline">\(\ell\)</span> (excluding <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>), and so we obtain the approximate relation</p>
<p><span class="math display">\[
r(k) \approx (n-2)r(k-1)p\;.
\]</span></p>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Why is this an approximation? Well, some of the paths between <span class="math inline">\(i\)</span> and <span class="math inline">\(\ell\)</span> that are counted in <span class="math inline">\(r(k-1)\)</span> could actually include the edge <span class="math inline">\((j, \ell)\)</span> <em>already</em>. An example is <span class="math inline">\((i,j), (j,\ell)\)</span>. In this case, the presence of edge <span class="math inline">\((j,\ell)\)</span> is not independent of the presence of the path between <span class="math inline">\(i\)</span> and <span class="math inline">\(\ell\)</span>. The derivation above implicitly treats these two events as independent. Again, because <em>cycles are rare in large, sparse ER</em>, this effect is small when <span class="math inline">\(k\)</span> is small.</p>
</div>
</div>
</div>
<p>Proceeding inductively and approximating <span class="math inline">\(n-2 \approx n-1\)</span> for <span class="math inline">\(n\)</span> large, we have the relation</p>
<p><span id="eq-path-expectation"><span class="math display">\[
r(k) \approx (n-1)^{k-1}p^{k-1}r(1) = c^{k-1}p
\tag{9.1}\]</span></span></p>
<p>in the sparse ER model.</p>
<p>Using this result, let’s ask a new question:</p>
<blockquote class="blockquote">
<p>What path length <span class="math inline">\(k\)</span> do I need to allow to be confident that there’s a path between nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>?</p>
</blockquote>
<p>Well, suppose we want there to be <span class="math inline">\(q\)</span> paths. Then, we can solve <span class="math inline">\(q = c^{k-1}p\)</span> for <span class="math inline">\(k\)</span>, which gives us:</p>
<p><span class="math display">\[
\begin{aligned}
q &amp;= c^{k-1}p \\
\log q &amp;= (k-1)\log c + \log p \\
\log q &amp;= (k-1)\log c + \log c - \log n \\
\frac{\log q + \log n}{\log c} &amp;= k
\end{aligned}
\]</span></p>
<p>Here, I’ve also approximated <span class="math inline">\(\log n-1 \approx \log n\)</span> for <span class="math inline">\(n\)</span> large.</p>
<p>So, supposing that I want there to be at least one path in expectation (<span class="math inline">\(q = 1\)</span>), I need to allow <span class="math inline">\(k = \frac{\log n}{\log c}\)</span>. This is pretty short, actually! For example, the population of the world is about <span class="math inline">\(8\times 10^9\)</span>, and Newman estimates that an average individual knows around 1,000 other people; that is, <span class="math inline">\(c = 10^3\)</span> in the world social network. The resulting value of <span class="math inline">\(k\)</span> here is around 3.3.</p>
<p>In other words, this calculation suggests that, if the world were an ER network, it would be the case that any two individuals would be pretty likely to have at least one path between them of length no longer than <span class="math inline">\(4\)</span>.</p>
<p>More formal calculations regarding the diameter of the ER graph confirm that the diameter of the ER graph grows slowly as a function of <span class="math inline">\(n\)</span>, even in relatively sparse cases.</p>
</section>
<section id="a-caveat" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="a-caveat">A Caveat</h3>
<p>If you spend some time looking at <a href="#eq-path-expectation" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>, you might find yourself wondering:</p>
<blockquote class="blockquote">
<p>Hey, what happens if <span class="math inline">\(c \leq 1\)</span>?</p>
</blockquote>
<p>Indeed, something <em>very</em> interesting happens here. Let’s assume <span class="math inline">\(c &lt; 1\)</span> (i.e.&nbsp;we’re ignoring the case <span class="math inline">\(c = 1\)</span>), and estimate the expected number of paths between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of <em>any</em> length. Using <a href="#eq-path-expectation" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>, we get</p>
<p><span class="math display">\[
\mathbb{E}\left[\sum_{k = 1}^{\infty} R(k)\right] = \sum_{k = 1}^\infty c^{k-1}p = \sum_{k = 0}^\infty c^kp = \frac{p}{1-c}\;.
\]</span></p>
<div class="page-columns page-full"><p>If we now use Markov’s inequality,  we find that the probability that there is a path of <em>any</em> length between nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is no larger than <span class="math inline">\(\frac{p}{1-c}.\)</span> In the sparse regime, we can substitute <span class="math inline">\(p = \frac{c}{n-1}\)</span> to see that</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Markov’s inequality states that <span class="math inline">\(\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}\)</span>.</span></div></div>
<p><span class="math display">\[
\frac{c}{(1-c)(n-1)}\rightarrow 0 \text{ as } n \to \infty\;.
\]</span></p>
<p>So, this suggests that, if <span class="math inline">\(c &lt; 1\)</span>, any two nodes are likely to be disconnected! On the other hand, if <span class="math inline">\(c &gt; 1\)</span>, we’ve argued that we can make <span class="math inline">\(k\)</span> large enough to have high probability of a path of length <span class="math inline">\(k\)</span> between those nodes.</p>
<p>So, what’s special about <span class="math inline">\(c = 1\)</span>? This question brings us to one of the first and most beautiful results in the theory of random graphs. To get there, let’s study in a bit more detail the sizes of the <em>connected components</em> of the ER graph.</p>
</section>
</section>
<section id="component-sizes-and-the-branching-process-approximation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="component-sizes-and-the-branching-process-approximation">Component Sizes and the Branching Process Approximation</h2>
<p>We’re now going to ask ourselves about the size of a “typical” component in the Erdős–Rényi model. In particular, we’re going to be interested in whether there exists a component that fills up “most” of the graph, or whether components tend to be vanishingly small in relation to the overall graph size.</p>
<p>Our first tool for thinking about this question is the <em>branching process approximation.</em> Informally, a branching process is a process of <em>random generational growth</em>. We’ll get to a formal mathematical definition in a moment, but the easiest way to get insight is to look at a diagram:</p>
<div id="fig-branching" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-branching-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<iframe width="560" height="315" src="http://1.bp.blogspot.com/-Y59SK92Nd0c/VIoqelzfc8I/AAAAAAAAAdA/XAQB5yDStUc/s1600/eg1.PNG">
</iframe>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-branching-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: <a href="http://mytechroad.com/markov-chain-branching-process/">Image source</a>.
</figcaption>
</figure>
</div>
<p>We start with a single entity, <span class="math inline">\(X_0\)</span>. Then, <span class="math inline">\(X_0\)</span> has a random number of “offspring”: <span class="math inline">\(X_1\)</span> in total. Then, each of those <span class="math inline">\(X_1\)</span> offspring has some offspring of their own; the total number of these offspring is <span class="math inline">\(X_2\)</span>. The process continues infinitely, although there is always a chance that at some point no more offspring are produced. In this case, we often say that the process “dies out.”</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Some of this exposition in this section draws on <a href="https://www.stat.berkeley.edu/users/aldous/Networks/lec2.pdf">these notes</a> by David Aldous.</p>
</div></div><div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-branching-process" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.4 (Branching Process)</strong></span> Let <span class="math inline">\(p\)</span> be a probability distribution on <span class="math inline">\(\mathbb{Z}\)</span>, called the <strong>offspring distribution</strong>.</p>
<p>A <strong>branching process</strong> with distribution <span class="math inline">\(p\)</span> is a sequence of random variables <span class="math inline">\(X_0, X_1,,X_2\ldots\)</span> such that <span class="math inline">\(X_0 = 1\)</span> and, for <span class="math inline">\(t \geq 1\)</span>,</p>
<p><span class="math display">\[
X_t = \sum_{i = 1}^{X_{t-1}} Y_i\;,
\]</span></p>
<p>where each <span class="math inline">\(Y_i\)</span> is distributed i.i.d. according to <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Technically, this is a <em>Galton-Watson</em> branching process, named after the two authors who first proposed it <span class="citation" data-cites="watson1875probability">(<a href="#ref-watson1875probability" role="doc-biblioref">Watson and Galton 1875</a>)</span>. <br> <br> <strong>History note</strong>: Galton, one of the founders of modern statistics, was a eugenicist. The cited paper is explicit about its eugenicist motivation: the guiding question was about whether certain family names associated with well-to-do aristocrats were giving way to less elite surnames.</p>
</div></div><section id="application-to-erdősrényi" class="level3">
<h3 class="anchored" data-anchor-id="application-to-erdősrényi">Application to Erdős–Rényi</h3>
<p>Branching processes create <em>trees</em> – graphs without cycles. The reason that branching processes are helpful when thinking about Erdős–Rényi models is that <em>cycles are rare in Erdős–Rényi random graphs</em>. So, if we can understand the behavior of branching processes, then we can learn something about the Erdős–Rényi random graph as well.</p>
<p>Here’s the particular form of the branching process approximation that we will use:</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-branching-approx" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.5 (Branching Process Approximation for ER Component Sizes)</strong></span> Sample a single node <span class="math inline">\(j\)</span> at random from a large, sparse ER graph with mean degree <span class="math inline">\(c\)</span>, and let <span class="math inline">\(S\)</span> be the size (number of nodes) of the component in which <span class="math inline">\(j\)</span> lies. Note that <span class="math inline">\(S\)</span> is random: it depends both on <span class="math inline">\(j\)</span> and on the realization of the ER graph.</p>
<p>Then, <span class="math inline">\(S\)</span> is distributed approximately as <span class="math inline">\(T\)</span>, where <span class="math inline">\(T = \sum_{i = 0}^{\infty}X_t\)</span> is the total number of offspring in a GW branching process with offspring distribution <span class="math inline">\(\text{Poisson}(c)\)</span>.</p>
</div>
</div>
</div>
</div>
<p>The idea behind this approximation is:</p>
<ul>
<li>We start at <span class="math inline">\(j\)</span>, whose number of neighbors <span class="math inline">\(\sim \text{Poisson}(c)\)</span>.</li>
<li>Each of these neighbors has a number of new neighbors <span class="math inline">\(\sim \text{Poisson}(c)\)</span>, and so on.</li>
<li>We keep visiting new neighbors until we run out, and add up the number of neighbors we’ve visited to obtain <span class="math inline">\(S\)</span>.<br>
</li>
<li>Since <em>cycles are rare in ER</em>, we are unlikely to double-count any nodes (doing so would create a cycle), and so this whole process <em>also</em> approximately describes <span class="math inline">\(T\)</span> in a branching process with a <span class="math inline">\(\text{Poisson}(c)\)</span> offpsring distribution.</li>
</ul>
<!-- :::{.callout-important}
**Exercise**: In the second generation, we get to a new node by following an edge. For that reason, shouldn't the number of *new* edges be $\text{Poisson}(c-1)$ rather than $\text{Poisson}(c-1)$? Why or why not? 
::: -->
</section>
<section id="the-subcritical-case" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-subcritical-case">The Subcritical Case</h3>
<p>The mean of a <span class="math inline">\(\text{Poisson}(c)\)</span> random variable is again <span class="math inline">\(c\)</span>. As you’ll show in homework, this implies that <span class="math inline">\(X_t\)</span>, the number of offspring in generation <span class="math inline">\(t\)</span>, satisfies <span class="math inline">\(\mathbb{E}[X_t] = c^{t}\)</span>. It follows that, when <span class="math inline">\(c &lt; 1\)</span>, <span class="math inline">\(\mathbb{E}[T] = \frac{1}{1-c}\)</span>.</p>
<p>Now using Markov’s inequality, we obtain the following results:</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a <span class="math inline">\(\text{Poisson}(c)\)</span> branching process with <span class="math inline">\(c &lt; 1\)</span>, <span class="math display">\[\mathbb{P}(X_t &gt; 0) \leq c^t\;.\]</span></p>
</div>
</div>
</div>
<p>So, the probability that the branching process hasn’t yet “died out” decays exponentially with timestep <span class="math inline">\(t\)</span>. In other words, the branching process becomes very likely to die out very quickly.</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>In a <span class="math inline">\(\text{Poisson}(c)\)</span> branching process with <span class="math inline">\(c &lt; 1\)</span>, <span class="math display">\[\mathbb{P}(T \geq a) \leq \frac{1}{a}\frac{1}{1-c}\]</span></p>
</div>
</div>
</div>
<p>In particular, for <span class="math inline">\(a\)</span> very large, we are guaranteed that <span class="math inline">\(\mathbb{P}(T &gt; a)\)</span> is very small.</p>
<div class="page-columns page-full"><p>Summing up, when <span class="math inline">\(c &lt; 1\)</span>, the GW branching process dies out quickly and contains a relatively small number of nodes: <span class="math inline">\(\frac{1}{1-c}\)</span> in expectation. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">In this setting, the branching process is called <em>subcritical</em>.</span></div></div>
<section id="back-to-er" class="level4">
<h4 class="anchored" data-anchor-id="back-to-er">Back to ER</h4>
<p>If we now translate back to the Erdős–Rényi random graph, the branching process approximation now suggests the following heuristic:</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Heuristic</strong>: In a sparse ER random graph with mean <span class="math inline">\(c &lt; 1\)</span>, the expected size of a component containing a randomly selected node is roughly <span class="math inline">\(\frac{1}{1-c}\)</span>.</p>
<p>In particular, since this quantity is independent of <span class="math inline">\(n\)</span>, we find that the <em>fraction</em> of the graph occupied by this component is <span class="math inline">\(\frac{1}{n}\frac{1}{1-c}\)</span> and therefore vanishes as <span class="math inline">\(n\rightarrow \infty\)</span>.</p>
<p>We can also turn this into a statement about probabilities: Markov’s inequality implies that, if <span class="math inline">\(S\)</span> is the size of a component containing a randomly selected node,</p>
<p><span class="math display">\[
\mathbb{P}(S/n &gt; a) \rightarrow 0
\]</span></p>
<p>for any constant <span class="math inline">\(a &gt; 0\)</span>. In other words, for large <span class="math inline">\(n\)</span>, the largest component is always vanishingly small in relation to the graph as a whole.</p>
</div>
</div>
</div>
<p>Let’s check this experimentally. The following code block computes the size of the component in an ER graph containing a random node, and averages the result across many realizations. The experimental result is quite close to the theoretical prediction.</p>
<div id="14d42075" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> component_size_of_node(n, c):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, c<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(nx.node_connected_component(G, <span class="dv">1</span>))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [component_size_of_node(<span class="dv">5000</span>, c) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> <span class="ss">f"""</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="ss">Average over experiments is </span><span class="sc">{</span>np<span class="sc">.</span>mean(sizes)<span class="sc">:.2f}</span><span class="ss">.</span><span class="ch">\n</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="ss">Theoretical expectation is </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>c)<span class="sc">:.2f}</span><span class="ss">.</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Average over experiments is 4.90.

Theoretical expectation is 5.00.
</code></pre>
</div>
</div>
<p>Note that the expected (and realized) component size is very small, even though the graph contains 5,000 nodes!</p>
<p>For this reason, we say that subcritical ER contains only <em>small</em> connected components, in the sense that each component contains approximately 0% of the graph as <span class="math inline">\(n\)</span> grows large.</p>
<p>This explains our result from earlier about path lengths. The probability that any two nodes have a path between them is the same as the probability that they are <em>on the same connected component</em>. But if every connected component is small, then the probability that two nodes occupy the same one is vanishes.</p>
</section>
</section>
<section id="the-giant-component" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-giant-component">The Giant Component</h3>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-giant-component" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.6 (Giant Component)</strong></span> We say that <span class="math inline">\(G(n,p)\)</span> has a <strong>giant component</strong> if <span class="math display">\[
\mathbb{P}(S/n &gt; a) \rightarrow b
\]</span> for some constant <span class="math inline">\(b &gt; 0\)</span>.</p>
<p>Intuitively, this means that there is a possibility of a connected component that takes up a nonzero fraction of the graph.</p>
</div>
</div>
</div>
</div>
<p>So far, we’ve argued using the branching process approximation that there is no giant component in the Erdős–Rényi model with <span class="math inline">\(c &lt; 1\)</span>. The theory of branching processes also suggests to us that there <em>could</em> be a giant component when <span class="math inline">\(c &gt; 1\)</span>.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside">The proof of this fact is usually done in terms of <em>generating functions</em> and is beyond our scope, but you can <a href="https://en.wikipedia.org/wiki/Branching_process#Extinction_problem_for_a_Galton_Watson_process">check Wikipedia</a> for an outline.</span></div></div>
<div class="callout callout-style-simple callout-warning no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Fact</strong>: when <span class="math inline">\(c &gt; 1\)</span>, there is a nonzero probability that the <span class="math inline">\(\text{Poisson}(c)\)</span> branching process continues forever; that is, never goes extinct.</p>
</div>
</div>
</div>
<div class="page-columns page-full"><p>Using our correspondence between components of the ER model and branching processes, this suggests that, if we pick a random node, the component it is in has the potential to be very large. In fact (and this requires some advanced probability to prove formally), when <span class="math inline">\(c &gt; 1\)</span>, there <em>is</em> a giant component. This is our first example of a phase transition. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">It is also possible to prove that, with high probability, there is only one giant component; Newman does this in 11.5.1.</span></div></div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-giant-component" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.7 (Phase Transition)</strong></span> A <strong>phase transition</strong> is a qualitative change in response to a small variation in a quantitative parameter.</p>
</div>
</div>
</div>
</div>
<p>Examples of phase transitions include freezing, in which a liquid undergoes a qualitative change into a solid in response to a small variation in temperature.</p>
<p><a href="#fig-giant-component-illustration" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> shows two sparse ER random graphs on either side of the <span class="math inline">\(c = 1\)</span> transition. We observe an apparent change in qualitative behavior between the two cases.</p>
<div id="cell-fig-giant-component-illustration" class="cell page-columns page-full" data-fig-height="4" data-fig-width="8" data-fig.cap="Two sparse ER graphs with 500 nodes and varying mean degree." data-out.width="100%" data-execution_count="3" data-cap-location="undefined">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb4" data-cap-location="undefined"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">1.3</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, c[i]<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    nx.draw(G, ax <span class="op">=</span> axarr[i], node_size <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    axarr[i].<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"c = </span><span class="sc">{</span>c[i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-giant-component-illustration" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" width="540" height="409">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-giant-component-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-random-graphs_files/figure-html/fig-giant-component-illustration-output-1.png" id="fig-giant-component-illustration" width="540" height="409" class="figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig quarto-uncaptioned margin-caption" id="fig-giant-component-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3
</figcaption>
</figure>
</div>
</div>
</div>
<section id="sec-component" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-component">Size of the Giant Component</h4>
<div class="page-columns page-full"><p>Perhaps surprisingly, while it’s difficult to prove that there is a giant component, it’s not hard at all to estimate its size. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">This argument is reproduced from Newman, pages 349-350</span></div></div>
<p>Let <span class="math inline">\(S\)</span> be the size of the giant component in an Erdős–Rényi random graph, assuming there is one. Then, <span class="math inline">\(s = S/n\)</span> is the probability that a randomly selected node is in the giant component. Let <span class="math inline">\(u = 1 - s\)</span> be the probability that a given node is <em>not</em> in the giant component.</p>
<p>Let’s take a random node <span class="math inline">\(i\)</span>, and ask it the probability that it’s in the giant component. Well, one answer to that question is just “<span class="math inline">\(u\)</span>.” On the other hand, we can <em>also</em> answer that question by looking at <span class="math inline">\(i\)</span>’s neighbors. If <span class="math inline">\(i\)</span> is not in the giant component, then it can’t be connected to any node that is in the giant component. So, for each other node <span class="math inline">\(j\neq i\)</span>, it must be the case that either:</p>
<ol type="1">
<li><span class="math inline">\(i\)</span> is not connected to <span class="math inline">\(j\)</span>. This happens with probability <span class="math inline">\(1-p\)</span>.</li>
<li><span class="math inline">\(i\)</span> is connected to <span class="math inline">\(j\)</span>, but <span class="math inline">\(j\)</span> is not in the giant component either. <span class="math inline">\(i\)</span> is connected to <span class="math inline">\(j\)</span> with probability <span class="math inline">\(p\)</span>, and <span class="math inline">\(j\)</span> is not in the giant component with probability <span class="math inline">\(u\)</span>.</li>
</ol>
<p>There are <span class="math inline">\(n-1\)</span> nodes other than <span class="math inline">\(i\)</span>, and so the probability that <span class="math inline">\(i\)</span> is not connected to any other node in the giant component is <span class="math inline">\((1 - p + pu)^{n-1}\)</span>. We therefore have the equation</p>
<p><span class="math display">\[
u = (1 - p + pu)^{n-1}\;.
\]</span></p>
<p>Let’s take the righthand side and use <span class="math inline">\(p = c/(n-1)\)</span>: <span class="math display">\[
\begin{aligned}
    u &amp;= (1 - p(1-u))^{n-1} \\
      &amp;= \left(1 - \frac{c(1-u)}{n-1}\right)^{n-1}\;.
\end{aligned}
\]</span> This is a good time to go back to precalculus and remember the limit definition of the function <span class="math inline">\(e^x\)</span>: <span class="math display">\[
e^x = \lim_{n \rightarrow \infty}\left(1 + \frac{x}{n}\right)^{n}\;.
\]</span> Since we are allowing <span class="math inline">\(n\)</span> to grow large in our application, we approximate</p>
<p><span class="math display">\[
u \approx e^{-c(1-u)}\;.
\]</span> So, now we have a description of the fraction of nodes that <em>aren’t</em> in the giant component. We can get a description of how many nodes <em>are</em> in the giant component by substituting <span class="math inline">\(s = 1-u\)</span>, after which we get the equation we’re really after: <span id="eq-giant-component-size"><span class="math display">\[
s = 1- e^{-cs}
\tag{9.2}\]</span></span></p>
<p>This equation doesn’t have a closed-form solution for <span class="math inline">\(s\)</span>, but we can still plot it and compare the result to simulations (<a href="#fig-giant-component" class="quarto-xref">Figure&nbsp;<span>9.4</span></a>). Not bad!</p>
<div id="cell-fig-giant-component" class="cell page-columns page-full" data-fig.cap="Each point gives the fraction of an ER graph with 50,000 nodes occupied by the largest component. The mean degree is on the horizontal axis. The black line gives the theoretical prediction of @eq-giant-component-size." data-out.width="80%" data-execution_count="4" data-cap-location="undefined">
<details class="code-fold">
<summary>Show code</summary>
<div class="sourceCode cell-code" id="cb5" data-cap-location="undefined"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># experiment: compute the size of the largest connected </span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># component as a function of graph size for a range of mean degrees. </span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> largest_component(n, p):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, p)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> <span class="bu">max</span>(nx.connected_components(G), key<span class="op">=</span><span class="bu">len</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(S) <span class="op">/</span> n</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">50000</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.repeat(np.linspace(<span class="fl">0.5</span>, <span class="fl">1.5</span>, <span class="dv">11</span>), <span class="dv">10</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> np.array([largest_component(n, c<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>)) <span class="cf">for</span> c <span class="kw">in</span> C])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># theory: prediction based on Newman 11.16</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">.001</span>, <span class="fl">.6</span>, <span class="dv">101</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>C_theory <span class="op">=</span> <span class="op">-</span>np.log(<span class="dv">1</span><span class="op">-</span>S)<span class="op">/</span>S</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the results to compare</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.plot(C_theory, </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>         S, </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>         color <span class="op">=</span> <span class="st">"black"</span>, </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>         label <span class="op">=</span> <span class="st">"Theoretical prediction"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>plt.scatter(C, </span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            U, </span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="st">"Experiment"</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Mean degree"</span>, </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>              ylabel <span class="op">=</span> <span class="st">"Proportion of graph in largest component"</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-giant-component" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" width="589" height="429">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-giant-component-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09-random-graphs_files/figure-html/fig-giant-component-output-1.png" id="fig-giant-component" width="589" height="429" class="figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig quarto-uncaptioned margin-caption" id="fig-giant-component-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-erdHos1960evolution" class="csl-entry" role="listitem">
Erdős, Paul, and Alfréd Rényi. 1960. <span>“On the Evolution of Random Graphs.”</span> <em>Publ. Math. Inst. Hung. Acad. Sci</em> 5 (1): 17–60.
</div>
<div id="ref-riordan2010diameter" class="csl-entry" role="listitem">
Riordan, Oliver, and Nicholas Wormald. 2010. <span>“The Diameter of Sparse Random Graphs.”</span> <em>Combinatorics, Probability and Computing</em> 19 (5-6): 835–926.
</div>
<div id="ref-watson1875probability" class="csl-entry" role="listitem">
Watson, Henry William, and Francis Galton. 1875. <span>“On the Probability of the Extinction of Families.”</span> <em>The Journal of the Anthropological Institute of Great Britain and Ireland</em> 4: 138–44.
</div>
<div id="ref-watts1998collective" class="csl-entry" role="listitem">
Watts, Duncan J, and Steven H Strogatz. 1998. <span>“Collective Dynamics of ‘Small-World’networks.”</span> <em>Nature</em> 393 (6684): 440–42.
</div>
</div>
</section>
</section>

<p><br> <br> <span style="color:grey;">© Heather Zinn Brooks and Phil Chodrow, 2025</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/08-power-laws.html" class="pagination-link" aria-label="Power Law Degree Distributions">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Power Law Degree Distributions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/10-configuration-model.html" class="pagination-link" aria-label="Configuration models">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Configuration models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">code-summary:</span><span class="co"> "Show code"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Random Graphs: Erdős–Rényi</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>*Open the live notebook in Google Colab [here](https://colab.research.google.com/github/network-science-notes/network-science-notes.github.io/blob/main/docs/live-notebooks/09-random-graphs.ipynb).* </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>A random graph is a probability distribution over graphs. In this set of lecture notes, we'll begin our exploration of several random graphs. </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Random Graphs?</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>Why should we even study random graphs? There are a few reasons!</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Graphs as Insightful Models</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>Random graphs allow us to build our intuition and skills in the study of networks. In many simple random graphs, quantities of interest (clustering coefficients, diameters, etc) can be calculated with pencil and paper. This allows us to build mathematical insight into the structure of many real-world models, without the need for detailed simulations. </span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Graphs as Mathematical Puzzles</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>Many properties of even simple random graphs are still under investigation by research mathematicians. While some properties can be calculated simply, others require extremely sophisticated machinery in order to understand. There are many mathematicians who spend their careers studying random graphs, and their cousins, random matrices. </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Graphs as Null Hypotheses</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>Suppose that you compute the global clustering coefficient of a graph and find it to be $0.31$. How do we interpret that? Is that high? Low? In this case, we should ask: *compared to what?* Random graphs allow us one way to make a comparison: that clustering coefficient is "high" if it's larger than the clustering coefficient in a suitably chosen, comparable random graph. How to choose a comparable random graph is an important question, and the answer is not always clear! </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>In this way, random graphs often serve as the *null hypothesis*, in exactly the same way you might have heard of the null hypothesis for other kinds of statistical tests. </span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Graphs and Statistical Inference</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>Many statistical algorithms for tasks like graph clustering, ranking, and prediction come from random graph models. The idea, generally speaking, is to imagine that we observe a graph, and then try to make the best educated guess possible about the model that generated that graph. This is classical statistical inference. </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="fu">## The $G(n,p)$ Model (Erdős–Rényi)</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>The model of @erdHos1960evolution is the simplest and most fundamental model of a random graph. Our primary interest in the ER model is for mathematical insight and null modeling. The ER model is *mostly* understood in its major mathematical properties, and it's almost never used in statistical inference. [The ER model is, however, a building block of models that *are* used in statistical inference. The <span class="co">[</span><span class="ot">stochastic blockmodel</span><span class="co">](https://en.wikipedia.org/wiki/Stochastic_block_model)</span>, for example, is often used for graph clustering and community detection. In its simplest form, it's a bunch of ER graphs glued together.]{.aside}</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>::: {#def-ER}</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## Erdős–Rényi Random Graph</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>An *Erdős–Rényi random graph* with $n$ nodes and connection probability $p$, written $G(n,p)$, is a random graph constructed by placing an edge with probability $p$ between each pair of distinct nodes. </span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>We can imagine visiting each possible pair of edges $(i,j)$ and flipping a coin with probability of heads $p$. If heads, we add $(i,j) \in E$; otherwise, we don't. </span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution}</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>**Exercise**: Let $i$ be a fixed node in a $G(n,p)$ graph, and let $K_i$ be its (random) degree. Show that $K_i$ has binomial distribution with success probability $p$ and $n-1$ trials. </span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>::: {.hide .solution}</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>A given node $u$ is connected to each of the other $n-1$ nodes (independently) with probability $p$. The probability of being connected to a particular set of $k$ nodes is </span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    \mathbb{P}<span class="co">[</span><span class="ot">\text{connected to $k$ given nodes}</span><span class="co">]</span> &amp;= \mathbb{P}<span class="co">[</span><span class="ot">\text{$k$ successes}</span><span class="co">]</span>\mathbb{P}<span class="co">[</span><span class="ot">\text{$n-1-k$ failures}</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>    &amp;= p^k (1-p)^{n-1-k} \,.</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>Building on this, the probability of being connected to $k$ other nodes in any way is</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>    \mathbb{P}<span class="co">[</span><span class="ot">\text{connected to $k$ nodes}</span><span class="co">]</span> &amp;= \left( \text{number of ways to choose $k$ nodes }\right) \mathbb{P}<span class="co">[</span><span class="ot">\text{$k$ successes}</span><span class="co">]</span>\mathbb{P}<span class="co">[</span><span class="ot">\text{$n-1-k$ failures}</span><span class="co">]</span> <span class="sc">\\</span> <span class="sc">\\</span></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>    &amp;= {n-1 \choose k} p^k (1-p)^{n-1-k} \,.</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>Indeed, this is exactly the binomial degree distribution, as required.</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution}</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>**Exercise**: Show that $\mathbb{E}<span class="co">[</span><span class="ot">K_i</span><span class="co">]</span> = p(n-1)$. </span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>::: {.hide .solution}</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>Using our previous calculations, we know $\mathbb{E}<span class="co">[</span><span class="ot">K_i</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">\frac{2m}{n}</span><span class="co">]</span>.$ However, $n$ is fixed for the $G(n,p)$ model, so we have $\mathbb{E}<span class="co">[</span><span class="ot">K_i</span><span class="co">]</span> = \frac{2}{n}\mathbb{E}<span class="co">[</span><span class="ot">m</span><span class="co">]</span>,$ where $\mathbb{E}<span class="co">[</span><span class="ot">m</span><span class="co">]</span>$ is the expected number of edges. This quantity will be equal to the probability that an edge exists between a pair of nodes multiplied by the number of possible pairs, which is ${n \choose 2}p = \frac{n(n-1)}{2}p$. Thus $\mathbb{E}<span class="co">[</span><span class="ot">K_i</span><span class="co">]</span> = (n-1)p$, as required.</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Clustering Coefficient</span></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>Both local and global clustering coefficients are defined in terms of a ratio of realized triangles to possible triangles. </span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>Analyzing ratios using probability theory can get tricky, but we can get a pretty reliable picture of things by computing the expectations of the numerator and denominator separately. </span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>Let's take the global clustering coefficient. For this, we need to compute the total number of triangles, and the total number of wedges. </span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>How many triangles are there? Well, there are $\binom{n}{3}$ ways to choose 3 nodes from all the possibilities, and the probability that all three edges exist to form the triangle is $p^3$. </span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>So, in expectation, there are $\binom{n}{3}p^3$ triangles in the graph. </span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>How many wedges are there? Well, each triple of nodes contains three possible wedges, and the probability of any given wedge existing is $p^2$. </span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>So, the expected number of wedges is $\binom{n}{3}p^2$. Our estimate for the expected clustering coefficient is that it should be *about* $p$, although we have been fast and loose with several mathematical details to arrive at this conclusion. </span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sparsity</span></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>Recall that it is possible to define sparsity more explicitly when we have a theoretical model (where it makes sense to take limits). Indeed, the *sparse* Erdős–Rényi model is very useful. Intuitively, the idea of sparsity is that there are not very many edges in comparison to the number of nodes. </span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>::: {#def-sparse-ER}</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>We say that a $G(n,p)$ graph is **sparse** when $p = c/(n-1)$ for some constant $c$. </span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>A consequence of sparsity is that $\mathbb{E}<span class="co">[</span><span class="ot">K_i</span><span class="co">]</span> = c$; i.e. the expected degree of a node in sparse $G(n,p)$ is constant. When studying sparse $G(n,p)$, we are almost always interested in the case $n\rightarrow \infty$. </span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a><span class="fu">### Clustering</span></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>What does this imply for our estimation of the global clustering coefficient from before? Well, we expect the global clustering coefficient to be *about* $p$, and if $p = c/(n-1)$, then $p \rightarrow 0$ as $n \to \infty$ for sparse $G(n,p)$. </span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">The need to move beyond the ER model to develop sparse graphs with clustering coefficients was part of the motivation of @watts1998collective, a famous paper that introduced the "small world model."</span><span class="co">]</span>{.aside}</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Sparse Erdős–Rényi graphs have vanishing clustering coefficients. </span></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a>@fig-clustering-decay shows how the global clustering coefficient of a sparse Erdős–Rényi random graph decays as we increase $n$. Although the estimate that the global clustering coefficient should be equal to $p$ was somewhat informal, experimentally it works quite well. </span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.cap : "Each point gives the global clustering coefficient of an ER graph with mean degree $c = 5$ and specified number of nodes. The black line gives the estimate $T = p = c/(n-1)$ for the clustering coefficient that we computed earlier. The function `nx.fast_gnp_random_graph()` is a special function from NetworkX for generating ER random graphs quickly; you'll learn about how it works in an upcoming homework assignment."</span></span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a><span class="co">#| out.width : 80%</span></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a><span class="co">#| cap-location: margin</span></span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-clustering-decay</span></span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> np.repeat(<span class="dv">2</span><span class="op">**</span>np.arange(<span class="dv">5</span>, <span class="dv">15</span>), <span class="dv">10</span>)</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> global_clustering(n, c):</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, c<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nx.transitivity(G)</span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> [global_clustering(n, c) <span class="cf">for</span> n <span class="kw">in</span> N]</span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>)</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>theory <span class="op">=</span> ax.plot(N, c<span class="op">/</span>(N<span class="op">-</span><span class="dv">1</span>), color <span class="op">=</span> <span class="st">"black"</span>, label <span class="op">=</span> <span class="st">"Estimate"</span>)</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>exp <span class="op">=</span> ax.scatter(N, T, label <span class="op">=</span> <span class="st">"Experiments"</span>)</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a>semil <span class="op">=</span> ax.semilogx()</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Number of nodes"</span>, </span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a>       ylabel <span class="op">=</span> <span class="st">"Global clustering coefficient"</span>)</span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cycles and Local Tree-Likeness</span></span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a>Recall the following definition:</span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>::: {#def-cycle}</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a>A **cycle** is a walk that does not repeat edges and ends at the same node that it begins. </span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>A triangle is an example of a cycle of length $3$. </span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a>::: {#thm-rare-cycles}</span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>In the sparse $G(n,p)$ model, for any length $k$, the probability that there exists a cycle of length $k$ attached to node $i$ shrinks to 0 as $n \rightarrow \infty$. </span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a>You'll prove a generalization of @thm-rare-cycles in an upcoming homework problem. </span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a>Graphs in which cycles are very rare are often called *locally tree-like*. A tree is a graph without cycles; if cycles are very rare, then we can often use techniques that are normally guaranteed to only work on trees without running into (too much) trouble. </span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a><span class="fu">### Path Lengths</span></span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a>How far apart are two nodes in $G(n,p)$? Again, exactly computing the length of geodesic paths involves some challenging mathematical detail. <span class="co">[</span><span class="ot">See @riordan2010diameter and references therein.</span><span class="co">]</span>{.aside} However, we can get a big-picture view of the situation by asking a slightly different question: </span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Given two nodes $i$ and $j$, what is the expected number of paths of length $k$ between them?</span></span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a>Let $R(k)$ denote the number of $k$-paths between $i$ and $j$. Let $r(k) = \mathbb{E}<span class="co">[</span><span class="ot">R(k)</span><span class="co">]</span>$. Let's estimate $r(k)$. </span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>First, we know that $r(1) = p$. For higher values of $k$, we'll use the following idea: in order for there to be a path of length $k$ from $i$ to $j$, there must be a node $\ell$ such that: </span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There exists a path from $i$ to $\ell$ of length $k-1$. In expectation, there are $r(k-1)$ of these. </span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There exists a path from $\ell$ to $j$ of length $1$. This happens with probability $p$. </span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>There are $n-2$ possibilities for $\ell$ (excluding $i$ and $j$), and so we obtain the approximate relation</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a>r(k) \approx (n-2)r(k-1)p\;. </span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a>:::{.callout-warning}</span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a>Why is this an approximation? Well, some of the paths between $i$ and $\ell$ that are counted in $r(k-1)$ could actually include the edge $(j, \ell)$ *already*. An example is $(i,j), (j,\ell)$. In this case, the presence of edge $(j,\ell)$ is not independent of the presence of the path between $i$ and $\ell$. The derivation above implicitly treats these two events as independent. Again, because *cycles are rare in large, sparse ER*, this effect is small when $k$ is small. </span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a>Proceeding inductively and approximating $n-2 \approx n-1$ for $n$ large, we have the relation </span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a>r(k) \approx (n-1)^{k-1}p^{k-1}r(1) = c^{k-1}p</span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a>$$ {#eq-path-expectation}</span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a>in the sparse ER model. </span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a>Using this result, let's ask a new question: </span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; What path length $k$ do I need to allow to be confident that there's a path between nodes $i$ and $j$? </span></span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a>Well, suppose we want there to be $q$ paths. Then, we can solve $q = c^{k-1}p$ for $k$, which gives us: </span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a>q &amp;= c^{k-1}p <span class="sc">\\</span> </span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a>\log q &amp;= (k-1)\log c + \log p <span class="sc">\\</span> </span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a>\log q &amp;= (k-1)\log c + \log c - \log n <span class="sc">\\</span> </span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a>\frac{\log q + \log n}{\log c} &amp;= k</span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a>Here, I've also approximated $\log n-1 \approx \log n$ for $n$ large. </span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a>So, supposing that I want there to be at least one path in expectation ($q = 1$), I need to allow $k = \frac{\log n}{\log c}$. This is pretty short, actually! For example, the population of the world is about $8\times 10^9$, and Newman estimates that an average individual knows around 1,000 other people; that is, $c = 10^3$ in the world social network. The resulting value of $k$ here is around 3.3. </span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a>In other words, this calculation suggests that, if the world were an ER network, it would be the case that any two individuals would be pretty likely to have at least one path between them of length no longer than $4$. </span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a>More formal calculations regarding the diameter of the ER graph confirm that the diameter of the ER graph grows slowly as a function of $n$, even in relatively sparse cases. </span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Caveat</span></span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a>If you spend some time looking at @eq-path-expectation, you might find yourself wondering:</span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Hey, what happens if $c \leq 1$?</span>  </span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a>Indeed, something *very* interesting happens here. </span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a>Let's assume $c &lt; 1$ (i.e. we're ignoring the case $c = 1$), and estimate the expected number of paths between $i$ and $j$ of *any* length. Using @eq-path-expectation, we get </span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">\sum_{k = 1}^{\infty} R(k)\right</span><span class="co">]</span> = \sum_{k = 1}^\infty c^{k-1}p = \sum_{k = 0}^\infty c^kp = \frac{p}{1-c}\;.</span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a>If we now use Markov's inequality, <span class="co">[</span><span class="ot">Markov's inequality states that $\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}$.</span><span class="co">]</span>{.aside} we find that the probability that there is a path of *any* length between nodes $i$ and $j$ is no larger than $\frac{p}{1-c}.$ In the sparse regime, we can substitute $p = \frac{c}{n-1}$ to see that  </span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a>\frac{c}{(1-c)(n-1)}\rightarrow 0 \text{ as } n \to \infty\;. </span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a>So, this suggests that, if $c &lt; 1$, any two nodes are likely to be disconnected! On the other hand, if $c &gt; 1$, we've argued that we can make $k$ large enough to have high probability of a path of length $k$ between those nodes. </span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a>So, what's special about $c = 1$? This question brings us to one of the first and most beautiful results in the theory of random graphs. To get there, let's study in a bit more detail the sizes of the *connected components* of the ER graph. </span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a><span class="fu">## Component Sizes and the Branching Process Approximation</span></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a>We're now going to ask ourselves about the size of a "typical" component in the Erdős–Rényi model. In particular, we're going to be interested in whether there exists a component that fills up "most" of the graph, or whether components tend to be vanishingly small in relation to the overall graph size. </span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a>Our first tool for thinking about this question is the *branching process approximation.* Informally, a branching process is a process of *random generational growth*. We'll get to a formal mathematical definition in a moment, but the easiest way to get insight is to look at a diagram: </span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a>::: {#fig-branching}</span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a>&lt;iframe width="560" height="315" src="http://1.bp.blogspot.com/-Y59SK92Nd0c/VIoqelzfc8I/AAAAAAAAAdA/XAQB5yDStUc/s1600/eg1.PNG"&gt;&lt;/iframe&gt;</span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Image source</span><span class="co">](http://mytechroad.com/markov-chain-branching-process/)</span>.</span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a>We start with a single entity, $X_0$. Then, $X_0$ has a random number of "offspring": $X_1$ in total. Then, each of those $X_1$ offspring has some offspring of their own; the total number of these offspring is $X_2$. The process continues infinitely, although there is always a chance that at some point no more offspring are produced. In this case, we often say that the process "dies out."  </span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a>Some of this exposition in this section draws on <span class="co">[</span><span class="ot">these notes</span><span class="co">](https://www.stat.berkeley.edu/users/aldous/Networks/lec2.pdf)</span> by David Aldous. </span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a>::: {#def-branching-process}</span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a><span class="fu">##  Branching Process</span></span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a>Let $p$ be a probability distribution on $\mathbb{Z}$, called the **offspring distribution**.  </span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a>A **branching process** with distribution $p$ is a sequence of random variables $X_0, X_1,,X_2\ldots$ such that $X_0 = 1$ and, for $t \geq 1$,  </span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a>X_t = \sum_{i = 1}^{X_{t-1}} Y_i\;,</span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a>where each $Y_i$ is distributed i.i.d. according to $p$. </span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a>Technically, this is a *Galton-Watson* branching process, named after the two authors who first proposed it [@watson1875probability]. &lt;br&gt; &lt;br&gt; **History note**: Galton, one of the founders of modern statistics, was a eugenicist. The cited paper is explicit about its eugenicist motivation: the guiding question was about whether certain family names associated with well-to-do aristocrats were giving way to less elite surnames. </span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application to Erdős–Rényi </span></span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a>Branching processes create *trees* -- graphs without cycles. The reason that branching processes are helpful when thinking about Erdős–Rényi models is that *cycles are rare in Erdős–Rényi random graphs*. So, if we can understand the behavior of branching processes, then we can learn something about the Erdős–Rényi random graph as well. </span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a>Here's the particular form of the branching process approximation that we will use: </span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a>:::{#def-branching-approx}</span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a><span class="fu">## Branching Process Approximation for ER Component Sizes</span></span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a>Sample a single node $j$ at random from a large, sparse ER graph  with mean degree $c$, and let $S$ be the size (number of nodes) of the component in which $j$ lies. Note that $S$ is random: it depends both on $j$ and on the realization of the ER graph. </span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a>Then, $S$ is distributed approximately as $T$, where $T = \sum_{i = 0}^{\infty}X_t$ is the total number of offspring in a GW branching process with offspring distribution $\text{Poisson}(c)$. </span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a>The idea behind this approximation is: </span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We start at $j$, whose number of neighbors $\sim \text{Poisson}(c)$. </span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each of these neighbors has a number of new neighbors $\sim \text{Poisson}(c)$, and so on. </span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We keep visiting new neighbors until we run out, and add up the number of neighbors we've visited to obtain $S$.  </span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Since *cycles are rare in ER*, we are unlikely to double-count any nodes (doing so would create a cycle), and so this whole process *also* approximately describes $T$ in a branching process with a $\text{Poisson}(c)$ offpsring distribution. </span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{.callout-important}</span></span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a><span class="co">**Exercise**: In the second generation, we get to a new node by following an edge. For that reason, shouldn't the number of *new* edges be $\text{Poisson}(c-1)$ rather than $\text{Poisson}(c-1)$? Why or why not? </span></span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Subcritical Case</span></span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a>The mean of a $\text{Poisson}(c)$ random variable is again $c$. As you'll show in homework, this implies that $X_t$, the number of offspring in generation $t$, satisfies $\mathbb{E}<span class="co">[</span><span class="ot">X_t</span><span class="co">]</span> = c^{t}$. It follows that, when $c &lt; 1$, $\mathbb{E}<span class="co">[</span><span class="ot">T</span><span class="co">]</span> = \frac{1}{1-c}$. </span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a>Now using Markov's inequality, we obtain the following results: </span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a>In a $\text{Poisson}(c)$ branching process with $c &lt; 1$, </span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(X_t &gt; 0) \leq c^t\;.$$ </span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a>So, the probability that the branching process hasn't yet "died out" decays exponentially with timestep $t$. In other words, the branching process becomes very likely to die out very quickly. </span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a>In a $\text{Poisson}(c)$ branching process with $c &lt; 1$, </span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a>$$\mathbb{P}(T \geq a) \leq \frac{1}{a}\frac{1}{1-c}$$</span>
<span id="cb6-340"><a href="#cb6-340" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-341"><a href="#cb6-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a>In particular, for $a$ very large, we are guaranteed that $\mathbb{P}(T &gt; a)$ is very small. </span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a>Summing up, when $c &lt; 1$, the GW branching process dies out quickly and contains a relatively small number of nodes: $\frac{1}{1-c}$ in expectation. <span class="co">[</span><span class="ot">In this setting, the branching process is called *subcritical*.</span><span class="co">]</span>{.aside}</span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Back to ER</span></span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a>If we now translate back to the Erdős–Rényi random graph, the branching process approximation now suggests the following heuristic: </span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a>**Heuristic**: In a sparse ER random graph with mean $c &lt; 1$, the expected size of a component containing a randomly selected node is roughly $\frac{1}{1-c}$. </span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a>In particular, since this quantity is independent of $n$, we find that the *fraction* of the graph occupied by this component is $\frac{1}{n}\frac{1}{1-c}$ and therefore vanishes as $n\rightarrow \infty$. </span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a>We can also turn this into a statement about probabilities: Markov's inequality implies that, if $S$ is the size of a component containing a randomly selected node,</span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(S/n &gt; a) \rightarrow 0</span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a>for any constant $a &gt; 0$. In other words, for large $n$, the largest component is always vanishingly small in relation to the graph as a whole. </span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a>Let's check this experimentally. The following code block computes the size of the component in an ER graph containing a random node, and averages the result across many realizations. The experimental result is quite close to the theoretical prediction. </span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> component_size_of_node(n, c):</span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, c<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(nx.node_connected_component(G, <span class="dv">1</span>))</span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [component_size_of_node(<span class="dv">5000</span>, c) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)]</span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> <span class="ss">f"""</span></span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a><span class="ss">Average over experiments is </span><span class="sc">{</span>np<span class="sc">.</span>mean(sizes)<span class="sc">:.2f}</span><span class="ss">.</span><span class="ch">\n</span></span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a><span class="ss">Theoretical expectation is </span><span class="sc">{</span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>c)<span class="sc">:.2f}</span><span class="ss">.</span></span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span></span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(out)</span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a>Note that the expected (and realized) component size is very small, even though the graph contains 5,000 nodes!</span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a>For this reason, we say that subcritical ER contains only *small* connected components, in the sense that each component contains approximately 0\% of the graph as $n$ grows large. </span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a>This explains our result from earlier about path lengths. The probability that any two nodes have a path between them is the same as the probability that they are *on the same connected component*. But if every connected component is small, then the probability that two nodes occupy the same one is vanishes. </span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Giant Component</span></span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a>::: {#def-giant-component}</span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a><span class="fu">## Giant Component</span></span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a>We say that $G(n,p)$ has a **giant component** if </span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(S/n &gt; a) \rightarrow b</span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a>for some constant $b &gt; 0$. </span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a>Intuitively, this means that there is a possibility of a connected component that takes up a nonzero fraction of the graph. </span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a>So far, we've argued using the branching process approximation that there is no giant component in the Erdős–Rényi model with $c &lt; 1$. The theory of branching processes also suggests to us that there *could* be a giant component when $c &gt; 1$. </span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">The proof of this fact is usually done in terms of *generating functions* and is beyond our scope, but you can [check Wikipedia](https://en.wikipedia.org/wiki/Branching_process#Extinction_problem_for_a_Galton_Watson_process) for an outline.</span><span class="co">]</span>{.aside}</span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a>**Fact**: when $c &gt; 1$, there is a nonzero probability that the $\text{Poisson}(c)$ branching process continues forever; that is, never goes extinct. </span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a>Using our correspondence between components of the ER model and branching processes, this suggests that, if we pick a random node, the component it is in has the potential to be very large. In fact (and this requires some advanced probability to prove formally), when $c &gt; 1$, there *is* a giant component. This is our first example of a phase transition. </span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">It is also possible to prove that, with high probability, there is only one giant component; Newman does this in 11.5.1.</span><span class="co">]</span>{.aside}</span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a>::: {#def-giant-component}</span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a><span class="fu">## Phase Transition</span></span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a>A **phase transition** is a qualitative change in response to a small variation in a quantitative parameter. </span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a>Examples of phase transitions include freezing, in which a liquid undergoes a qualitative change into a solid in response to a small variation in temperature. </span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a>@fig-giant-component-illustration shows  two sparse ER random graphs on either side of the $c = 1$ transition. We observe an apparent change in qualitative behavior between the two cases. </span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.cap : "Two sparse ER graphs with 500 nodes and varying mean degree."</span></span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a><span class="co">#| out.width : 100%</span></span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a><span class="co">#| cap-location: margin</span></span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-giant-component-illustration</span></span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">1.3</span>]</span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, c[i]<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a>    nx.draw(G, ax <span class="op">=</span> axarr[i], node_size <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a>    axarr[i].<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"c = </span><span class="sc">{</span>c[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Size of the Giant Component {#sec-component}</span></span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a>Perhaps surprisingly, while it's difficult to prove that there is a giant component, it's not hard at all to estimate its size. </span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">This argument is reproduced from Newman, pages 349-350</span><span class="co">]</span>{.aside}</span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a>Let $S$ be the size of the giant component in an Erdős–Rényi random graph, assuming there is one. Then, $s = S/n$ is the probability that a randomly selected node is in the giant component. Let $u = 1 - s$ be the probability that a given node is *not* in the giant component. </span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a>Let's take a random node $i$, and ask it the probability that it's in the giant component. Well, one answer to that question is just "$u$." On the other hand, we can *also* answer that question by looking at $i$'s neighbors. If $i$ is not in the giant component, then it can't be connected to any node that is in the giant component. So, for each other node $j\neq i$, it must be the case that either: </span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$i$ is not connected to $j$. This happens with probability $1-p$. </span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$i$ is connected to $j$, but $j$ is not in the giant component either. $i$ is connected to $j$ with probability $p$, and $j$ is not in the giant component with probability $u$. </span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a>There are $n-1$ nodes other than $i$, and so the probability that $i$ is not connected to any other node in the giant component is $(1 - p + pu)^{n-1}$. We therefore have the equation </span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a>u = (1 - p + pu)^{n-1}\;.</span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a>Let's take the righthand side and use $p = c/(n-1)$: </span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a>    u &amp;= (1 - p(1-u))^{n-1} <span class="sc">\\</span> </span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a>      &amp;= \left(1 - \frac{c(1-u)}{n-1}\right)^{n-1}\;.</span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a>This is a good time to go back to precalculus and remember the limit definition of the function $e^x$:</span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a>e^x = \lim_{n \rightarrow \infty}\left(1 + \frac{x}{n}\right)^{n}\;. </span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a>Since we are allowing $n$ to grow large in our application, we approximate </span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a>u \approx e^{-c(1-u)}\;. </span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a>So, now we have a description of the fraction of nodes that *aren't* in the giant component. We can get a description of how many nodes *are* in the giant component by substituting $s = 1-u$, after which we get the equation we're really after: </span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a>s = 1- e^{-cs}</span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a>$${#eq-giant-component-size}</span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a>This equation doesn't have a closed-form solution for $s$, but we can still plot it and compare the result to simulations (@fig-giant-component). Not bad! </span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.cap : "Each point gives the fraction of an ER graph with 50,000 nodes occupied by the largest component. The mean degree is on the horizontal axis. The black line gives the theoretical prediction of @eq-giant-component-size."</span></span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a><span class="co">#| out.width : 80%</span></span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a><span class="co">#| cap-location: margin</span></span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-giant-component</span></span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a><span class="co"># experiment: compute the size of the largest connected </span></span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a><span class="co"># component as a function of graph size for a range of mean degrees. </span></span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> largest_component(n, p):</span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.fast_gnp_random_graph(n, p)</span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> <span class="bu">max</span>(nx.connected_components(G), key<span class="op">=</span><span class="bu">len</span>)</span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(S) <span class="op">/</span> n</span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">50000</span></span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.repeat(np.linspace(<span class="fl">0.5</span>, <span class="fl">1.5</span>, <span class="dv">11</span>), <span class="dv">10</span>)</span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> np.array([largest_component(n, c<span class="op">/</span>(n<span class="op">-</span><span class="dv">1</span>)) <span class="cf">for</span> c <span class="kw">in</span> C])</span>
<span id="cb6-525"><a href="#cb6-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-526"><a href="#cb6-526" aria-hidden="true" tabindex="-1"></a><span class="co"># theory: prediction based on Newman 11.16</span></span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">.001</span>, <span class="fl">.6</span>, <span class="dv">101</span>)</span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a>C_theory <span class="op">=</span> <span class="op">-</span>np.log(<span class="dv">1</span><span class="op">-</span>S)<span class="op">/</span>S</span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the results to compare</span></span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a>plt.plot(C_theory, </span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a>         S, </span>
<span id="cb6-535"><a href="#cb6-535" aria-hidden="true" tabindex="-1"></a>         color <span class="op">=</span> <span class="st">"black"</span>, </span>
<span id="cb6-536"><a href="#cb6-536" aria-hidden="true" tabindex="-1"></a>         label <span class="op">=</span> <span class="st">"Theoretical prediction"</span>)</span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a>plt.scatter(C, </span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a>            U, </span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> <span class="st">"Experiment"</span>)</span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Mean degree"</span>, </span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a>              ylabel <span class="op">=</span> <span class="st">"Proportion of graph in largest component"</span>)</span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a><span class="fu">### References</span></span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>