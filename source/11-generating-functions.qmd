---
code-fold: true
code-summary: "Show code"
jupyter: python3
---

# Probability Generating Functions

In this lecture we'll introduce a powerful tool for calculating network properties: probability generating functions.

::: {.callout-note icon=false appearance="minimal"}
::: {#def-pgf}

Let $K$ be the random variable denoting node degree, with degree distribution $p_k$.  The **probability generating function** $g(z)$ is given by

$$
    g(z) = p_0 + p_1z + p_2z^2 + \dots = \sum_{k=0}^\infty p_kz^k \,.
$$

:::
:::

We note that the degree distribution and the probability generating function give two different mathematical representations of the same idea. The probability generating function is a **power series** representation of the degree distribution of $K$. This is quite useful because the probability generating function is always a polynomial, and there is a lot of rich theory we can leverage.

::: {.callout-caution}
**Exercise**: Suppose we have a network where we observe the following data.

| Node Degree | Percentage of Nodes with Given Degree|
| ------ | ----- |
| 0 | 12% |
| 1 | 20% |
| 2 | 58% |
| 3 | 10% |

What is the probability generating function $g(z)$ associated with the degree distribution for this network? 
:::

::: {.hide .solution}
The probability that a node has degree $k$ in our network is equal to the fraction of nodes with that degree. Therefore, we have
$$
    g(z) = 0.12 + .2z + 0.58z^2 + .1 z^3 \,,
$$
noting that $p_k = 0$ for $k>3.$
:::

::: {.callout-caution}
**Exercise**: Suppose $K$ is Poisson distributed with mean degree $c$. What is the probability generating function for $K$?

*Recall that the probability mass function a Poisson distribution with mean $c$ is given by $\frac{c^k e^{-c}}{k!}.$*
:::

::: {.hide .solution}
Substituting $p_k = \frac{c^k e^{-c}}{k!}$ into the formula above yields the probability generating function
\begin{align*}
    g(z) &= \sum_{k=0}^\infty e^{-c}\frac{c^k}{k!}z^k \\
        &= e^{-c}\sum_{k=0}^\infty\frac{c^k}{k!}z^k \\
        &= e^{-c} e^{cz} \\
        &= e^{c(z-1)} \,,
\end{align*}
where in the third line we have used the Taylor series
$$
e^{x} = \sum_{k=0}^\infty \frac{x^k}{k!} \,.  
$$ 

This series converges for all (finite) values of $c$ and $k$.
:::

::: {.callout-caution}
**Exercise**: What is the probability generating function for the exponential distribution of the form
$$
    p_k = (1-a)a^k, \quad 0<a<1 \,.
$$
:::

::: {.hide .solution}
Substituting $p_k = (1-a)a^k$ into the formula above yields the probability generating function
$$
    g(z) = (1-a)\sum_{k=0}^\infty \left(az \right)^k \,.
$$

However, notice that this series diverges if $az \geq 1$, that is, if $z \geq \frac{1}{a}.$ Thus, we have
$$
    g(z) = \frac{1-a}{1-az}, \quad \vert z \vert <\frac{1}{a}.
$$
:::

The examples above give a sense of how to calculate probability generating functions from known degree distributions. As the next theorem shows, we can also find the degree distribution from the probability generating function. We'll prove this theorem in class.

::: {.callout-tip icon=false collapse=true}
::: {#thm-pgf}
Let $g(z)$ be the probability generating function associated with random variable $K$. Then the associated probability distribution $p_k$ satisfies
$$
p_k = \frac{1}{k!}\frac{d^k}{dz^k}g(z) \bigg \vert_{z=0} 
$$
for all $k = 0, 1, 2, \dots \,.$
:::
:::

::: {.hide .proof}
We know that $g(z)$ is a polynomial, and thus infinitely differentiable. Using the power rule for polynomial differentiation, the $k$th derivative of $g$ with respect to $z$ satisfies
$$
g^{(k)}(z) = k!p_k + \frac{(k+1)}{1!}p_{k+1}z + \frac{(k+2)}{2!}p_{k+2}z^2 + \dots \,.
$$
If we evaluate at $g=0$, all terms except the first will vanish:
$$
g^{(k)}(0) = k!p_k \,. 
$$

Solving for $p_k$ gives the desired result.
:::

## Normalization and moments

The **moments** of a probability distribution are quantities that give us important information about the shape of the distribution. The $m$th moment of a probability distribution $p_k$ is

$$
 \langle k^m \rangle = \sum_{k=0}^\infty k^m p_k \,. 
$$

We recognize that moments have already shown up in several of our calculations thus far. For example, the zeroth moment of a degree distribution is always 1 since $\sum_{k=0}^\infty p_k = 1$. The first moment $\langle k \rangle$ is the expected value. The second moment showed up in our previous lecture when discussing the friendship paradox.

We can now see that generating functions give us a nice way to calculate moments. Using the definition of the probability generating function, we see that the zeroth moment is given by
$$
    g(1) = \sum_{k=0}^\infty p_k = 1 \,.
$$
If we take the derivative of the probability generating function with respect to $z$ we obtain
$$
    g'(z) = \sum_{k=0}^\infty kp_kz^{k-1} \,.
$$
If we evaluate at $z=1$, we get the first moment of $p_k$:
$$
    g'(1) = \sum_{k=0}^\infty kp_k = \langle k \rangle\,.
$$

Unfortunately, we can see that $g''(1)$ will not exactly give us the second moment. A small modification, however, will do the trick. 

\begin{align*}
    z \frac{d}{dz} \left(z \frac{dg}{dz}\right) &=  z \frac{d}{dz} \left( \sum_{k=0}^\infty kp_kz^{k} \right) \\
    &= z \sum_{k=0}^\infty k^2p_kz^{k-1} \\
    &= \sum_{k=0}^\infty k^2p_kz^{k} \,.
\end{align*}

Evaluating this quantity at $z=1$ will give the second moment of the distribution:
$$
    \left( z \frac{d}{dz} \right)^2 g(z) \bigg \vert_{z=1} = \langle k^2 \rangle \,.
$$

This strategy generalizes to higher-order moments as well.

::: {.callout-note icon=false appearance="minimal"}
::: {#def-moments}

Let $K$ be the random variable denoting node degree with probability generating function $g(z)$. The **$m$th moment** of the distribution of $K$ is given by
$$
    \langle k^m \rangle = \left( z \frac{d}{dz} \right)^m g(z) \bigg \vert_{z=1} \,.
$$ 

:::
:::

## Products of generating functions

In this section, we will show the following very useful property of generating functions.

::: {.callout-note icon=false appearance="minimal"}
::: {#thm-multiplicative}

Let $K_1, \dots, K_m$ be independent and identically distributed (i.i.d.) random variables drawn from probability distributions $p_k^{(1)}, p_k^{(2)}, \dots, p_k^{(m)}$ with corresponding probability generating functions $g_1(z), \dots g_m(z)$. Then $X = \sum_{i=1}^m K_i$ has the generating function $g_X(z) = \prod_{i=1}^m g_i(z)$.

:::
:::

::: {.proof}
Since $K_i$ are i.i.d, the probability of drawing a particular set of values $\mathcal{K} = \{k_1, k_2, \dots, k_m\}$ is 
$$
    (p_{k_1}^{(1)})(p_{k_2}^{(2)}) \dots (p_{k_m}^{(m)}) =  \prod_{i=1}^m p_{k_i}^{(i)} \,.
$$ 

Next, we will calculate the probability $\pi_s$ that the values add to a specific total $s$. We do this by summing the product over all sets $\mathcal{K}$ that add to $s$.

$$
    \pi_s = \sum_{k_1=0}^ \infty \dots \sum_{k_m=0}^ \infty \delta_{s, \sum_i k_i} \prod_{i=1}^m p_{k_i}^{(i)}\,.
$$
where $\delta_{s, \sum_i k_i}$ is the Kronecker delta.

Now that we have this probability, we can write down a probability generating function for the distribution $\pi_s$:

\begin{align*}
    h(z) &= \sum_{s=0}^\infty \pi_s z^s \\
    &= \sum_{s=0}^\infty z^s \left[ \sum_{k_1=0}^\infty \dots \sum_{k_m=0}^\infty \delta_{s, \sum_i k_i} \prod_{i=1}^m p_{k_i}^{(i)} \right] \,.
\end{align*}

Since $s= \sum_i k_i$ for each case, we can rewrite the above as
\begin{align*}
    h(z) &= \sum_{k_1=0}^\infty \dots \sum_{k_m=0}^\infty z^{\sum_i k_i}\prod_{i=1}^m p_{k_i}^{(i)} \\
    &= \prod_{i=1}^m \sum_{k_i = 0}^\infty p_{k_i}^{(i)} z^{k_i} \,.
\end{align*}

But, simplifying the notation on the indices, we see that $\sum_{k_i = 0}^\infty p_{k_i}^{(i)} z^{k_i} = \sum_{k = 0}^\infty p_{k}^{(i)} z^{k} = g_i(z).$ 

Thus,
$$
    h(z) = \prod_{i=1}^m g_i(z) \,
$$
as required.

:::

This theorem has a very useful corollary in the case where all the random variables are drawn from the same distribution.

::: {.callout-note icon=false appearance="minimal"}
**Corollary**. If $K_1, \dots, K_m$ are independent and identically distributed (i.i.d.) random variables drawn from the same probability distribution $p_k$, then the sum $X = \sum_{i=1}^m K_i$ has the generating function $(g(z))^m$, where $g(z)$ is the probability generating function for $p_k.$
:::

This corollary is useful when summing the degrees of $m$ randomly chosen nodes from degree distribution $p_k$. 

## References









