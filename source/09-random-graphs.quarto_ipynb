{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "code-fold: true\n",
        "code-summary: \"Show code\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Random Graphs: Erdős–Rényi\n",
        "\n",
        "A random graph is a probability distribution over graphs. In this set of lecture notes, we'll begin our exploration of several random graphs. \n",
        "\n",
        "## Why Random Graphs?\n",
        "\n",
        "Why should we even study random graphs? There are a few reasons!\n",
        "\n",
        "### Random Graphs as Insightful Models\n",
        "\n",
        "Random graphs allow us to build our intuition and skills in the study of networks. In many simple random graphs, quantities of interest (clustering coefficients, diameters, etc) can be calculated with pencil and paper. This allows us to build mathematical insight into the structure of many real-world models, without the need for detailed simulations. \n",
        "\n",
        "### Random Graphs as Mathematical Puzzles\n",
        "\n",
        "Many properties of even simple random graphs are still under investigation by research mathematicians. While some properties can be calculated simply, others require extremely sophisticated machinery in order to understand. There are many mathematicians who spend their careers studying random graphs, and their cousins, random matrices. \n",
        "\n",
        "### Random Graphs as Null Hypotheses\n",
        "\n",
        "Suppose that you compute the global clustering coefficient of a graph and find it to be $0.31$. How do we interpret that? Is that high? Low? In this case, we should ask: *compared to what?* Random graphs allow us one way to make a comparison: that clustering coefficient is \"high\" if it's larger than the clustering coefficient in a suitably chosen, comparable random graph. How to choose a comparable random graph is an important question, and the answer is not always clear! \n",
        "\n",
        "In this way, random graphs often serve as the *null hypothesis*, in exactly the same way you might have heard of the null hypothesis for other kinds of statistical tests. \n",
        "\n",
        "### Random Graphs and Statistical Inference\n",
        "\n",
        "Many statistical algorithms for tasks like graph clustering, ranking, and prediction come from random graph models. The idea, generally speaking, is to imagine that we observe a graph, and then try to make the best educated guess possible about the model that generated that graph. This is classical statistical inference. \n",
        "\n",
        "## The $G(n,p)$ Model (Erdős–Rényi)\n",
        "\n",
        "The model of @erdHos1960evolution is the simplest and most fundamental model of a random graph. Our primary interest in the ER model is for mathematical insight and null modeling. The ER model is *mostly* understood in its major mathematical properties, and it's almost never used in statistical inference. [The ER model is, however, a building block of models that *are* used in statistical inference. The [stochastic blockmodel](https://en.wikipedia.org/wiki/Stochastic_block_model), for example, is often used for graph clustering and community detection. In its simplest form, it's a bunch of ER graphs glued together.]{.aside}\n",
        "\n",
        "::: {.callout-note}\n",
        "::: {#def-ER}\n",
        "\n",
        "## Erdős–Rényi Random Graph\n",
        "\n",
        "An *Erdős–Rényi random graph* with $n$ nodes and connection probability $p$, written $G(n,p)$, is a random graph constructed by placing an edge with probability $p$ between each pair of distinct nodes. \n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "We can imagine visiting each possible pair of edges $(i,j)$ and flipping a coin with probability of heads $p$. If heads, we add $(i,j) \\in E$; otherwise, we don't. \n",
        "\n",
        "::: {.callout-caution}\n",
        "**Exercise**: Let $i$ be a fixed node in a $G(n,p)$ graph, and let $K_i$ be its (random) degree. Show that $K_i$ has binomial distribution with success probability $p$ and $n-1$ trials. \n",
        ":::\n",
        "\n",
        "::: {.hide .solution}\n",
        "A given node $u$ is connected to each of the other $n-1$ nodes (independently) with probability $p$. The probability of being connected to a particular set of $k$ nodes is \n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbb{P}[\\text{connected to $k$ given nodes}] &= \\mathbb{P}[\\text{$k$ successes}]\\mathbb{P}[\\text{$n-1-k$ failures}] \\\\\n",
        "    &= p^k (1-p)^{n-1-k} \\,.\n",
        "\\end{align}\n",
        "\n",
        "Building on this, the probability of being connected to $k$ other nodes in any way is\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbb{P}[\\text{connected to $k$ nodes}] &= \\left( \\text{number of ways to choose $k$ nodes }\\right) \\mathbb{P}[\\text{$k$ successes}]\\mathbb{P}[\\text{$n-1-k$ failures}] \\\\ \\\\\n",
        "    &= {n-1 \\choose k} p^k (1-p)^{n-1-k} \\,.\n",
        "\\end{align}\n",
        "\n",
        "Indeed, this is exactly the binomial degree distribution, as required.\n",
        ":::\n",
        "\n",
        "::: {.callout-caution}\n",
        "**Exercise**: Show that $\\mathbb{E}[K_i] = p(n-1)$. \n",
        ":::\n",
        "\n",
        "::: {.hide .solution}\n",
        "Using our previous calculations, we know $\\mathbb{E}[K_i] = \\mathbb{E}[\\frac{2m}{n}].$ However, $n$ is fixed for the $G(n,p)$ model, so we have $\\mathbb{E}[K_i] = \\frac{2}{n}\\mathbb{E}[m],$ where $\\mathbb{E}[m]$ is the expected number of edges. This quantity will be equal to the probability that an edge exists between a pair of nodes multiplied by the number of possible pairs, which is ${n \\choose 2}p = \\frac{n(n-1)}{2}p$. Thus $\\mathbb{E}[K_i] = (n-1)p$, as required.\n",
        ":::\n",
        "\n",
        "### Clustering Coefficient\n",
        "\n",
        "Both local and global clustering coefficients are defined in terms of a ratio of realized triangles to possible triangles. \n",
        "Analyzing ratios using probability theory can get tricky, but we can get a pretty reliable picture of things by computing the expectations of the numerator and denominator separately. \n",
        "\n",
        "Let's take the global clustering coefficient. For this, we need to compute the total number of triangles, and the total number of wedges. \n",
        "\n",
        "How many triangles are there? Well, there are $\\binom{n}{3}$ ways to choose 3 nodes from all the possibilities, and the probability that all three edges exist to form the triangle is $p^3$. \n",
        "So, in expectation, there are $\\binom{n}{3}p^3$ triangles in the graph. \n",
        "\n",
        "How many wedges are there? Well, each triple of nodes contains three possible wedges, and the probability of any given wedge existing is $p^2$. \n",
        "So, the expected number of wedges is $\\binom{n}{3}p^2$. Our estimate for the expected clustering coefficient is that it should be *about* $p$, although we have been fast and loose with several mathematical details to arrive at this conclusion. \n",
        "\n",
        "## Sparsity\n",
        "\n",
        "Recall that it is possible to define sparsity more explicitly when we have a theoretical model (where it makes sense to take limits). Indeed, the *sparse* Erdős–Rényi model is very useful. Intuitively, the idea of sparsity is that there are not very many edges in comparison to the number of nodes. \n",
        "\n",
        "::: {.callout-note}\n",
        "::: {#def-sparse-ER}\n",
        "We say that a $G(n,p)$ graph is **sparse** when $p = c/(n-1)$ for some constant $c$. \n",
        ":::\n",
        ":::\n",
        "\n",
        "A consequence of sparsity is that $\\mathbb{E}[K_i] = c$; i.e. the expected degree of a node in sparse $G(n,p)$ is constant. When studying sparse $G(n,p)$, we are almost always interested in the case $n\\rightarrow \\infty$. \n",
        "\n",
        "\n",
        "### Clustering\n",
        "\n",
        "What does this imply for our estimation of the global clustering coefficient from before? Well, we expect the global clustering coefficient to be *about* $p$, and if $p = c/(n-1)$, then $p \\rightarrow 0$ as $n \\to \\infty$ for sparse $G(n,p)$. \n",
        "\n",
        "[The need to move beyond the ER model to develop sparse graphs with clustering coefficients was part of the motivation of @watts1998collective, a famous paper that introduced the \"small world model.\"]{.aside}\n",
        "\n",
        "> Sparse Erdős–Rényi graphs have vanishing clustering coefficients. \n",
        "\n",
        "@fig-clustering-decay shows how the global clustering coefficient of a sparse Erdős–Rényi random graph decays as we increase $n$. Although the estimate that the global clustering coefficient should be equal to $p$ was somewhat informal, experimentally it works quite well. \n"
      ],
      "id": "67c2e29d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig.cap": "Each point gives the global clustering coefficient of an ER graph with mean degree $c = 5$ and specified number of nodes. The black line gives the estimate $T = p = c/(n-1)$ for the clustering coefficient that we computed earlier. The function `nx.fast_gnp_random_graph()` is a special function from NetworkX for generating ER random graphs quickly; you'll learn about how it works in an upcoming homework assignment.",
        "out.width": "80%"
      },
      "source": [
        "#| cap-location: margin\n",
        "#| label: fig-clustering-decay\n",
        "\n",
        "import networkx as nx\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "c = 5\n",
        "N = np.repeat(2**np.arange(5, 15), 10)\n",
        "\n",
        "def global_clustering(n, c):\n",
        "    G = nx.fast_gnp_random_graph(n, c/(n-1))\n",
        "    return nx.transitivity(G)\n",
        "\n",
        "T = [global_clustering(n, c) for n in N]\n",
        "\n",
        "fig, ax = plt.subplots(1)\n",
        "\n",
        "theory = ax.plot(N, c/(N-1), color = \"black\", label = \"Estimate\")\n",
        "\n",
        "exp = ax.scatter(N, T, label = \"Experiments\")\n",
        "semil = ax.semilogx()\n",
        "labels = ax.set(xlabel = \"Number of nodes\", \n",
        "       ylabel = \"Global clustering coefficient\")\n",
        "plt.show()"
      ],
      "id": "fig-clustering-decay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cycles and Local Tree-Likeness\n",
        "\n",
        "Recall the following definition:\n",
        "\n",
        "::: {.callout-note}\n",
        "::: {#def-cycle}\n",
        "A **cycle** is a walk that does not repeat edges and ends at the same node that it begins. \n",
        ":::\n",
        ":::\n",
        "\n",
        "A triangle is an example of a cycle of length $3$. \n",
        "\n",
        "::: {.callout-tip}\n",
        "::: {#thm-rare-cycles}\n",
        "In the sparse $G(n,p)$ model, for any length $k$, the probability that there exists a cycle of length $k$ attached to node $i$ shrinks to 0 as $n \\rightarrow \\infty$. \n",
        ":::\n",
        ":::\n",
        "\n",
        "You'll prove a generalization of @thm-rare-cycles in an upcoming homework problem. \n",
        "\n",
        "Graphs in which cycles are very rare are often called *locally tree-like*. A tree is a graph without cycles; if cycles are very rare, then we can often use techniques that are normally guaranteed to only work on trees without running into (too much) trouble. \n",
        "\n",
        "\n",
        "### Path Lengths\n",
        "\n",
        "How far apart are two nodes in $G(n,p)$? Again, exactly computing the length of geodesic paths involves some challenging mathematical detail. [See @riordan2010diameter and references therein.]{.aside} However, we can get a big-picture view of the situation by asking a slightly different question: \n",
        "\n",
        "> Given two nodes $i$ and $j$, what is the expected number of paths of length $k$ between them?\n",
        "\n",
        "Let $R(k)$ denote the number of $k$-paths between $i$ and $j$. Let $r(k) = \\mathbb{E}[R(k)]$. Let's estimate $r(k)$. \n",
        "\n",
        "First, we know that $r(1) = p$. For higher values of $k$, we'll use the following idea: in order for there to be a path of length $k$ from $i$ to $j$, there must be a node $\\ell$ such that: \n",
        "\n",
        "- There exists a path from $i$ to $\\ell$ of length $k-1$. In expectation, there are $r(k-1)$ of these. \n",
        "- There exists a path from $\\ell$ to $j$ of length $1$. This happens with probability $p$. \n",
        "\n",
        "There are $n-2$ possibilities for $\\ell$ (excluding $i$ and $j$), and so we obtain the approximate relation\n",
        "\n",
        "$$\n",
        "r(k) \\approx (n-2)r(k-1)p\\;. \n",
        "$$\n",
        "\n",
        "\n",
        ":::{.callout-warning}\n",
        "Why is this an approximation? Well, some of the paths between $i$ and $\\ell$ that are counted in $r(k-1)$ could actually include the edge $(j, \\ell)$ *already*. An example is $(i,j), (j,\\ell)$. In this case, the presence of edge $(j,\\ell)$ is not independent of the presence of the path between $i$ and $\\ell$. The derivation above implicitly treats these two events as independent. Again, because *cycles are rare in large, sparse ER*, this effect is small when $k$ is small. \n",
        ":::\n",
        "\n",
        "Proceeding inductively and approximating $n-2 \\approx n-1$ for $n$ large, we have the relation \n",
        "\n",
        "$$\n",
        "r(k) \\approx (n-1)^{k-1}p^{k-1}r(1) = c^{k-1}p\n",
        "$$ {#eq-path-expectation}\n",
        "\n",
        "in the sparse ER model. \n",
        "\n",
        "Using this result, let's ask a new question: \n",
        "\n",
        "> What path length $k$ do I need to allow to be confident that there's a path between nodes $i$ and $j$? \n",
        "\n",
        "Well, suppose we want there to be $q$ paths. Then, we can solve $q = c^{k-1}p$ for $k$, which gives us: \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "q &= c^{k-1}p \\\\ \n",
        "\\log q &= (k-1)\\log c + \\log p \\\\ \n",
        "\\log q &= (k-1)\\log c + \\log c - \\log n \\\\ \n",
        "\\frac{\\log q + \\log n}{\\log c} &= k\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Here, I've also approximated $\\log n-1 \\approx \\log n$ for $n$ large. \n",
        "\n",
        "So, supposing that I want there to be at least one path in expectation ($q = 1$), I need to allow $k = \\frac{\\log n}{\\log c}$. This is pretty short, actually! For example, the population of the world is about $8\\times 10^9$, and Newman estimates that an average individual knows around 1,000 other people; that is, $c = 10^3$ in the world social network. The resulting value of $k$ here is around 3.3. \n",
        "\n",
        "In other words, this calculation suggests that, if the world were an ER network, it would be the case that any two individuals would be pretty likely to have at least one path between them of length no longer than $4$. \n",
        "\n",
        "More formal calculations regarding the diameter of the ER graph confirm that the diameter of the ER graph grows slowly as a function of $n$, even in relatively sparse cases. \n",
        "\n",
        "### A Caveat\n",
        "\n",
        "If you spend some time looking at @eq-path-expectation, you might find yourself wondering:\n",
        "\n",
        "> Hey, what happens if $c \\leq 1$?  \n",
        "\n",
        "Indeed, something *very* interesting happens here. \n",
        "Let's assume $c < 1$ (i.e. we're ignoring the case $c = 1$), and estimate the expected number of paths between $i$ and $j$ of *any* length. Using @eq-path-expectation, we get \n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\sum_{k = 1}^{\\infty} R(k)\\right] = \\sum_{k = 1}^\\infty c^{k-1}p = \\sum_{k = 0}^\\infty c^kp = \\frac{p}{1-c}\\;.\n",
        "$$\n",
        "\n",
        "If we now use Markov's inequality, [Markov's inequality states that $\\mathbb{P}(X \\geq a) \\leq \\frac{\\mathbb{E}(X)}{a}$.]{.aside} we find that the probability that there is a path of *any* length between nodes $i$ and $j$ is no larger than $\\frac{p}{1-c}.$ In the sparse regime, we can substitute $p = \\frac{c}{n-1}$ to see that  \n",
        "\n",
        "$$\n",
        "\\frac{c}{(1-c)(n-1)}\\rightarrow 0 \\text{ as } n \\to \\infty\\;. \n",
        "$$\n",
        "\n",
        "So, this suggests that, if $c < 1$, any two nodes are likely to be disconnected! On the other hand, if $c > 1$, we've argued that we can make $k$ large enough to have high probability of a path of length $k$ between those nodes. \n",
        "\n",
        "So, what's special about $c = 1$? This question brings us to one of the first and most beautiful results in the theory of random graphs. To get there, let's study in a bit more detail the sizes of the *connected components* of the ER graph. \n",
        "\n",
        "## Component Sizes and the Branching Process Approximation\n",
        "\n",
        "We're now going to ask ourselves about the size of a \"typical\" component in the Erdős–Rényi model. In particular, we're going to be interested in whether there exists a component that fills up \"most\" of the graph, or whether components tend to be vanishingly small in relation to the overall graph size. \n",
        "\n",
        "Our first tool for thinking about this question is the *branching process approximation.* Informally, a branching process is a process of *random generational growth*. We'll get to a formal mathematical definition in a moment, but the easiest way to get insight is to look at a diagram: \n",
        "\n",
        "::: {#fig-branching}\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"http://1.bp.blogspot.com/-Y59SK92Nd0c/VIoqelzfc8I/AAAAAAAAAdA/XAQB5yDStUc/s1600/eg1.PNG\"></iframe>\n",
        "\n",
        "[Image source](http://mytechroad.com/markov-chain-branching-process/).\n",
        ":::\n",
        "\n",
        "We start with a single entity, $X_0$. Then, $X_0$ has a random number of \"offspring\": $X_1$ in total. Then, each of those $X_1$ offspring has some offspring of their own; the total number of these offspring is $X_2$. The process continues infinitely, although there is always a chance that at some point no more offspring are produced. In this case, we often say that the process \"dies out.\"  \n",
        "\n",
        "\n",
        "::: {.column-margin}\n",
        "Some of this exposition in this section draws on [these notes](https://www.stat.berkeley.edu/users/aldous/Networks/lec2.pdf) by David Aldous. \n",
        ":::\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "::: {#def-branching-process}\n",
        "\n",
        "##  Branching Process\n",
        "\n",
        "Let $p$ be a probability distribution on $\\mathbb{Z}$, called the **offspring distribution**.  \n",
        "\n",
        "A **branching process** with distribution $p$ is a sequence of random variables $X_0, X_1,,X_2\\ldots$ such that $X_0 = 1$ and, for $t \\geq 1$,  \n",
        "\n",
        "$$\n",
        "X_t = \\sum_{i = 1}^{X_{t-1}} Y_i\\;,\n",
        "$$\n",
        "\n",
        "where each $Y_i$ is distributed i.i.d. according to $p$. \n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column-margin}\n",
        "Technically, this is a *Galton-Watson* branching process, named after the two authors who first proposed it [@watson1875probability]. <br> <br> **History note**: Galton, one of the founders of modern statistics, was a eugenicist. The cited paper is explicit about its eugenicist motivation: the guiding question was about whether certain family names associated with well-to-do aristocrats were giving way to less elite surnames. \n",
        ":::\n",
        "\n",
        "### Application to Erdős–Rényi \n",
        "\n",
        "Branching processes create *trees* -- graphs without cycles. The reason that branching processes are helpful when thinking about Erdős–Rényi models is that *cycles are rare in Erdős–Rényi random graphs*. So, if we can understand the behavior of branching processes, then we can learn something about the Erdős–Rényi random graph as well. \n",
        "\n",
        "Here's the particular form of the branching process approximation that we will use: \n",
        "\n",
        "\n",
        ":::{.callout-note}\n",
        ":::{#def-branching-approx}\n",
        "\n",
        "## Branching Process Approximation for ER Component Sizes\n",
        "\n",
        "Sample a single node $j$ at random from a large, sparse ER graph  with mean degree $c$, and let $S$ be the size (number of nodes) of the component in which $j$ lies. Note that $S$ is random: it depends both on $j$ and on the realization of the ER graph. \n",
        "\n",
        "Then, $S$ is distributed approximately as $T$, where $T = \\sum_{i = 0}^{\\infty}X_t$ is the total number of offspring in a GW branching process with offspring distribution $\\text{Poisson}(c)$. \n",
        ":::\n",
        ":::\n",
        "\n",
        "The idea behind this approximation is: \n",
        "\n",
        "- We start at $j$, whose number of neighbors $\\sim \\text{Poisson}(c)$. \n",
        "- Each of these neighbors has a number of new neighbors $\\sim \\text{Poisson}(c)$, and so on. \n",
        "- We keep visiting new neighbors until we run out, and add up the number of neighbors we've visited to obtain $S$.  \n",
        "- Since *cycles are rare in ER*, we are unlikely to double-count any nodes (doing so would create a cycle), and so this whole process *also* approximately describes $T$ in a branching process with a $\\text{Poisson}(c)$ offpsring distribution. \n",
        "\n",
        "<!-- :::{.callout-important}\n",
        "**Exercise**: In the second generation, we get to a new node by following an edge. For that reason, shouldn't the number of *new* edges be $\\text{Poisson}(c-1)$ rather than $\\text{Poisson}(c-1)$? Why or why not? \n",
        "::: -->\n",
        "\n",
        "### The Subcritical Case\n",
        "\n",
        "The mean of a $\\text{Poisson}(c)$ random variable is again $c$. As you'll show in homework, this implies that $X_t$, the number of offspring in generation $t$, satisfies $\\mathbb{E}[X_t] = c^{t}$. It follows that, when $c < 1$, $\\mathbb{E}[T] = \\frac{1}{1-c}$. \n",
        "\n",
        "Now using Markov's inequality, we obtain the following results: \n",
        "\n",
        "::: {.callout-tip}\n",
        "\n",
        "In a $\\text{Poisson}(c)$ branching process with $c < 1$, \n",
        "$$\\mathbb{P}(X_t > 0) \\leq c^t\\;.$$ \n",
        ":::\n",
        "\n",
        "So, the probability that the branching process hasn't yet \"died out\" decays exponentially with timestep $t$. In other words, the branching process becomes very likely to die out very quickly. \n",
        "\n",
        "::: {.callout-tip}\n",
        "\n",
        "In a $\\text{Poisson}(c)$ branching process with $c < 1$, \n",
        "$$\\mathbb{P}(T \\geq a) \\leq \\frac{1}{a}\\frac{1}{1-c}$$\n",
        ":::\n",
        "\n",
        "In particular, for $a$ very large, we are guaranteed that $\\mathbb{P}(T > a)$ is very small. \n",
        "\n",
        "Summing up, when $c < 1$, the GW branching process dies out quickly and contains a relatively small number of nodes: $\\frac{1}{1-c}$ in expectation. [In this setting, the branching process is called *subcritical*.]{.aside}\n",
        "\n",
        "#### Back to ER\n",
        "\n",
        "If we now translate back to the Erdős–Rényi random graph, the branching process approximation now suggests the following heuristic: \n",
        "\n",
        "::: {.callout-note}\n",
        "**Heuristic**: In a sparse ER random graph with mean $c < 1$, the expected size of a component containing a randomly selected node is roughly $\\frac{1}{1-c}$. \n",
        "\n",
        "In particular, since this quantity is independent of $n$, we find that the *fraction* of the graph occupied by this component is $\\frac{1}{n}\\frac{1}{1-c}$ and therefore vanishes as $n\\rightarrow \\infty$. \n",
        "\n",
        "We can also turn this into a statement about probabilities: Markov's inequality implies that, if $S$ is the size of a component containing a randomly selected node,\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(S/n > a) \\rightarrow 0\n",
        "$$\n",
        "\n",
        "for any constant $a > 0$. In other words, for large $n$, the largest component is always vanishingly small in relation to the graph as a whole. \n",
        ":::\n",
        "\n",
        "Let's check this experimentally. The following code block computes the size of the component in an ER graph containing a random node, and averages the result across many realizations. The experimental result is quite close to the theoretical prediction. \n"
      ],
      "id": "a4be5639"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "def component_size_of_node(n, c):\n",
        "    G = nx.fast_gnp_random_graph(n, c/(n-1))\n",
        "    return len(nx.node_connected_component(G, 1))\n",
        "\n",
        "c = 0.8\n",
        "sizes = [component_size_of_node(5000, c) for i in range(1000)]\n",
        "\n",
        "out = f\"\"\"\n",
        "Average over experiments is {np.mean(sizes):.2f}.\\n\n",
        "Theoretical expectation is {1/(1-c):.2f}.\n",
        "\"\"\"\n",
        "\n",
        "print(out)"
      ],
      "id": "44ea6e67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the expected (and realized) component size is very small, even though the graph contains 5,000 nodes!\n",
        "\n",
        "For this reason, we say that subcritical ER contains only *small* connected components, in the sense that each component contains approximately 0\\% of the graph as $n$ grows large. \n",
        "\n",
        "This explains our result from earlier about path lengths. The probability that any two nodes have a path between them is the same as the probability that they are *on the same connected component*. But if every connected component is small, then the probability that two nodes occupy the same one is vanishes. \n",
        "\n",
        "### The Giant Component\n",
        "\n",
        "::: {.callout-note}\n",
        "::: {#def-giant-component}\n",
        "\n",
        "## Giant Component\n",
        "\n",
        "We say that $G(n,p)$ has a **giant component** if \n",
        "$$\n",
        "\\mathbb{P}(S/n > a) \\rightarrow b\n",
        "$$\n",
        "for some constant $b > 0$. \n",
        "\n",
        "Intuitively, this means that there is a possibility of a connected component that takes up a nonzero fraction of the graph. \n",
        ":::\n",
        ":::\n",
        "\n",
        "So far, we've argued using the branching process approximation that there is no giant component in the Erdős–Rényi model with $c < 1$. The theory of branching processes also suggests to us that there *could* be a giant component when $c > 1$. \n",
        "\n",
        "[The proof of this fact is usually done in terms of *generating functions* and is beyond our scope, but you can [check Wikipedia](https://en.wikipedia.org/wiki/Branching_process#Extinction_problem_for_a_Galton_Watson_process) for an outline.]{.aside}\n",
        "\n",
        "::: {.callout-warning}\n",
        "**Fact**: when $c > 1$, there is a nonzero probability that the $\\text{Poisson}(c)$ branching process continues forever; that is, never goes extinct. \n",
        ":::\n",
        "\n",
        "Using our correspondence between components of the ER model and branching processes, this suggests that, if we pick a random node, the component it is in has the potential to be very large. In fact (and this requires some advanced probability to prove formally), when $c > 1$, there *is* a giant component. This is our first example of a phase transition. \n",
        "[It is also possible to prove that, with high probability, there is only one giant component; Newman does this in 11.5.1.]{.aside}\n",
        "\n",
        "::: {.callout-note}\n",
        "::: {#def-giant-component}\n",
        "\n",
        "## Phase Transition\n",
        "\n",
        "A **phase transition** is a qualitative change in response to a small variation in a quantitative parameter. \n",
        ":::\n",
        ":::\n",
        "\n",
        "Examples of phase transitions include freezing, in which a liquid undergoes a qualitative change into a solid in response to a small variation in temperature. \n",
        "\n",
        "@fig-giant-component-illustration shows  two sparse ER random graphs on either side of the $c = 1$ transition. We observe an apparent change in qualitative behavior between the two cases. \n"
      ],
      "id": "9d4db553"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig.cap": "Two sparse ER graphs with 500 nodes and varying mean degree.",
        "out.width": "100%",
        "fig-width": 8,
        "fig-height": 4
      },
      "source": [
        "#| cap-location: margin\n",
        "#| label: fig-giant-component-illustration\n",
        "\n",
        "import networkx as nx\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, axarr = plt.subplots(1, 2)\n",
        "\n",
        "n = 500\n",
        "c = [0.7, 1.3]\n",
        "\n",
        "for i in range(2):\n",
        "    G = nx.fast_gnp_random_graph(n, c[i]/(n-1))\n",
        "    nx.draw(G, ax = axarr[i], node_size = 1)\n",
        "    axarr[i].set(title = f\"c = {c[i]}\")"
      ],
      "id": "fig-giant-component-illustration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Size of the Giant Component\n",
        "\n",
        "Perhaps surprisingly, while it's difficult to prove that there is a giant component, it's not hard at all to estimate its size. \n",
        "[This argument is reproduced from Newman, pages 349-350]{.aside}\n",
        "\n",
        "Let $S$ be the size of the giant component in an Erdős–Rényi random graph, assuming there is one. Then, $s = S/n$ is the probability that a randomly selected node is in the giant component. Let $u = 1 - s$ be the probability that a given node is *not* in the giant component. \n",
        "\n",
        "Let's take a random node $i$, and ask it the probability that it's in the giant component. Well, one answer to that question is just \"$u$.\" On the other hand, we can *also* answer that question by looking at $i$'s neighbors. If $i$ is not in the giant component, then it can't be connected to any node that is in the giant component. So, for each other node $j\\neq i$, it must be the case that either: \n",
        "\n",
        "1. $i$ is not connected to $j$. This happens with probability $1-p$. \n",
        "2. $i$ is connected to $j$, but $j$ is not in the giant component either. $i$ is connected to $j$ with probability $p$, and $j$ is not in the giant component with probability $u$. \n",
        "\n",
        "There are $n-1$ nodes other than $i$, and so the probability that $i$ is not connected to any other node in the giant component is $(1 - p + pu)^{n-1}$. We therefore have the equation \n",
        "\n",
        "$$\n",
        "u = (1 - p + pu)^{n-1}\\;.\n",
        "$$\n",
        "\n",
        "Let's take the righthand side and use $p = c/(n-1)$: \n",
        "$$\n",
        "\\begin{aligned}\n",
        "    u &= (1 - p(1-u))^{n-1} \\\\ \n",
        "      &= \\left(1 - \\frac{c(1-u)}{n-1}\\right)^{n-1}\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "This is a good time to go back to precalculus and remember the limit definition of the function $e^x$:\n",
        "$$\n",
        "e^x = \\lim_{n \\rightarrow \\infty}\\left(1 + \\frac{x}{n}\\right)^{n}\\;. \n",
        "$$\n",
        "Since we are allowing $n$ to grow large in our application, we approximate \n",
        "\n",
        "$$\n",
        "u \\approx e^{-c(1-u)}\\;. \n",
        "$$\n",
        "So, now we have a description of the fraction of nodes that *aren't* in the giant component. We can get a description of how many nodes *are* in the giant component by substituting $s = 1-u$, after which we get the equation we're really after: \n",
        "$$\n",
        "s = 1- e^{-cs}\n",
        "$${#eq-giant-component-size}\n",
        "\n",
        "This equation doesn't have a closed-form solution for $s$, but we can still plot it and compare the result to simulations (@fig-giant-component). Not bad! \n"
      ],
      "id": "c045be8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig.cap": "Each point gives the fraction of an ER graph with 50,000 nodes occupied by the largest component. The mean degree is on the horizontal axis. The black line gives the theoretical prediction of @eq-giant-component-size.",
        "out.width": "80%"
      },
      "source": [
        "#| cap-location: margin\n",
        "#| label: fig-giant-component\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# experiment: compute the size of the largest connected \n",
        "# component as a function of graph size for a range of mean degrees. \n",
        "\n",
        "def largest_component(n, p):\n",
        "    G = nx.fast_gnp_random_graph(n, p)\n",
        "    S = max(nx.connected_components(G), key=len)\n",
        "    return len(S) / n\n",
        "\n",
        "n = 50000\n",
        "C = np.repeat(np.linspace(0.5, 1.5, 11), 10)\n",
        "U = np.array([largest_component(n, c/(n-1)) for c in C])\n",
        "\n",
        "# theory: prediction based on Newman 11.16\n",
        "\n",
        "S = np.linspace(-.001, .6, 101)\n",
        "C_theory = -np.log(1-S)/S\n",
        "\n",
        "# plot the results to compare\n",
        "\n",
        "plt.plot(C_theory, \n",
        "         S, \n",
        "         color = \"black\", \n",
        "         label = \"Theoretical prediction\")\n",
        "\n",
        "plt.scatter(C, \n",
        "            U, \n",
        "            label = \"Experiment\")\n",
        "\n",
        "plt.gca().set(xlabel = \"Mean degree\", \n",
        "              ylabel = \"Proportion of graph in largest component\")\n",
        "\n",
        "plt.legend()"
      ],
      "id": "fig-giant-component",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n"
      ],
      "id": "0de58966"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/philchodrow/opt/anaconda3/envs/ml-0451/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}