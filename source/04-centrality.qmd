---
code-fold: true
code-summary: "Show code"
jupyter: python3
---

# Centrality

With our foundational mathematical and computational tools about networks in hand, we turn our attention toward measuring structural features in a network.

The first question we'll tackle is *importance*: Which nodes are the most important in a network? The answer depends on how you define importance! We'll discuss possible interpretations of this question and the resulting *centrality measures*.

**Centrality measures** quantify the relative importance of nodes in a network by assigning numbers to each node; these numbers are sometimes called **centrality scores**. A node's centrality score allows for comparison with another node in the same network. 

Depending on the application, having good answers to the question of importance could help determine how to target interventions, predict how diseases or information might spread, or create rankings and search algorithms.

## Degree-based Centrality Measures

### Degree Centrality

One natural definition of importance would be to suppose that important nodes have lots of connections to other nodes. Conveniently, we already have a way to measure this quantity: this is captured by degree!

If $k_i$ is the degree of node $i$ and we have a network with $n$ nodes, then we can define the $n \times 1$ vector ${\bf c}_{deg} = \left(k_1, k_2, \dots, k_n \right)$ to contain the centralities of each node. Using what we know about degree, that means we can calculate centrality directly from the adjacency matrix $A$:

$$
 {\bf c}_{deg} = {\bf A}{\bf 1}
$$

where ${\bf 1}$ is the vector containing all ones.

For a directed network, we could use either in- or out-degree as centrality measures, depending on what is useful for the context or application.

#### Advantages and Disadvantages

Degree centrality is quick to calculate and interpret, which makes it an attractive choice of centrality measure. The link between network structure and centrality is very clear.

However, we might miss a key feature of relative importance using this simple measure. Perhaps what matters is not only how many connections a node has, but whether or not it is connected to other important nodes. For example, if I have two friends, you may view me as more important if my two friends are Beyonc\'{e} and Taylor Swift than if they were two people you had never heard of. 

### Eigenvector Centrality

**Eigenvector centrality** accounts for the phenomenon described above by assuming that a node with an important connection is more important. The idea is to "weight" each connection for a node by the centrality of the adjacent neighbor. In this way, high centrality is achieved either by having lots of connections, or by having a few important connections.

Suppose we have an undirected network with $n$ nodes. We calculate the centrality $c_i$ of node $i$ by summing the centralities of its neighbor, using some proportionality constant $\frac{1}{r}.$ This gives the equation 

\begin{align}
    c_i &= \frac{1}{r}\sum_{j \in \text{neighbors of } i} c_j \, \\
    &= \frac{1}{r} \sum_{j=1}^n A_{ij}c_j \,.
\end{align}

Let ${\bf c}_{eig}$ be our new centrality vector. Writing the formula above in matrix notation gives

$$
    {\bf c}_{eig} = \frac{1}{r}{\bf A}{\bf c}_{eig} \implies r{\bf c}_{eig} = {\bf A}{\bf c}_{eig} \,.
$$

Now the name of this centrality measure is very clear: ${\bf c}_{eig}$ is an eigenvector of ${\bf A}$ with associated eigenvalue $r$!

This leads us to a challenge: which eigenvector should we choose? We have up to $n$ linearly independent eigenvectors to choose from, as well as linear combinations of these, so our task of making a meaningful choice seems quite daunting. One constraint we'd like to impose is that we'd like our centrality scores to be nonnegative. Will we always be able to find such an eigenvector for any graph?

Fortunately, we have a powerful theorem that can help us with this task.

#### Perron--Frobenius Theorem

::: {.callout-tip icon=false collapse=true}
::: {#thm-perron-frobenius}

## Perron--Frobenius Theorem 

A nonnegative matrix $A$ has a nonnegative eigenvector with corresponding positive eigenvalue. Furthermore, if the matrix is an adjacency matrix for a (strongly) connected network, then the eigenvector is unique and strictly positive, and the corresponding eigenvalue is the largest of all eigenvalues of the matrix.

:::
:::

We won't reproduce the entire theorem here. @horn2012matrix provides a comprehensive discussion of the proof of this theorem. @keener1993perron also provides a concise proof and interesting applications to ranking.

From the Perron--Frobenius theorem, we know we are guaranteed to have at least one nonnegative eigenvector for $A$. This is good news! For the case where we have a strongly connected graph, then we have a nice *unique* answer to our problem. We should choose an eigenvector associated with the largest eigenvalue (i.e., the *leading eigenvalue*). Notice that an scalar multiple of this eigenvector will also work, as the relative rankings of nodes is still preserved. Many people will choose to use a normalized eigenvector for convenience.

::: {.callout-note icon=false appearance="minimal"}
::: {#def-eigenvector-centrality}

The **eigenvector centrality** ${\bf c}_{eig}$ satisfies

$$
    r{\bf c}_{eig} = {\bf A}{\bf c}_{eig} \,
$$

where $r$ is the leading eigenvalue of $A$. That is, the eigenvector centrality of node $i$ is the $i$th element of the leading eigenvector of the adjacency matrix.
:::
:::

If you have multiple strongly connected components, the second statement of the theorem will not hold. However, you can still calculate the eigenvector centrality of each component separately, as each strongly connected component satisfies all conditions for the theorem.

#### Complications with directed networks

- Should this centrality use in-edges or out-edges? This depends on the context! In-edges correspond to right eigenvectors; and out-edges correspond to left eigenvectors.
- Only nodes that are in a strongly connected component of two or more nodes, or in the out-component of such a strongly connected component, have nonzero eigenvector centrality.

We will see this challenge in the network below. Calculate the in-degree eigenvector centrality $x_i$ of each node, using the component-wise formula:

$$
    x_i = \frac{1}{r} \sum_j A_{ij} x_j \,.
$$


```{python}
#| fig-cap : "Calculate the eigenvector centrality of this network."
#| out.width : 80%
#| fig-cap-location: margin
#| label: fig-strong-component-exercise

from matplotlib import pyplot as plt
import networkx as nx
plt.style.use('seaborn-v0_8-whitegrid')

DG = nx.DiGraph()
DG.add_edges_from([(1,2), (1, 4), (2, 3), (2, 4), (3, 4), (3, 5), (4, 5)])

nx.draw(DG, with_labels = True, arrowsize = 20, font_color = 'white', font_weight = 'bold')

```

### Katz Centrality

It would be nice to be able to generalize eigenvector centrality while being able to avoid some of the issues that arose with nodes having zero centrality if they have zero in-degree.

We could try to introduce an intuitive fix by giving each node some centrality "for free." That is,

$$
    c_i = \alpha \sum_j A_{ij}c_j + \beta \,
$$

where $\alpha, \beta >0$ are constant. The first term follows the form we derived for eigenvector centrality, and the second is the baseline level of centrality (the "free centrality").

Writing in matrix-vector form, our Katz centrality ${\bf c}_{Katz}$ is 

$$
    {\bf c}_{Katz} = \alpha A {\bf c}_{Katz} + \beta {\bf 1} \,.
$$

If $I-\alpha A$ is invertible, then we will be able to write a nice expression for ${\bf c}_{Katz}$.
We know this matrix is not invertible when $det(I-\alpha A) = 0$, which is equivalent to the scalar multiple $det(\frac{1}{\alpha}I - A) = 0$. We deduce that this occurs when $\lambda = \frac{1}{\alpha}$, where $\lambda$ are the eigenvalues of the adjacency matrix.

Thus, if we want to be safe and guarantee convergence of our centrality measure, then we should choose $\alpha < {1}{\lambda_1}$, where $\lambda_1$ is the largest (most positive) eigenvalue of $A$. Then we are guaranteed convergence for $0 < \alpha < \frac{1}{\lambda_1}$.

Notice that within this constraint, $\alpha$ acts like a tunable parameter: As $\alpha \to 0$, all the nodes have the same centrality. As $\alpha \to \frac{1}{\lambda_1}$, we recover eigenvector centrality.

#### Advantages and disadvantages

Katz centrality keeps several of the nice features of eigenvector centrality while avoiding the zero-centrality pitfalls. It's also relatively quick to calculate.

However, if a node with high Katz centrality points to many other nodes in a directed network, then they will all "inherit" this high centrality as well, which may be undesirable.

## References